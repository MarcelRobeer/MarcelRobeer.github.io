
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="canonical" href="https://marcelrobeer.github.io/text_sensitivity/reference/text_sensitivity/sensitivity/">
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.2, mkdocs-material-7.2.0">
    
    
      
        <title>Sensitivity - text_sensitivity</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.fe914879.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.ba0d045b.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../../../docs/css/extra.css">
    
    
      


    
    
  
  
  
    
  
  <meta property="og:type" content="website" />
  <meta property="og:title" content="text_sensitivity Documentation - Sensitivity" />
  <meta property="og:description" content="Documentation for Python package text_sensitivity (extends text_explainability)" />
  <meta property="og:url" content="https://marcelrobeer.github.io/text_sensitivity/reference/text_sensitivity/sensitivity/" />
  <meta property="og:image" content="https://i.ibb.co/2nzcC1P/Text-Logo-Logo-large-sensitivity.png" />
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="343" />
  <meta property="og:image:height" content="101" />

  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:site" content="https://marcelrobeer.github.io/text_sensitivity/reference/text_sensitivity/sensitivity/" />
  <meta name="twitter:creator" content="Marcel Robeer" />
  <meta name="twitter:title" content="text_sensitivity Documentation - Sensitivity" />
  <meta name="twitter:description" content="Documentation for Python package text_sensitivity (extends text_explainability)" />
  <meta name="twitter:image" content="https://i.ibb.co/2nzcC1P/Text-Logo-Logo-large-sensitivity.png" />

  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="light" data-md-color-primary="" data-md-color-accent="">
  
    
    <script>function __prefix(e){return new URL("../../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
      <script>var palette=__get("__palette");if(null!==palette&&"object"==typeof palette.color)for(var key in palette.color)document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#module-text_sensitivitysensitivity" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="text_sensitivity" class="md-header__button md-logo" aria-label="text_sensitivity" data-md-component="logo">
      
  <img src="https://i.ibb.co/pvkkWqZ/Text-Logo-Logo-white.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            text_sensitivity
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Sensitivity
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="light" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M7 10a2 2 0 0 1 2 2 2 2 0 0 1-2 2 2 2 0 0 1-2-2 2 2 0 0 1 2-2m10-3a5 5 0 0 1 5 5 5 5 0 0 1-5 5H7a5 5 0 0 1-5-5 5 5 0 0 1 5-5h10M7 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3h10a3 3 0 0 0 3-3 3 3 0 0 0-3-3H7z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="dark" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://git.science.uu.nl/m.j.robeer/text_sensitivity" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    text_sensitivity
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="text_sensitivity" class="md-nav__button md-logo" aria-label="text_sensitivity" data-md-component="logo">
      
  <img src="https://i.ibb.co/pvkkWqZ/Text-Logo-Logo-white.png" alt="logo">

    </a>
    text_sensitivity
  </label>
  
    <div class="md-nav__source">
      
<a href="https://git.science.uu.nl/m.j.robeer/text_sensitivity" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    text_sensitivity
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/installation/" class="md-nav__link">
        Installation
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../example_usage/" class="md-nav__link">
        Example usage
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../CHANGELOG/" class="md-nav__link">
        Changelog
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" checked>
      
      <label class="md-nav__link" for="__nav_5">
        Reference
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Reference" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Reference
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_1" type="checkbox" id="__nav_5_1" checked>
      
      <label class="md-nav__link" for="__nav_5_1">
        Text Sensitivity
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Text Sensitivity" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_1">
          <span class="md-nav__icon md-icon"></span>
          Text Sensitivity
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        Index
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../metrics/" class="md-nav__link">
        Metrics
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../return_types/" class="md-nav__link">
        Return Types
      </a>
    </li>
  

          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Sensitivity
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Sensitivity
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#functions" class="md-nav__link">
    Functions
  </a>
  
    <nav class="md-nav" aria-label="Functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#apply_perturbation" class="md-nav__link">
    apply_perturbation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_accuracy" class="md-nav__link">
    compare_accuracy
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_metric" class="md-nav__link">
    compare_metric
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_precision" class="md-nav__link">
    compare_precision
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_recall" class="md-nav__link">
    compare_recall
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#equal_ground_truth" class="md-nav__link">
    equal_ground_truth
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#input_space_robustness" class="md-nav__link">
    input_space_robustness
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#invariance" class="md-nav__link">
    invariance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mean_score" class="md-nav__link">
    mean_score
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
            
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_1_5" type="checkbox" id="__nav_5_1_5" >
      
      <label class="md-nav__link" for="__nav_5_1_5">
        Data
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Data" data-md-level="3">
        <label class="md-nav__title" for="__nav_5_1_5">
          <span class="md-nav__icon md-icon"></span>
          Data
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../data/generate/" class="md-nav__link">
        Generate
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../data/" class="md-nav__link">
        Index
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../data/lists/" class="md-nav__link">
        Lists
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../data/wordlist/" class="md-nav__link">
        Wordlist
      </a>
    </li>
  

          
            
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_1_5_5" type="checkbox" id="__nav_5_1_5_5" >
      
      <label class="md-nav__link" for="__nav_5_1_5_5">
        Random
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Random" data-md-level="4">
        <label class="md-nav__title" for="__nav_5_1_5_5">
          <span class="md-nav__icon md-icon"></span>
          Random
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../data/random/entity/" class="md-nav__link">
        Entity
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../data/random/" class="md-nav__link">
        Index
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../data/random/string/" class="md-nav__link">
        String
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

          
            
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_1_6" type="checkbox" id="__nav_5_1_6" >
      
      <label class="md-nav__link" for="__nav_5_1_6">
        Perturbation
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Perturbation" data-md-level="3">
        <label class="md-nav__title" for="__nav_5_1_6">
          <span class="md-nav__icon md-icon"></span>
          Perturbation
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../perturbation/base/" class="md-nav__link">
        Base
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../perturbation/characters/" class="md-nav__link">
        Characters
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../perturbation/" class="md-nav__link">
        Index
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../perturbation/sentences/" class="md-nav__link">
        Sentences
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../perturbation/words/" class="md-nav__link">
        Words
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

          
            
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_1_7" type="checkbox" id="__nav_5_1_7" >
      
      <label class="md-nav__link" for="__nav_5_1_7">
        Ui
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Ui" data-md-level="3">
        <label class="md-nav__title" for="__nav_5_1_7">
          <span class="md-nav__icon md-icon"></span>
          Ui
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../ui/" class="md-nav__link">
        Index
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../ui/notebook/" class="md-nav__link">
        Notebook
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#functions" class="md-nav__link">
    Functions
  </a>
  
    <nav class="md-nav" aria-label="Functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#apply_perturbation" class="md-nav__link">
    apply_perturbation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_accuracy" class="md-nav__link">
    compare_accuracy
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_metric" class="md-nav__link">
    compare_metric
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_precision" class="md-nav__link">
    compare_precision
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_recall" class="md-nav__link">
    compare_recall
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#equal_ground_truth" class="md-nav__link">
    equal_ground_truth
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#input_space_robustness" class="md-nav__link">
    input_space_robustness
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#invariance" class="md-nav__link">
    invariance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mean_score" class="md-nav__link">
    mean_score
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="module-text_sensitivitysensitivity">Module text_sensitivity.sensitivity</h1>
<p>Sensitivity testing, for fairness and robustness.</p>
<p>None</p>
<details class="example"><summary>View Source</summary><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>&quot;&quot;&quot;Sensitivity testing, for fairness and robustness.&quot;&quot;&quot;

from typing import List, Optional, Tuple, Union

from genbase import add_callargs

from instancelib.analysis.base import label_metrics

from instancelib.environment.text import TextEnvironment

from instancelib.instances.base import InstanceProvider

from instancelib.instances.text import TextInstanceProvider

from instancelib.labels.memory import MemoryLabelProvider

from instancelib.machinelearning.base import AbstractClassifier

from instancelib.typehints import LT

from text_sensitivity.data.generate import from_pattern

from text_sensitivity.data.random.string import (RandomString,

                                                 combine_generators)

from text_sensitivity.perturbation.base import Perturbation

from text_sensitivity.return_types import LabelMetrics, MeanScore, SuccessTest

def apply_perturbation(dataset: Union[InstanceProvider, TextEnvironment],

                       perturbation: Perturbation) -&gt; Tuple[TextInstanceProvider, MemoryLabelProvider]:

    &quot;&quot;&quot;Apply a perturbation to a dataset, getting the perturbed instances and corresponding attribute labels.

    Examples:

        Repeat each string twice:

        &gt;&gt;&gt; from text_sensitivity.perturbation.sentences import repeat_k_times

        &gt;&gt;&gt; apply_perturbation(env, repeat_k_times(k=2))

        Add the unrelated string &#39;This is unrelated.&#39; before each instance:

        &gt;&gt;&gt; from text_sensitivity.perturbation import OneToOnePerturbation

        &gt;&gt;&gt; perturbation = OneToOnePerturbation.from_string(prefix=&#39;This is unrelated.&#39;)

        &gt;&gt;&gt; apply_perturbation(env, perturbation)

    Args:

        dataset (Union[InstanceProvider, TextEnvironment]): Dataset to apply perturbation to (e.g. all data, train set,

            test set, set belonging to a given label, or subset of data for a (un)protected group).

        perturbation (Perturbation): Perturbation to apply, one-to-one or one-to-many.

    Returns:

        Tuple[TextInstanceProvider, MemoryLabelProvider]: Perturbed instances and corresponding attribute labels.

    &quot;&quot;&quot;

    if isinstance(dataset, TextEnvironment):

        dataset = dataset.dataset

    if not isinstance(perturbation, Perturbation):

        perturbation = perturbation()

    new_data, attributes = [], []

    for key in dataset:

        for instances, labels in perturbation(dataset[key]):

            new_data.extend(instances) if isinstance(instances, list) else new_data.append(instances)

            attributes.extend(labels) if isinstance(labels, list) else attributes.append(labels)

    instanceprovider = TextInstanceProvider(new_data)

    instanceprovider.add_range(*dataset.get_all())

    labelprovider = MemoryLabelProvider.from_tuples(attributes)

    return instanceprovider, labelprovider

def equal_ground_truth(ground_truth, instances):

    # TODO: add ability to provide a different expectation of what will happen to the instance labels after perturbation

    for key in instances.keys():

        parent_key = key.split(&#39;|&#39;)[0] if isinstance(key, str) else str(key)

        parent_key = int(parent_key) if parent_key.isdigit() else parent_key

        yield (key, ground_truth._labeldict[parent_key])

@add_callargs

def compare_metric(env: TextEnvironment,

                   model: AbstractClassifier,

                   perturbation: Perturbation,

                   **kwargs) -&gt; LabelMetrics:

    &quot;&quot;&quot;Get metrics for each ground-truth label and attribute.

    Examples:

        Compare metric of `model` performance (e.g. accuracy, precision) before and after mapping each instance in a

        dataset to uppercase.

        &gt;&gt;&gt; from text_sensitivity.perturbation.sentences import to_upper

        &gt;&gt;&gt; compare_metric(env, model, to_upper)

        Compare metric when randomly adding 10 perturbed instances with typos to each instance in a dataset.

        &gt;&gt;&gt; from text_sensitivity.perturbation.characters import add_typos

        &gt;&gt;&gt; compare_metric(env, model, add_typos(n=10))

    Args:

        env (TextEnvironment): Environment containing original instances (`.dataset`)

            and ground-truth labels (`.labels`).

        model (AbstractClassifier): Black-box model to compare metrics on.

        perturbation (Perturbation): Peturbation to apply.

    Returns:

        LabelMetrics: Original label (before perturbation), perturbed label (after perturbation)

        and metrics for label-attribute pair.

    &quot;&quot;&quot;

    callargs = kwargs.pop(&#39;__callargs__&#39;, None)

    # Apply perturbations and get attributes

    instances, attributes = apply_perturbation(env, perturbation)

    # Perform prediction on original instances and perturbed instances

    model_predictions = MemoryLabelProvider.from_tuples(model.predict(instances))

    # Expectation (for now that labels should remain equal)

    ground_truth = MemoryLabelProvider.from_tuples(list(equal_ground_truth(env.labels, instances)))

    lm = [(label, attribute, label_metrics(model_predictions,

                                           ground_truth,

                                           attributes.get_instances_by_label(attribute),

                                           label))

                    for attribute in list(attributes.labelset)

                    for label in list(model_predictions.labelset)]

    return LabelMetrics(instances=instances,

                        label_metrics=lm,

                        callargs=callargs)

def compare_accuracy(*args, **kwargs):

    &quot;&quot;&quot;Compare accuracy scores for each ground-truth label and attribute.&quot;&quot;&quot;

    import pandas as pd

    return pd.DataFrame([(label, attribute, metrics.accuracy)

                         for label, attribute, metrics in compare_metric(*args, **kwargs)],

                        columns=[&#39;label&#39;, &#39;attribute&#39;, &#39;accuracy&#39;])

def compare_precision(*args, **kwargs):

    &quot;&quot;&quot;Compare precision scores for each ground-truth label and attribute.&quot;&quot;&quot;

    import pandas as pd

    return pd.DataFrame([(label, attribute, metrics.precision)

                         for label, attribute, metrics in compare_metric(*args, **kwargs)],

                        columns=[&#39;label&#39;, &#39;attribute&#39;, &#39;precision&#39;])

def compare_recall(*args, **kwargs):

    &quot;&quot;&quot;Compare recall scores for each ground-truth label and attribute.&quot;&quot;&quot;

    import pandas as pd

    return pd.DataFrame([(label, attribute, metrics.recall)

                         for label, attribute, metrics in compare_metric(*args, **kwargs)],

                        columns=[&#39;label&#39;, &#39;attribute&#39;, &#39;recall&#39;])

@add_callargs

def input_space_robustness(model: AbstractClassifier,

                           generators: List[RandomString],

                           n_samples: int = 100,

                           min_length: int = 0,

                           max_length: int = 100,

                           seed: Optional[int] = 0,

                           **kwargs) -&gt; SuccessTest:

    &quot;&quot;&quot;Test the robustness of a machine learning model to different input types.

    Example:

        Test a pretrained black-box `model` for its robustness to 1000 random strings (length 0 to 500),

        containing whitespace characters, ASCII (upper, lower and numbers), emojis and Russian Cyrillic characters:

        &gt;&gt;&gt; from text_sensitivity.data.random.string import RandomAscii, RandomCyrillic, RandomEmojis, RandomWhitespace

        &gt;&gt;&gt; input_space_robustness(model,

        &gt;&gt;&gt;                        [RandomWhitespace(), RandomAscii(), RandomEmojis(base=True), RandomCyrillic(&#39;ru&#39;)],

        &gt;&gt;&gt;                        n_samples=1000,

        &gt;&gt;&gt;                        min_length=0,

        &gt;&gt;&gt;                        max_length=500)

    Args:

        model (AbstractClassifier): Machine learning model to test.

        generators (List[RandomString]): Random character generators.

        n_samples (int, optional): Number of test samples. Defaults to 100.

        min_length (int, optional): Input minimum length. Defaults to 0.

        max_length (int, optional): Input maximum length. Defaults to 100.

        seed (Optional[int], optional): Seed for reproducibility purposes. Defaults to 0.

    Returns:

        SuccessTest: Percentage of success cases, list of succeeded/failed instances

    &quot;&quot;&quot;

    callargs = kwargs.pop(&#39;__callargs__&#39;, None)

    # Combine all generators into one

    generator = combine_generators(*generators, seed=seed)

    # Generate instances

    instances = generator.generate(n=n_samples, min_length=min_length, max_length=max_length)

    # Percentage success, instances that succeeded, instances that failed

    success: int = 0

    successes: List[List[str]] = []

    failures: List[List[str]] = []

    # Do not perform it batchwise but per instance, in order to return the error-throwing failures

    for i in instances:

        try:

            model.predict([instances[i]])

            success += 1

            successes.append(instances[i])

        except Exception:

            failures.append(instances[i])

    return SuccessTest(1.0 if len(instances) == 0 else success / len(instances),

                       successes,

                       failures,

                       type=&#39;robustness&#39;,

                       subtype=&#39;input_space&#39;,

                       callargs=callargs)

@add_callargs

def invariance(pattern: str,

               model: AbstractClassifier,

               expectation: Optional[LT] = None,

               **kwargs,

               ) -&gt; SuccessTest:

    &quot;&quot;&quot;Test for the failure rate under invariance.

    Args:

        pattern (str): String pattern to generate examples from.

        model (AbstractClassifier): Model to test.

        expectation (Optional[LT], optional): Expected outcome values. Defaults to None.

        **kwargs: Optional arguments passed onto the `data.generate.from_pattern()` function.

    Returns:

        SuccessTest: Percentage of success cases, list of succeeded (invariant)/failed (variant) instances

    &quot;&quot;&quot;

    callargs = kwargs.pop(&#39;__callargs__&#39;, None)

    # Generate instances from pattern and predict

    instances, _ = from_pattern(pattern, **kwargs)

    predictions = model.predict(instances)

    if expectation is None:

        if len(predictions) == 0:

            return 0.0, [], []

        expectation = predictions[0][-1]

    if not isinstance(expectation, frozenset):

        expectation = frozenset({expectation})

    correct = [instances[id] for id, label in predictions if label == expectation]

    wrong = [instances[id] for id, label in predictions if label != expectation]

    return SuccessTest(1.0 if len(predictions) == 0 else len(correct) / len(predictions),

                       correct,

                       wrong,

                       predictions=predictions,

                       type=&#39;sensitivity&#39;,

                       subtype=&#39;invariance&#39;,

                       callargs=callargs)

@add_callargs

def mean_score(pattern: str,

               model: AbstractClassifier,

               selected_label: Optional[LT] = None,

               **kwargs) -&gt; MeanScore:

    &quot;&quot;&quot;Calculate mean (probability) score for a given label, for data generated from a pattern.

    Args:

        pattern (str):

        model (AbstractClassifier): Model to generate scores from.

        selected_label (Optional[LT], optional): Label name to select. If None is replaced by the first label.

            Defaults to None.

    Returns:

        MeanScore: Mean score for the selected label, generated instances and label scores.

    &quot;&quot;&quot;

    callargs = kwargs.pop(&#39;__callargs__&#39;, None)

    instances, _ = from_pattern(pattern, **kwargs)

    predictions = model.predict_proba_provider(instances)

    if selected_label is None:

        if len(predictions) == 0:

            return

        selected_label = list(model.encoder.labelset)[0]

    if isinstance(selected_label, frozenset):

        selected_label = list(selected_label)[0]

    predictions_by_label = [(instances[id], [proba for label, proba in list(probas) if label == selected_label][0])

                            for id, probas in predictions]

    return MeanScore(scores=[p for _, p in predictions_by_label],

                     label=selected_label,

                     instances=instances,

                     callargs=callargs,

                     **kwargs)
</code></pre></div>
</td></tr></table>
</details>
<h2 id="functions">Functions</h2>
<h3 id="apply_perturbation">apply_perturbation</h3>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">apply_perturbation</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">instancelib</span><span class="o">.</span><span class="n">instances</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">InstanceProvider</span><span class="p">,</span> <span class="n">instancelib</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">TextEnvironment</span><span class="p">],</span>
    <span class="n">perturbation</span><span class="p">:</span> <span class="n">text_sensitivity</span><span class="o">.</span><span class="n">perturbation</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Perturbation</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">instancelib</span><span class="o">.</span><span class="n">instances</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">TextInstanceProvider</span><span class="p">,</span> <span class="n">instancelib</span><span class="o">.</span><span class="n">labels</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">MemoryLabelProvider</span><span class="p">]</span>
</code></pre></div>
</td></tr></table>
<p>Apply a perturbation to a dataset, getting the perturbed instances and corresponding attribute labels.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>dataset</td>
<td>Union[InstanceProvider, TextEnvironment]</td>
<td>Dataset to apply perturbation to (e.g. all data, train set,</td>
<td></td>
</tr>
<tr>
<td>test set, set belonging to a given label, or subset of data for a (un)protected group).</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>perturbation</td>
<td>Perturbation</td>
<td>Perturbation to apply, one-to-one or one-to-many.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tuple[TextInstanceProvider, MemoryLabelProvider]</td>
<td>Perturbed instances and corresponding attribute labels.</td>
</tr>
</tbody>
</table>
<details class="example"><summary>View Source</summary><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>def apply_perturbation(dataset: Union[InstanceProvider, TextEnvironment],

                       perturbation: Perturbation) -&gt; Tuple[TextInstanceProvider, MemoryLabelProvider]:

    &quot;&quot;&quot;Apply a perturbation to a dataset, getting the perturbed instances and corresponding attribute labels.

    Examples:

        Repeat each string twice:

        &gt;&gt;&gt; from text_sensitivity.perturbation.sentences import repeat_k_times

        &gt;&gt;&gt; apply_perturbation(env, repeat_k_times(k=2))

        Add the unrelated string &#39;This is unrelated.&#39; before each instance:

        &gt;&gt;&gt; from text_sensitivity.perturbation import OneToOnePerturbation

        &gt;&gt;&gt; perturbation = OneToOnePerturbation.from_string(prefix=&#39;This is unrelated.&#39;)

        &gt;&gt;&gt; apply_perturbation(env, perturbation)

    Args:

        dataset (Union[InstanceProvider, TextEnvironment]): Dataset to apply perturbation to (e.g. all data, train set,

            test set, set belonging to a given label, or subset of data for a (un)protected group).

        perturbation (Perturbation): Perturbation to apply, one-to-one or one-to-many.

    Returns:

        Tuple[TextInstanceProvider, MemoryLabelProvider]: Perturbed instances and corresponding attribute labels.

    &quot;&quot;&quot;

    if isinstance(dataset, TextEnvironment):

        dataset = dataset.dataset

    if not isinstance(perturbation, Perturbation):

        perturbation = perturbation()

    new_data, attributes = [], []

    for key in dataset:

        for instances, labels in perturbation(dataset[key]):

            new_data.extend(instances) if isinstance(instances, list) else new_data.append(instances)

            attributes.extend(labels) if isinstance(labels, list) else attributes.append(labels)

    instanceprovider = TextInstanceProvider(new_data)

    instanceprovider.add_range(*dataset.get_all())

    labelprovider = MemoryLabelProvider.from_tuples(attributes)

    return instanceprovider, labelprovider
</code></pre></div>
</td></tr></table>
</details>
<h3 id="compare_accuracy">compare_accuracy</h3>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">compare_accuracy</span><span class="p">(</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
</td></tr></table>
<p>Compare accuracy scores for each ground-truth label and attribute.</p>
<details class="example"><summary>View Source</summary><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>def compare_accuracy(*args, **kwargs):

    &quot;&quot;&quot;Compare accuracy scores for each ground-truth label and attribute.&quot;&quot;&quot;

    import pandas as pd

    return pd.DataFrame([(label, attribute, metrics.accuracy)

                         for label, attribute, metrics in compare_metric(*args, **kwargs)],

                        columns=[&#39;label&#39;, &#39;attribute&#39;, &#39;accuracy&#39;])
</code></pre></div>
</td></tr></table>
</details>
<h3 id="compare_metric">compare_metric</h3>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">compare_metric</span><span class="p">(</span>
    <span class="n">env</span><span class="p">:</span> <span class="n">instancelib</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">TextEnvironment</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">instancelib</span><span class="o">.</span><span class="n">machinelearning</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractClassifier</span><span class="p">,</span>
    <span class="n">perturbation</span><span class="p">:</span> <span class="n">text_sensitivity</span><span class="o">.</span><span class="n">perturbation</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Perturbation</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">text_sensitivity</span><span class="o">.</span><span class="n">return_types</span><span class="o">.</span><span class="n">LabelMetrics</span>
</code></pre></div>
</td></tr></table>
<p>Get metrics for each ground-truth label and attribute.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>env</td>
<td>TextEnvironment</td>
<td>Environment containing original instances (<code>.dataset</code>)</td>
<td></td>
</tr>
<tr>
<td>and ground-truth labels (<code>.labels</code>).</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>model</td>
<td>AbstractClassifier</td>
<td>Black-box model to compare metrics on.</td>
<td>None</td>
</tr>
<tr>
<td>perturbation</td>
<td>Perturbation</td>
<td>Peturbation to apply.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>LabelMetrics</td>
<td>Original label (before perturbation), perturbed label (after perturbation)</td>
</tr>
<tr>
<td>and metrics for label-attribute pair.</td>
<td></td>
</tr>
</tbody>
</table>
<details class="example"><summary>View Source</summary><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>@add_callargs

def compare_metric(env: TextEnvironment,

                   model: AbstractClassifier,

                   perturbation: Perturbation,

                   **kwargs) -&gt; LabelMetrics:

    &quot;&quot;&quot;Get metrics for each ground-truth label and attribute.

    Examples:

        Compare metric of `model` performance (e.g. accuracy, precision) before and after mapping each instance in a

        dataset to uppercase.

        &gt;&gt;&gt; from text_sensitivity.perturbation.sentences import to_upper

        &gt;&gt;&gt; compare_metric(env, model, to_upper)

        Compare metric when randomly adding 10 perturbed instances with typos to each instance in a dataset.

        &gt;&gt;&gt; from text_sensitivity.perturbation.characters import add_typos

        &gt;&gt;&gt; compare_metric(env, model, add_typos(n=10))

    Args:

        env (TextEnvironment): Environment containing original instances (`.dataset`)

            and ground-truth labels (`.labels`).

        model (AbstractClassifier): Black-box model to compare metrics on.

        perturbation (Perturbation): Peturbation to apply.

    Returns:

        LabelMetrics: Original label (before perturbation), perturbed label (after perturbation)

        and metrics for label-attribute pair.

    &quot;&quot;&quot;

    callargs = kwargs.pop(&#39;__callargs__&#39;, None)

    # Apply perturbations and get attributes

    instances, attributes = apply_perturbation(env, perturbation)

    # Perform prediction on original instances and perturbed instances

    model_predictions = MemoryLabelProvider.from_tuples(model.predict(instances))

    # Expectation (for now that labels should remain equal)

    ground_truth = MemoryLabelProvider.from_tuples(list(equal_ground_truth(env.labels, instances)))

    lm = [(label, attribute, label_metrics(model_predictions,

                                           ground_truth,

                                           attributes.get_instances_by_label(attribute),

                                           label))

                    for attribute in list(attributes.labelset)

                    for label in list(model_predictions.labelset)]

    return LabelMetrics(instances=instances,

                        label_metrics=lm,

                        callargs=callargs)
</code></pre></div>
</td></tr></table>
</details>
<h3 id="compare_precision">compare_precision</h3>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">compare_precision</span><span class="p">(</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
</td></tr></table>
<p>Compare precision scores for each ground-truth label and attribute.</p>
<details class="example"><summary>View Source</summary><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>def compare_precision(*args, **kwargs):

    &quot;&quot;&quot;Compare precision scores for each ground-truth label and attribute.&quot;&quot;&quot;

    import pandas as pd

    return pd.DataFrame([(label, attribute, metrics.precision)

                         for label, attribute, metrics in compare_metric(*args, **kwargs)],

                        columns=[&#39;label&#39;, &#39;attribute&#39;, &#39;precision&#39;])
</code></pre></div>
</td></tr></table>
</details>
<h3 id="compare_recall">compare_recall</h3>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">compare_recall</span><span class="p">(</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
</td></tr></table>
<p>Compare recall scores for each ground-truth label and attribute.</p>
<details class="example"><summary>View Source</summary><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>def compare_recall(*args, **kwargs):

    &quot;&quot;&quot;Compare recall scores for each ground-truth label and attribute.&quot;&quot;&quot;

    import pandas as pd

    return pd.DataFrame([(label, attribute, metrics.recall)

                         for label, attribute, metrics in compare_metric(*args, **kwargs)],

                        columns=[&#39;label&#39;, &#39;attribute&#39;, &#39;recall&#39;])
</code></pre></div>
</td></tr></table>
</details>
<h3 id="equal_ground_truth">equal_ground_truth</h3>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">equal_ground_truth</span><span class="p">(</span>
    <span class="n">ground_truth</span><span class="p">,</span>
    <span class="n">instances</span>
<span class="p">)</span>
</code></pre></div>
</td></tr></table>
<details class="example"><summary>View Source</summary><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>def equal_ground_truth(ground_truth, instances):

    # TODO: add ability to provide a different expectation of what will happen to the instance labels after perturbation

    for key in instances.keys():

        parent_key = key.split(&#39;|&#39;)[0] if isinstance(key, str) else str(key)

        parent_key = int(parent_key) if parent_key.isdigit() else parent_key

        yield (key, ground_truth._labeldict[parent_key])
</code></pre></div>
</td></tr></table>
</details>
<h3 id="input_space_robustness">input_space_robustness</h3>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span>
<span class="normal">8</span>
<span class="normal">9</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">input_space_robustness</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">instancelib</span><span class="o">.</span><span class="n">machinelearning</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractClassifier</span><span class="p">,</span>
    <span class="n">generators</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">text_sensitivity</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">string</span><span class="o">.</span><span class="n">RandomString</span><span class="p">],</span>
    <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">min_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">text_sensitivity</span><span class="o">.</span><span class="n">return_types</span><span class="o">.</span><span class="n">SuccessTest</span>
</code></pre></div>
</td></tr></table>
<p>Test the robustness of a machine learning model to different input types.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>AbstractClassifier</td>
<td>Machine learning model to test.</td>
<td>None</td>
</tr>
<tr>
<td>generators</td>
<td>List[RandomString]</td>
<td>Random character generators.</td>
<td>None</td>
</tr>
<tr>
<td>n_samples</td>
<td>int</td>
<td>Number of test samples. Defaults to 100.</td>
<td>100</td>
</tr>
<tr>
<td>min_length</td>
<td>int</td>
<td>Input minimum length. Defaults to 0.</td>
<td>0</td>
</tr>
<tr>
<td>max_length</td>
<td>int</td>
<td>Input maximum length. Defaults to 100.</td>
<td>100</td>
</tr>
<tr>
<td>seed</td>
<td>Optional[int]</td>
<td>Seed for reproducibility purposes. Defaults to 0.</td>
<td>0</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>SuccessTest</td>
<td>Percentage of success cases, list of succeeded/failed instances</td>
</tr>
</tbody>
</table>
<details class="example"><summary>View Source</summary><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>@add_callargs

def input_space_robustness(model: AbstractClassifier,

                           generators: List[RandomString],

                           n_samples: int = 100,

                           min_length: int = 0,

                           max_length: int = 100,

                           seed: Optional[int] = 0,

                           **kwargs) -&gt; SuccessTest:

    &quot;&quot;&quot;Test the robustness of a machine learning model to different input types.

    Example:

        Test a pretrained black-box `model` for its robustness to 1000 random strings (length 0 to 500),

        containing whitespace characters, ASCII (upper, lower and numbers), emojis and Russian Cyrillic characters:

        &gt;&gt;&gt; from text_sensitivity.data.random.string import RandomAscii, RandomCyrillic, RandomEmojis, RandomWhitespace

        &gt;&gt;&gt; input_space_robustness(model,

        &gt;&gt;&gt;                        [RandomWhitespace(), RandomAscii(), RandomEmojis(base=True), RandomCyrillic(&#39;ru&#39;)],

        &gt;&gt;&gt;                        n_samples=1000,

        &gt;&gt;&gt;                        min_length=0,

        &gt;&gt;&gt;                        max_length=500)

    Args:

        model (AbstractClassifier): Machine learning model to test.

        generators (List[RandomString]): Random character generators.

        n_samples (int, optional): Number of test samples. Defaults to 100.

        min_length (int, optional): Input minimum length. Defaults to 0.

        max_length (int, optional): Input maximum length. Defaults to 100.

        seed (Optional[int], optional): Seed for reproducibility purposes. Defaults to 0.

    Returns:

        SuccessTest: Percentage of success cases, list of succeeded/failed instances

    &quot;&quot;&quot;

    callargs = kwargs.pop(&#39;__callargs__&#39;, None)

    # Combine all generators into one

    generator = combine_generators(*generators, seed=seed)

    # Generate instances

    instances = generator.generate(n=n_samples, min_length=min_length, max_length=max_length)

    # Percentage success, instances that succeeded, instances that failed

    success: int = 0

    successes: List[List[str]] = []

    failures: List[List[str]] = []

    # Do not perform it batchwise but per instance, in order to return the error-throwing failures

    for i in instances:

        try:

            model.predict([instances[i]])

            success += 1

            successes.append(instances[i])

        except Exception:

            failures.append(instances[i])

    return SuccessTest(1.0 if len(instances) == 0 else success / len(instances),

                       successes,

                       failures,

                       type=&#39;robustness&#39;,

                       subtype=&#39;input_space&#39;,

                       callargs=callargs)
</code></pre></div>
</td></tr></table>
</details>
<h3 id="invariance">invariance</h3>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">invariance</span><span class="p">(</span>
    <span class="n">pattern</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">instancelib</span><span class="o">.</span><span class="n">machinelearning</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractClassifier</span><span class="p">,</span>
    <span class="n">expectation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="o">~</span><span class="n">LT</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">text_sensitivity</span><span class="o">.</span><span class="n">return_types</span><span class="o">.</span><span class="n">SuccessTest</span>
</code></pre></div>
</td></tr></table>
<p>Test for the failure rate under invariance.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>pattern</td>
<td>str</td>
<td>String pattern to generate examples from.</td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>AbstractClassifier</td>
<td>Model to test.</td>
<td>None</td>
</tr>
<tr>
<td>expectation</td>
<td>Optional[LT]</td>
<td>Expected outcome values. Defaults to None.</td>
<td>None</td>
</tr>
<tr>
<td>**kwargs</td>
<td>None</td>
<td>Optional arguments passed onto the <code>data.generate.from_pattern()</code> function.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>SuccessTest</td>
<td>Percentage of success cases, list of succeeded (invariant)/failed (variant) instances</td>
</tr>
</tbody>
</table>
<details class="example"><summary>View Source</summary><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>@add_callargs

def invariance(pattern: str,

               model: AbstractClassifier,

               expectation: Optional[LT] = None,

               **kwargs,

               ) -&gt; SuccessTest:

    &quot;&quot;&quot;Test for the failure rate under invariance.

    Args:

        pattern (str): String pattern to generate examples from.

        model (AbstractClassifier): Model to test.

        expectation (Optional[LT], optional): Expected outcome values. Defaults to None.

        **kwargs: Optional arguments passed onto the `data.generate.from_pattern()` function.

    Returns:

        SuccessTest: Percentage of success cases, list of succeeded (invariant)/failed (variant) instances

    &quot;&quot;&quot;

    callargs = kwargs.pop(&#39;__callargs__&#39;, None)

    # Generate instances from pattern and predict

    instances, _ = from_pattern(pattern, **kwargs)

    predictions = model.predict(instances)

    if expectation is None:

        if len(predictions) == 0:

            return 0.0, [], []

        expectation = predictions[0][-1]

    if not isinstance(expectation, frozenset):

        expectation = frozenset({expectation})

    correct = [instances[id] for id, label in predictions if label == expectation]

    wrong = [instances[id] for id, label in predictions if label != expectation]

    return SuccessTest(1.0 if len(predictions) == 0 else len(correct) / len(predictions),

                       correct,

                       wrong,

                       predictions=predictions,

                       type=&#39;sensitivity&#39;,

                       subtype=&#39;invariance&#39;,

                       callargs=callargs)
</code></pre></div>
</td></tr></table>
</details>
<h3 id="mean_score">mean_score</h3>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">mean_score</span><span class="p">(</span>
    <span class="n">pattern</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">instancelib</span><span class="o">.</span><span class="n">machinelearning</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractClassifier</span><span class="p">,</span>
    <span class="n">selected_label</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="o">~</span><span class="n">LT</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">text_sensitivity</span><span class="o">.</span><span class="n">return_types</span><span class="o">.</span><span class="n">MeanScore</span>
</code></pre></div>
</td></tr></table>
<p>Calculate mean (probability) score for a given label, for data generated from a pattern.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>pattern</td>
<td>str</td>
<td></td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>AbstractClassifier</td>
<td>Model to generate scores from.</td>
<td>None</td>
</tr>
<tr>
<td>selected_label</td>
<td>Optional[LT]</td>
<td>Label name to select. If None is replaced by the first label.</td>
<td></td>
</tr>
<tr>
<td>Defaults to None.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>MeanScore</td>
<td>Mean score for the selected label, generated instances and label scores.</td>
</tr>
</tbody>
</table>
<details class="example"><summary>View Source</summary><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>@add_callargs

def mean_score(pattern: str,

               model: AbstractClassifier,

               selected_label: Optional[LT] = None,

               **kwargs) -&gt; MeanScore:

    &quot;&quot;&quot;Calculate mean (probability) score for a given label, for data generated from a pattern.

    Args:

        pattern (str):

        model (AbstractClassifier): Model to generate scores from.

        selected_label (Optional[LT], optional): Label name to select. If None is replaced by the first label.

            Defaults to None.

    Returns:

        MeanScore: Mean score for the selected label, generated instances and label scores.

    &quot;&quot;&quot;

    callargs = kwargs.pop(&#39;__callargs__&#39;, None)

    instances, _ = from_pattern(pattern, **kwargs)

    predictions = model.predict_proba_provider(instances)

    if selected_label is None:

        if len(predictions) == 0:

            return

        selected_label = list(model.encoder.labelset)[0]

    if isinstance(selected_label, frozenset):

        selected_label = list(selected_label)[0]

    predictions_by_label = [(instances[id], [proba for label, proba in list(probas) if label == selected_label][0])

                            for id, probas in predictions]

    return MeanScore(scores=[p for _, p in predictions_by_label],

                     label=selected_label,

                     instances=instances,

                     callargs=callargs,

                     **kwargs)
</code></pre></div>
</td></tr></table>
</details>
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../return_types/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Return Types" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Return Types
            </div>
          </div>
        </a>
      
      
        
        <a href="../data/generate/" class="md-footer__link md-footer__link--next" aria-label="Next: Generate" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Generate
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2021 Marcel Robeer
          </div>
        
        
          Powered by
          <a href="http://timothycrosley.github.io/portray" target="_blank" rel="noopener">portray</a>
          using theme
          <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
            Material for MkDocs
          </a>
        
        
      </div>
      
  <div class="md-footer-social">
    
      
      
      <a href="https://git.science.uu.nl/m.j.robeer/text_sensitivity" target="_blank" rel="noopener" title="text_sensitivity on GitLab" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M105.2 24.9c-3.1-8.9-15.7-8.9-18.9 0L29.8 199.7h132c-.1 0-56.6-174.8-56.6-174.8zM.9 287.7c-2.6 8 .3 16.9 7.1 22l247.9 184-226.2-294zm160.8-88 94.3 294 94.3-294zm349.4 88-28.8-88-226.3 294 247.9-184c6.9-5.1 9.7-14 7.2-22zM425.7 24.9c-3.1-8.9-15.7-8.9-18.9 0l-56.6 174.8h132z"/></svg>
      </a>
    
      
      
      <a href="https://pypi.org/project/text-sensitivity/" target="_blank" rel="noopener" title="text_sensitivity on PyPI" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6zM286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3zM167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4zm-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../../assets/javascripts/workers/search.53c85856.min.js", "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.716f8af4.min.js"></script>
      
    
  </body>
</html>