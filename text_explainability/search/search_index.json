{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A generic explainability architecture for explaining text machine learning models text_explainability provides a generic architecture from which well-known state-of-the-art explainability approaches for text can be composed. This modular architecture allows components to be swapped out and combined, to quickly develop new types of explainability approaches for (natural language) text, or to improve a plethora of approaches by improving a single module . Several example methods are included, which provide local explanations ( explaining the prediction of a single instance , e.g. LIME and SHAP ) or global explanations ( explaining the dataset, or model behavior on the dataset , e.g. TokenFrequency and MMDCritic ). By replacing the default modules (e.g. local data generation, global data sampling or improved embedding methods), these methods can be improved upon or new methods can be introduced. \u00a9 Marcel Robeer, 2021-2022 Quick tour Local explanation : explain a models' prediction on a given sample, self-provided or from a dataset. 1 2 3 4 5 6 7 8 9 10 from text_explainability import LIME , LocalTree # Define sample to explain sample = 'Explain why this is positive and not negative!' # LIME explanation (local feature importance) LIME () . explain ( sample , model ) . scores # List of local rules, extracted from tree LocalTree () . explain ( sample , model ) . rules Global explanation : explain the whole dataset (e.g. train set, test set), and what they look like for the ground-truth or predicted labels. 1 2 3 4 5 6 7 8 9 10 from text_explainability import import_data , TokenFrequency , MMDCritic # Import dataset env = import_data ( './datasets/test.csv' , data_cols = [ 'fulltext' ], label_cols = [ 'label' ]) # Top-k most frequent tokens per label TokenFrequency ( env . dataset ) . explain ( labelprovider = env . labels , explain_model = False , k = 3 ) # 2 prototypes and 1 criticisms for the dataset MMDCritic ( env . dataset )( n_prototypes = 2 , n_criticisms = 1 ) Installation See the installation instructions for an extended installation guide. Method Instructions pip Install from PyPI via pip3 install text_explainability . To speed up the explanation generation process use pip3 install text_explainability[fast] . Local Clone this repository and install via pip3 install -e . or locally run python3 setup.py install . Documentation Full documentation of the latest version is provided at https://marcelrobeer.github.io/text_explainability/ . Example usage See example usage to see an example of how the package can be used, or run the lines in example_usage.py to do explore it interactively. Explanation methods included text_explainability includes methods for model-agnostic local explanation and global explanation . Each of these methods can be fully customized to fit the explainees' needs. Type Explanation method Description Paper/link Local explanation LIME Calculate feature attribution with Local Intepretable Model-Agnostic Explanations (LIME). [ Ribeiro2016 ], interpretable-ml/lime KernelSHAP Calculate feature attribution with Shapley Additive Explanations (SHAP). [ Lundberg2017 ], interpretable-ml/shap LocalTree Fit a local decision tree around a single decision. [ Guidotti2018 ] LocalRules Fit a local sparse set of label-specific rules using SkopeRules . github/skope-rules FoilTree Fit a local contrastive/counterfactual decision tree around a single decision. [ Robeer2018 ] Global explanation TokenFrequency Show the top- k number of tokens for each ground-truth or predicted label. TokenInformation Show the top- k token mutual information for a dataset or model. wikipedia/mutual_information KMedoids Embed instances and find top- n prototypes (can also be performed for each label using LabelwiseKMedoids ). interpretable-ml/prototypes MMDCritic Embed instances and find top- n prototypes and top- n criticisms (can also be performed for each label using LabelwiseMMDCritic ). [ Kim2016 ], interpretable-ml/prototypes Releases text_explainability is officially released through PyPI . See CHANGELOG.md for a full overview of the changes for each version. Extensions text_explainability can be extended to also perform sensitivity testing , checking for machine learning model robustness and fairness. The text_sensitivity package is available through PyPI and fully documented at https://marcelrobeer.github.io/text_sensitivity/ . Citation 1 2 3 4 5 6 @misc { text_explainability , title = {Python package text\\_explainability} , author = {Marcel Robeer} , howpublished = {\\url{https://git.science.uu.nl/m.j.robeer/text_explainability}} , year = {2021} } Maintenance Contributors Marcel Robeer ( @m.j.robeer ) Michiel Bron ( @mpbron-phd ) Todo Tasks yet to be done: Implement local post-hoc explanations: Implement Anchors Implement global post-hoc explanations: Representative subset Add support for regression models More complex data augmentation Top-k replacement (e.g. according to LM / WordNet) Tokens to exclude from being changed Bag-of-words style replacements Add rule-based return type Write more tests Credits Florian Gardin, Ronan Gautier, Nicolas Goix, Bibi Ndiaye and Jean-Matthieu Schertzer. Skope-rules . 2020. Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Dino Pedreschi, Franco Turini and Fosca Gianotti. Local Rule-Based Explanations of Black Box Decision Systems . 2018. Been Kim, Rajiv Khanna and Oluwasanmi O. Koyejo. Examples are not Enough, Learn to Criticize! Criticism for Interpretability . Advances in Neural Information Processing Systems (NIPS 2016) . 2016. Scott Lundberg and Su-In Lee. A Unified Approach to Interpreting Model Predictions . 31st Conference on Neural Information Processing Systems (NIPS 2017) . 2017. Christoph Molnar. Interpretable Machine Learning: A Guide for Making Black Box Models Explainable . 2021. Marco Tulio Ribeiro, Sameer Singh and Carlos Guestrin. \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier . Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2016) . 2016. Marco Tulio Ribeiro, Sameer Singh and Carlos Guestrin. Anchors: High-Precision Model-Agnostic Explanations . AAAI Conference on Artificial Intelligence (AAAI) . 2018. Jasper van der Waa, Marcel Robeer, Jurriaan van Diggelen, Matthieu Brinkhuis and Mark Neerincx. \"Contrastive Explanations with Local Foil Trees\" . 2018 Workshop on Human Interpretability in Machine Learning (WHI 2018) . 2018.","title":"Home"},{"location":"#quick-tour","text":"Local explanation : explain a models' prediction on a given sample, self-provided or from a dataset. 1 2 3 4 5 6 7 8 9 10 from text_explainability import LIME , LocalTree # Define sample to explain sample = 'Explain why this is positive and not negative!' # LIME explanation (local feature importance) LIME () . explain ( sample , model ) . scores # List of local rules, extracted from tree LocalTree () . explain ( sample , model ) . rules Global explanation : explain the whole dataset (e.g. train set, test set), and what they look like for the ground-truth or predicted labels. 1 2 3 4 5 6 7 8 9 10 from text_explainability import import_data , TokenFrequency , MMDCritic # Import dataset env = import_data ( './datasets/test.csv' , data_cols = [ 'fulltext' ], label_cols = [ 'label' ]) # Top-k most frequent tokens per label TokenFrequency ( env . dataset ) . explain ( labelprovider = env . labels , explain_model = False , k = 3 ) # 2 prototypes and 1 criticisms for the dataset MMDCritic ( env . dataset )( n_prototypes = 2 , n_criticisms = 1 )","title":"Quick tour"},{"location":"#installation","text":"See the installation instructions for an extended installation guide. Method Instructions pip Install from PyPI via pip3 install text_explainability . To speed up the explanation generation process use pip3 install text_explainability[fast] . Local Clone this repository and install via pip3 install -e . or locally run python3 setup.py install .","title":"Installation"},{"location":"#documentation","text":"Full documentation of the latest version is provided at https://marcelrobeer.github.io/text_explainability/ .","title":"Documentation"},{"location":"#example-usage","text":"See example usage to see an example of how the package can be used, or run the lines in example_usage.py to do explore it interactively.","title":"Example usage"},{"location":"#explanation-methods-included","text":"text_explainability includes methods for model-agnostic local explanation and global explanation . Each of these methods can be fully customized to fit the explainees' needs. Type Explanation method Description Paper/link Local explanation LIME Calculate feature attribution with Local Intepretable Model-Agnostic Explanations (LIME). [ Ribeiro2016 ], interpretable-ml/lime KernelSHAP Calculate feature attribution with Shapley Additive Explanations (SHAP). [ Lundberg2017 ], interpretable-ml/shap LocalTree Fit a local decision tree around a single decision. [ Guidotti2018 ] LocalRules Fit a local sparse set of label-specific rules using SkopeRules . github/skope-rules FoilTree Fit a local contrastive/counterfactual decision tree around a single decision. [ Robeer2018 ] Global explanation TokenFrequency Show the top- k number of tokens for each ground-truth or predicted label. TokenInformation Show the top- k token mutual information for a dataset or model. wikipedia/mutual_information KMedoids Embed instances and find top- n prototypes (can also be performed for each label using LabelwiseKMedoids ). interpretable-ml/prototypes MMDCritic Embed instances and find top- n prototypes and top- n criticisms (can also be performed for each label using LabelwiseMMDCritic ). [ Kim2016 ], interpretable-ml/prototypes","title":"Explanation methods included"},{"location":"#releases","text":"text_explainability is officially released through PyPI . See CHANGELOG.md for a full overview of the changes for each version.","title":"Releases"},{"location":"#extensions","text":"text_explainability can be extended to also perform sensitivity testing , checking for machine learning model robustness and fairness. The text_sensitivity package is available through PyPI and fully documented at https://marcelrobeer.github.io/text_sensitivity/ .","title":"Extensions"},{"location":"#citation","text":"1 2 3 4 5 6 @misc { text_explainability , title = {Python package text\\_explainability} , author = {Marcel Robeer} , howpublished = {\\url{https://git.science.uu.nl/m.j.robeer/text_explainability}} , year = {2021} }","title":"Citation"},{"location":"#maintenance","text":"","title":"Maintenance"},{"location":"#contributors","text":"Marcel Robeer ( @m.j.robeer ) Michiel Bron ( @mpbron-phd )","title":"Contributors"},{"location":"#todo","text":"Tasks yet to be done: Implement local post-hoc explanations: Implement Anchors Implement global post-hoc explanations: Representative subset Add support for regression models More complex data augmentation Top-k replacement (e.g. according to LM / WordNet) Tokens to exclude from being changed Bag-of-words style replacements Add rule-based return type Write more tests","title":"Todo"},{"location":"#credits","text":"Florian Gardin, Ronan Gautier, Nicolas Goix, Bibi Ndiaye and Jean-Matthieu Schertzer. Skope-rules . 2020. Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Dino Pedreschi, Franco Turini and Fosca Gianotti. Local Rule-Based Explanations of Black Box Decision Systems . 2018. Been Kim, Rajiv Khanna and Oluwasanmi O. Koyejo. Examples are not Enough, Learn to Criticize! Criticism for Interpretability . Advances in Neural Information Processing Systems (NIPS 2016) . 2016. Scott Lundberg and Su-In Lee. A Unified Approach to Interpreting Model Predictions . 31st Conference on Neural Information Processing Systems (NIPS 2017) . 2017. Christoph Molnar. Interpretable Machine Learning: A Guide for Making Black Box Models Explainable . 2021. Marco Tulio Ribeiro, Sameer Singh and Carlos Guestrin. \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier . Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2016) . 2016. Marco Tulio Ribeiro, Sameer Singh and Carlos Guestrin. Anchors: High-Precision Model-Agnostic Explanations . AAAI Conference on Artificial Intelligence (AAAI) . 2018. Jasper van der Waa, Marcel Robeer, Jurriaan van Diggelen, Matthieu Brinkhuis and Mark Neerincx. \"Contrastive Explanations with Local Foil Trees\" . 2018 Workshop on Human Interpretability in Machine Learning (WHI 2018) . 2018.","title":"Credits"},{"location":"CHANGELOG/","text":"Changelog All notable changes to text_explainability will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . Unreleased 0.6.1 - 2022-03-16 Changed Requires genbase>=0.2.4 Requires instancelib>=0.4.3.1 Fixed Typo fixes and small bugs 0.6.0 - 2022-03-04 Added More tests to increase test coverage Changed Requires genbase>=0.2.2 Renamed pyproject.toml to .portray to avoid build errors Made fastcountvectorizer optional Fixed Bugfix when installing package, by moving __version___ to /_version.py 0.5.8 - 2021-12-02 Added get_meta_descriptors() to get type/subtype/method from meta Changed Requires genbase>=0.1.13 Fixed Bugfix in MMDCritic for prototype indices Bugfix in TRANSLATION_DICT 0.5.7 - 2021-12-01 Added Return type for Instances Rendering of Instances Rendering of FeatureList Extended rendering of render_subtitle() Changed Ensure MMDCritic / KMedoids returns Instances Requires genbase>=0.1.11 Fixed Bugfix of instance identifier in PrototypeSampler._select_from_provider() 0.5.6 - 2021-11-30 Added Added meta information with genbase.MetaInfo Rendering with and extended genbase.Render Changed Moved Readable to genbase Use genbase.SeedMixin for seeds Use genbase.internationalization for internationalization Requires genbase>=0.1.10 Fixed Selected features are in order in FeatureList 0.5.5 - 2021-11-17 Changed TokenFrequency and TokenInformation now use the faster fastcountvectorizer implementation Fixed Bugfixes in return type of TokenFrequency and TokenInformation 0.5.4 - 2021-10-27 Fixed Bugfixes in local explanation return types 0.5.3 - 2021-10-19 Fixed Made alpha optional in LinearSurrogate Added skope-rules dependency to setup.py 0.5.2 - 2021-10-05 Fixed Hotfix in FeatureSelector._information_criterion() 0.5.1 - 2021-10-05 Added Added text_explainability.data.from_list Changed Added example results in README.md Fixed Added new methods and classes to __init__.py 0.5.0 - 2021-10-04 Added Security testing with bandit More locale translations Wrappers around instancelib in text_explainability.data and text_explainability.model Changed Extended description in README.md Changed example usage to fit workflow changes Logo link in README.md Fixed Bugfixes in MMDCritic Bugfixes in KernelSHAP 0.4.6 - 2021-10-02 Added External documentation Documentation styling Citation information Changed Word tokenizer can now combine tokens in curly bracket when setting exclude_curly_brackets=True 0.4.5 - 2021-09-24 Added Decorator to allow strings to be converted into TextInstances Decorator to ensure TextInstances are tokenized when required Fixed Typing fixes 0.4.4 - 2021-09-23 Added Character-level tokenizer/detokenizer 0.4.3 - 2021-09-20 Added New embeddings not requiring internet ( CountVectorizer , TfidfVectorizer ) Rules return type First version of local rules using SkopeRules More test cases Changed New default embedding method for MMDCritic and KMedoids Version moved to __init__.py New README.md layout Updates to Anchor local explanations Added random state in example_usage to ensure reproducibility 0.4.2 - 2021-09-13 Fixed Hotfix to fix predict_proba usage 0.4.1 - 2021-09-13 Fixed Hotfix to make dependency on internet optional 0.4.0 - 2021-09-13 Added Initial support for embeddings/vectors Support for dimensionality reduction Initial implementation of MMD-Critic Initial implementation of labelwise MMD-Critic Initial implementation of prototype selection using k-Medoids Changed Updated README.md 0.3.8 - 2021-09-07 Changed Support for dimensionality reduction Fixed Bugfix in including locale/*.json files during setup 0.3.7 - 2021-09-07 Added Dependencies for package 0.3.6 - 2021-09-07 Added PyPI release script to .gitignore Badges to README.md Added dependencies to setup.py 0.3.5 - 2021-09-03 Changed Locale changed to .json format, to remove optional dependency Fixed Bugfix for getting key in TokenFrequency Bugfixes in FeatureAttribution return type Bugfixes in i18n 0.3.4 - 2021-08-18 Changed External logo url Fixed Hotfix in FeatureAttribution 0.3.3 - 2021-08-18 Added Updated to support instancelib==0.3.1.2 i18n internationalization support CHANGELOG.md Changed Additional samples in example dataset Fixed Bugfixes for LIME and FeatureAttribution return type 0.3.2 - 2021-07-27 Added Initial support for Foil Trees Logo in documentation Changed Improved documentation 0.3.1 - 2021-07-23 Added flake8 linting CI/CD Pipeline Run test scripts 0.3.0 - 2021-07-20 Added Updated to support instancelib==0.3.0.0 Changed Improved documentation global_explanation classes have equal return types 0.2 - 2021-06-22 Added LICENSE.md Updated to support instancelib==0.2.3.1 Changed Module description 0.1 - 2021-05-28 Added README.md Example usage Local explanation classes (LIME, KernelSHAP) Global explanation classes Data augmentation/sampling Feature selection Local surrogates Tokenization git setup","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"All notable changes to text_explainability will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"CHANGELOG/#unreleased","text":"","title":"Unreleased"},{"location":"CHANGELOG/#061-2022-03-16","text":"","title":"0.6.1 - 2022-03-16"},{"location":"CHANGELOG/#changed","text":"Requires genbase>=0.2.4 Requires instancelib>=0.4.3.1","title":"Changed"},{"location":"CHANGELOG/#fixed","text":"Typo fixes and small bugs","title":"Fixed"},{"location":"CHANGELOG/#060-2022-03-04","text":"","title":"0.6.0 - 2022-03-04"},{"location":"CHANGELOG/#added","text":"More tests to increase test coverage","title":"Added"},{"location":"CHANGELOG/#changed_1","text":"Requires genbase>=0.2.2 Renamed pyproject.toml to .portray to avoid build errors Made fastcountvectorizer optional","title":"Changed"},{"location":"CHANGELOG/#fixed_1","text":"Bugfix when installing package, by moving __version___ to /_version.py","title":"Fixed"},{"location":"CHANGELOG/#058-2021-12-02","text":"","title":"0.5.8 - 2021-12-02"},{"location":"CHANGELOG/#added_1","text":"get_meta_descriptors() to get type/subtype/method from meta","title":"Added"},{"location":"CHANGELOG/#changed_2","text":"Requires genbase>=0.1.13","title":"Changed"},{"location":"CHANGELOG/#fixed_2","text":"Bugfix in MMDCritic for prototype indices Bugfix in TRANSLATION_DICT","title":"Fixed"},{"location":"CHANGELOG/#057-2021-12-01","text":"","title":"0.5.7 - 2021-12-01"},{"location":"CHANGELOG/#added_2","text":"Return type for Instances Rendering of Instances Rendering of FeatureList Extended rendering of render_subtitle()","title":"Added"},{"location":"CHANGELOG/#changed_3","text":"Ensure MMDCritic / KMedoids returns Instances Requires genbase>=0.1.11","title":"Changed"},{"location":"CHANGELOG/#fixed_3","text":"Bugfix of instance identifier in PrototypeSampler._select_from_provider()","title":"Fixed"},{"location":"CHANGELOG/#056-2021-11-30","text":"","title":"0.5.6 - 2021-11-30"},{"location":"CHANGELOG/#added_3","text":"Added meta information with genbase.MetaInfo Rendering with and extended genbase.Render","title":"Added"},{"location":"CHANGELOG/#changed_4","text":"Moved Readable to genbase Use genbase.SeedMixin for seeds Use genbase.internationalization for internationalization Requires genbase>=0.1.10","title":"Changed"},{"location":"CHANGELOG/#fixed_4","text":"Selected features are in order in FeatureList","title":"Fixed"},{"location":"CHANGELOG/#055-2021-11-17","text":"","title":"0.5.5 - 2021-11-17"},{"location":"CHANGELOG/#changed_5","text":"TokenFrequency and TokenInformation now use the faster fastcountvectorizer implementation","title":"Changed"},{"location":"CHANGELOG/#fixed_5","text":"Bugfixes in return type of TokenFrequency and TokenInformation","title":"Fixed"},{"location":"CHANGELOG/#054-2021-10-27","text":"","title":"0.5.4 - 2021-10-27"},{"location":"CHANGELOG/#fixed_6","text":"Bugfixes in local explanation return types","title":"Fixed"},{"location":"CHANGELOG/#053-2021-10-19","text":"","title":"0.5.3 - 2021-10-19"},{"location":"CHANGELOG/#fixed_7","text":"Made alpha optional in LinearSurrogate Added skope-rules dependency to setup.py","title":"Fixed"},{"location":"CHANGELOG/#052-2021-10-05","text":"","title":"0.5.2 - 2021-10-05"},{"location":"CHANGELOG/#fixed_8","text":"Hotfix in FeatureSelector._information_criterion()","title":"Fixed"},{"location":"CHANGELOG/#051-2021-10-05","text":"","title":"0.5.1 - 2021-10-05"},{"location":"CHANGELOG/#added_4","text":"Added text_explainability.data.from_list","title":"Added"},{"location":"CHANGELOG/#changed_6","text":"Added example results in README.md","title":"Changed"},{"location":"CHANGELOG/#fixed_9","text":"Added new methods and classes to __init__.py","title":"Fixed"},{"location":"CHANGELOG/#050-2021-10-04","text":"","title":"0.5.0 - 2021-10-04"},{"location":"CHANGELOG/#added_5","text":"Security testing with bandit More locale translations Wrappers around instancelib in text_explainability.data and text_explainability.model","title":"Added"},{"location":"CHANGELOG/#changed_7","text":"Extended description in README.md Changed example usage to fit workflow changes Logo link in README.md","title":"Changed"},{"location":"CHANGELOG/#fixed_10","text":"Bugfixes in MMDCritic Bugfixes in KernelSHAP","title":"Fixed"},{"location":"CHANGELOG/#046-2021-10-02","text":"","title":"0.4.6 - 2021-10-02"},{"location":"CHANGELOG/#added_6","text":"External documentation Documentation styling Citation information","title":"Added"},{"location":"CHANGELOG/#changed_8","text":"Word tokenizer can now combine tokens in curly bracket when setting exclude_curly_brackets=True","title":"Changed"},{"location":"CHANGELOG/#045-2021-09-24","text":"","title":"0.4.5 - 2021-09-24"},{"location":"CHANGELOG/#added_7","text":"Decorator to allow strings to be converted into TextInstances Decorator to ensure TextInstances are tokenized when required","title":"Added"},{"location":"CHANGELOG/#fixed_11","text":"Typing fixes","title":"Fixed"},{"location":"CHANGELOG/#044-2021-09-23","text":"","title":"0.4.4 - 2021-09-23"},{"location":"CHANGELOG/#added_8","text":"Character-level tokenizer/detokenizer","title":"Added"},{"location":"CHANGELOG/#043-2021-09-20","text":"","title":"0.4.3 - 2021-09-20"},{"location":"CHANGELOG/#added_9","text":"New embeddings not requiring internet ( CountVectorizer , TfidfVectorizer ) Rules return type First version of local rules using SkopeRules More test cases","title":"Added"},{"location":"CHANGELOG/#changed_9","text":"New default embedding method for MMDCritic and KMedoids Version moved to __init__.py New README.md layout Updates to Anchor local explanations Added random state in example_usage to ensure reproducibility","title":"Changed"},{"location":"CHANGELOG/#042-2021-09-13","text":"","title":"0.4.2 - 2021-09-13"},{"location":"CHANGELOG/#fixed_12","text":"Hotfix to fix predict_proba usage","title":"Fixed"},{"location":"CHANGELOG/#041-2021-09-13","text":"","title":"0.4.1 - 2021-09-13"},{"location":"CHANGELOG/#fixed_13","text":"Hotfix to make dependency on internet optional","title":"Fixed"},{"location":"CHANGELOG/#040-2021-09-13","text":"","title":"0.4.0 - 2021-09-13"},{"location":"CHANGELOG/#added_10","text":"Initial support for embeddings/vectors Support for dimensionality reduction Initial implementation of MMD-Critic Initial implementation of labelwise MMD-Critic Initial implementation of prototype selection using k-Medoids","title":"Added"},{"location":"CHANGELOG/#changed_10","text":"Updated README.md","title":"Changed"},{"location":"CHANGELOG/#038-2021-09-07","text":"","title":"0.3.8 - 2021-09-07"},{"location":"CHANGELOG/#changed_11","text":"Support for dimensionality reduction","title":"Changed"},{"location":"CHANGELOG/#fixed_14","text":"Bugfix in including locale/*.json files during setup","title":"Fixed"},{"location":"CHANGELOG/#037-2021-09-07","text":"","title":"0.3.7 - 2021-09-07"},{"location":"CHANGELOG/#added_11","text":"Dependencies for package","title":"Added"},{"location":"CHANGELOG/#036-2021-09-07","text":"","title":"0.3.6 - 2021-09-07"},{"location":"CHANGELOG/#added_12","text":"PyPI release script to .gitignore Badges to README.md Added dependencies to setup.py","title":"Added"},{"location":"CHANGELOG/#035-2021-09-03","text":"","title":"0.3.5 - 2021-09-03"},{"location":"CHANGELOG/#changed_12","text":"Locale changed to .json format, to remove optional dependency","title":"Changed"},{"location":"CHANGELOG/#fixed_15","text":"Bugfix for getting key in TokenFrequency Bugfixes in FeatureAttribution return type Bugfixes in i18n","title":"Fixed"},{"location":"CHANGELOG/#034-2021-08-18","text":"","title":"0.3.4 - 2021-08-18"},{"location":"CHANGELOG/#changed_13","text":"External logo url","title":"Changed"},{"location":"CHANGELOG/#fixed_16","text":"Hotfix in FeatureAttribution","title":"Fixed"},{"location":"CHANGELOG/#033-2021-08-18","text":"","title":"0.3.3 - 2021-08-18"},{"location":"CHANGELOG/#added_13","text":"Updated to support instancelib==0.3.1.2 i18n internationalization support CHANGELOG.md","title":"Added"},{"location":"CHANGELOG/#changed_14","text":"Additional samples in example dataset","title":"Changed"},{"location":"CHANGELOG/#fixed_17","text":"Bugfixes for LIME and FeatureAttribution return type","title":"Fixed"},{"location":"CHANGELOG/#032-2021-07-27","text":"","title":"0.3.2 - 2021-07-27"},{"location":"CHANGELOG/#added_14","text":"Initial support for Foil Trees Logo in documentation","title":"Added"},{"location":"CHANGELOG/#changed_15","text":"Improved documentation","title":"Changed"},{"location":"CHANGELOG/#031-2021-07-23","text":"","title":"0.3.1 - 2021-07-23"},{"location":"CHANGELOG/#added_15","text":"flake8 linting CI/CD Pipeline Run test scripts","title":"Added"},{"location":"CHANGELOG/#030-2021-07-20","text":"","title":"0.3.0 - 2021-07-20"},{"location":"CHANGELOG/#added_16","text":"Updated to support instancelib==0.3.0.0","title":"Added"},{"location":"CHANGELOG/#changed_16","text":"Improved documentation global_explanation classes have equal return types","title":"Changed"},{"location":"CHANGELOG/#02-2021-06-22","text":"","title":"0.2 - 2021-06-22"},{"location":"CHANGELOG/#added_17","text":"LICENSE.md Updated to support instancelib==0.2.3.1","title":"Added"},{"location":"CHANGELOG/#changed_17","text":"Module description","title":"Changed"},{"location":"CHANGELOG/#01-2021-05-28","text":"","title":"0.1 - 2021-05-28"},{"location":"CHANGELOG/#added_18","text":"README.md Example usage Local explanation classes (LIME, KernelSHAP) Global explanation classes Data augmentation/sampling Feature selection Local surrogates Tokenization git setup","title":"Added"},{"location":"example_usage/","text":"Example Usage Dependencies text_explainability uses instances and machine learning models wrapped with the InstanceLib library. For your convenience, we wrap some instancelib functions in text_explainability.data and explainability.model . 1 2 from text_explainability.data import import_data , train_test_split , from_string from text_explainability.model import import_model Dataset and model As a dummy black-box model, we use the example dataset in ./datasets/test.csv and train a machine learning model on it with scikit-learn . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from sklearn.pipeline import Pipeline from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.ensemble import RandomForestClassifier # Create train/test dataset env = import_data ( './datasets/test.csv' , data_cols = 'fulltext' , label_cols = 'label' ) env = train_test_split ( env , train_size = 0.70 ) # Create sklearn model with pipeline pipeline = Pipeline ([( 'tfidf' , TfidfVectorizer ( use_idf = True )), ( 'rf' , RandomForestClassifier ( random_state = 0 ))]) # Build and fit (train) model model = import_model ( pipeline , environment = env ) Using Text Explainability Text Explainability is used for local explanations (explaining a single prediction) or global explanations (explaining general dataset/model behavior). Local explanations Popular local explanations include LIME , KernelSHAP , local decion trees ( LocalTree ), local decision rules ( LocalRules ) and FoilTree . First, let us create a sample to explain: 1 2 3 from text_explainability.data import from_string sample = from_string ( 'Dit is zeer positieve of negatieve proef... Of toch negatief?' ) Next, the prediction of model on sample can be explained by generating neighborhood data ( text_explainability.data.augmentation.TokenReplacement ), used by LIME , LocalTree , FoilTree and KernelSHAP : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from text_explainability import LIME , LocalTree , FoilTree , KernelSHAP # LIME explainer for `sample` on `model` explainer = LIME ( env ) explainer ( sample , model , labels = [ 'neutraal' , 'positief' ]) . scores # SHAP explanation for `sample` on `model`, limited to 4 features KernelSHAP ( label_names = labelprovider )( sample , model , n_samples = 50 , l1_reg = 4 ) # Local tree explainer for `sample` on `model` (non-weighted neighborhood data) LocalTree ()( sample , model , weigh_samples = False ) # Contrastive local tree explainer for `sample` on `model` (why not 'positief'?) FoilTree ()( sample , model , foil_fn = 'positief' ) . rules # LocalRules on `model` (why 'positief'?) LocalRules ()( sample , model , foil_fn = 'negatief' , n_samples = 100 ) . rules Global explanations Global explanations provide information on the dataset and its ground-truth labels, or the dataset and corresponding predictions by the model . Example global explanations are TokenFrequency (the frequency of each token per label/class/bucket) or TokenInformation (how informative each token is for predicting the various labels). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from text_explainability import TokenFrequency , TokenInformation # Global word frequency explanation on ground-truth labels tf = TokenFrequency ( env . dataset ) tf ( labelprovider = env . labels , explain_model = False , k = 10 ) . scores # Global word frequency explanation on model predictions tf ( model = model , explain_model = True , k = 3 , filter_words = PUNCTUATION ) # Token information for dataset ti = TokenInformation ( env . dataset ) ti ( labelprovider = env . labels , explain_model = False , k = 50 ) . scores # Token information for model ti ( model = model , explain_model = True , k = 50 , filter_words = PUNCTUATION ) Global explanation: Explanation by example Explanations by example provide information on a dataset (e.g. the test set) or subsets thereof (e.g. all training instances with label 0) by showing representative instances. Examples of representative instances are prototypes ( n most representative instances, e.g. of a class) and criticsms ( n instances not well represented by prototypes). Example explanations by example are KMedoids (using the k-Medoids algorithm to extract prototypes) and MMDCritic (extracting prototypes and corresponding criticisms). In addition, each of these can be performed labelwise (e.g. for the ground-truth labels in a labelprovider or for each models' predicted class). 1 2 3 4 5 6 7 8 9 10 11 12 13 from text_explainability import KMedoids , MMDCritic , LabelwiseMMDCritic # Extract top-2 prototypes with KMedoids KMedoids ( env . dataset ) . prototypes ( n = 2 ) # Extract top-2 prototypes and top-2 criticisms label with MMDCritic MMDCritic ( env . dataset )( n_prototypes = 2 , n_criticisms = 2 ) # Extract 1 prototype for each ground-truth label with MMDCritic LabelwiseMMDCritic ( env . dataset , labelprovider ) . prototypes ( n = 1 ) # Extract 1 prototype and 2 criticisms for each predicted label with MMDCritic LabelwiseMMDCritic ( env . dataset , model )( n_prototypes = 1 , n_criticisms = 2 )","title":"Example usage"},{"location":"example_usage/#example-usage","text":"","title":"Example Usage"},{"location":"example_usage/#dependencies","text":"text_explainability uses instances and machine learning models wrapped with the InstanceLib library. For your convenience, we wrap some instancelib functions in text_explainability.data and explainability.model . 1 2 from text_explainability.data import import_data , train_test_split , from_string from text_explainability.model import import_model","title":"Dependencies"},{"location":"example_usage/#dataset-and-model","text":"As a dummy black-box model, we use the example dataset in ./datasets/test.csv and train a machine learning model on it with scikit-learn . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from sklearn.pipeline import Pipeline from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.ensemble import RandomForestClassifier # Create train/test dataset env = import_data ( './datasets/test.csv' , data_cols = 'fulltext' , label_cols = 'label' ) env = train_test_split ( env , train_size = 0.70 ) # Create sklearn model with pipeline pipeline = Pipeline ([( 'tfidf' , TfidfVectorizer ( use_idf = True )), ( 'rf' , RandomForestClassifier ( random_state = 0 ))]) # Build and fit (train) model model = import_model ( pipeline , environment = env )","title":"Dataset and model"},{"location":"example_usage/#using-text-explainability","text":"Text Explainability is used for local explanations (explaining a single prediction) or global explanations (explaining general dataset/model behavior).","title":"Using Text Explainability"},{"location":"example_usage/#local-explanations","text":"Popular local explanations include LIME , KernelSHAP , local decion trees ( LocalTree ), local decision rules ( LocalRules ) and FoilTree . First, let us create a sample to explain: 1 2 3 from text_explainability.data import from_string sample = from_string ( 'Dit is zeer positieve of negatieve proef... Of toch negatief?' ) Next, the prediction of model on sample can be explained by generating neighborhood data ( text_explainability.data.augmentation.TokenReplacement ), used by LIME , LocalTree , FoilTree and KernelSHAP : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from text_explainability import LIME , LocalTree , FoilTree , KernelSHAP # LIME explainer for `sample` on `model` explainer = LIME ( env ) explainer ( sample , model , labels = [ 'neutraal' , 'positief' ]) . scores # SHAP explanation for `sample` on `model`, limited to 4 features KernelSHAP ( label_names = labelprovider )( sample , model , n_samples = 50 , l1_reg = 4 ) # Local tree explainer for `sample` on `model` (non-weighted neighborhood data) LocalTree ()( sample , model , weigh_samples = False ) # Contrastive local tree explainer for `sample` on `model` (why not 'positief'?) FoilTree ()( sample , model , foil_fn = 'positief' ) . rules # LocalRules on `model` (why 'positief'?) LocalRules ()( sample , model , foil_fn = 'negatief' , n_samples = 100 ) . rules","title":"Local explanations"},{"location":"example_usage/#global-explanations","text":"Global explanations provide information on the dataset and its ground-truth labels, or the dataset and corresponding predictions by the model . Example global explanations are TokenFrequency (the frequency of each token per label/class/bucket) or TokenInformation (how informative each token is for predicting the various labels). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from text_explainability import TokenFrequency , TokenInformation # Global word frequency explanation on ground-truth labels tf = TokenFrequency ( env . dataset ) tf ( labelprovider = env . labels , explain_model = False , k = 10 ) . scores # Global word frequency explanation on model predictions tf ( model = model , explain_model = True , k = 3 , filter_words = PUNCTUATION ) # Token information for dataset ti = TokenInformation ( env . dataset ) ti ( labelprovider = env . labels , explain_model = False , k = 50 ) . scores # Token information for model ti ( model = model , explain_model = True , k = 50 , filter_words = PUNCTUATION )","title":"Global explanations"},{"location":"example_usage/#global-explanation-explanation-by-example","text":"Explanations by example provide information on a dataset (e.g. the test set) or subsets thereof (e.g. all training instances with label 0) by showing representative instances. Examples of representative instances are prototypes ( n most representative instances, e.g. of a class) and criticsms ( n instances not well represented by prototypes). Example explanations by example are KMedoids (using the k-Medoids algorithm to extract prototypes) and MMDCritic (extracting prototypes and corresponding criticisms). In addition, each of these can be performed labelwise (e.g. for the ground-truth labels in a labelprovider or for each models' predicted class). 1 2 3 4 5 6 7 8 9 10 11 12 13 from text_explainability import KMedoids , MMDCritic , LabelwiseMMDCritic # Extract top-2 prototypes with KMedoids KMedoids ( env . dataset ) . prototypes ( n = 2 ) # Extract top-2 prototypes and top-2 criticisms label with MMDCritic MMDCritic ( env . dataset )( n_prototypes = 2 , n_criticisms = 2 ) # Extract 1 prototype for each ground-truth label with MMDCritic LabelwiseMMDCritic ( env . dataset , labelprovider ) . prototypes ( n = 1 ) # Extract 1 prototype and 2 criticisms for each predicted label with MMDCritic LabelwiseMMDCritic ( env . dataset , model )( n_prototypes = 1 , n_criticisms = 2 )","title":"Global explanation: Explanation by example"},{"location":"docs/installation/","text":"Installation Installation of text_explainability requires Python 3.8 or higher. 1. Python installation Install Python on your operating system using the Python Setup and Usage guide. 2. Installing text_explainability text_explainability can be installed: using pip : pip3 install (released on PyPI ) locally : cloning the repository and using python3 setup.py install Using pip Open up a terminal (Linux / macOS) or cmd.exe / powershell.exe (Windows) Run the command: pip3 install text_explainability , or pip install text_explainability . 1 2 3 4 5 user@terminal:~$ pip3 install text_explainability Collecting text_explainability ... Installing collected packages: text-explainability Successfully installed text-explainability Speeding up the explanation-generation process can be done by using pip3 install text_explainability[fast] or having fastcountvectorizer installed. Locally Download the folder from GitLab/GitHub : Clone this repository, or Download it as a .zip file and extract it. Open up a terminal (Linux / macOS) or cmd.exe / powershell.exe (Windows) and navigate to the folder you downloaded text_explainability in. In the main folder (containing the setup.py file) run: python3 setup.py install , or python setup.py install . 1 2 3 4 5 6 7 user@terminal:~$ cd ~/text_explainability user@terminal:~/text_explanability$ python3 setup.py install running install running bdist_egg running egg_info ... Finished processing dependencies for text-explainability","title":"Installation"},{"location":"docs/installation/#installation","text":"Installation of text_explainability requires Python 3.8 or higher.","title":"Installation"},{"location":"docs/installation/#1-python-installation","text":"Install Python on your operating system using the Python Setup and Usage guide.","title":"1. Python installation"},{"location":"docs/installation/#2-installing-text_explainability","text":"text_explainability can be installed: using pip : pip3 install (released on PyPI ) locally : cloning the repository and using python3 setup.py install","title":"2. Installing text_explainability"},{"location":"docs/installation/#using-pip","text":"Open up a terminal (Linux / macOS) or cmd.exe / powershell.exe (Windows) Run the command: pip3 install text_explainability , or pip install text_explainability . 1 2 3 4 5 user@terminal:~$ pip3 install text_explainability Collecting text_explainability ... Installing collected packages: text-explainability Successfully installed text-explainability Speeding up the explanation-generation process can be done by using pip3 install text_explainability[fast] or having fastcountvectorizer installed.","title":"Using pip"},{"location":"docs/installation/#locally","text":"Download the folder from GitLab/GitHub : Clone this repository, or Download it as a .zip file and extract it. Open up a terminal (Linux / macOS) or cmd.exe / powershell.exe (Windows) and navigate to the folder you downloaded text_explainability in. In the main folder (containing the setup.py file) run: python3 setup.py install , or python setup.py install . 1 2 3 4 5 6 7 user@terminal:~$ cd ~/text_explainability user@terminal:~/text_explanability$ python3 setup.py install running install running bdist_egg running egg_info ... Finished processing dependencies for text-explainability","title":"Locally"},{"location":"reference/text_explainability/","text":"Module text_explainability None None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 from text_explainability._version import __version__, __version_info__ from text_explainability.data import from_string, import_data, train_test_split from text_explainability.global_explanation import (KMedoids, LabelwiseKMedoids, LabelwiseMMDCritic, MMDCritic, TokenFrequency, TokenInformation) from text_explainability.local_explanation import (LIME, Anchor, KernelSHAP, LocalRules, LocalTree) from text_explainability.model import import_model from text_explainability.utils import (character_detokenizer, character_tokenizer, default_detokenizer, default_tokenizer, word_detokenizer, word_tokenizer) Sub-modules text_explainability.data text_explainability.decorators text_explainability.generation text_explainability.global_explanation text_explainability.local_explanation text_explainability.model text_explainability.ui text_explainability.utils","title":"Index"},{"location":"reference/text_explainability/#module-text_explainability","text":"None None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 from text_explainability._version import __version__, __version_info__ from text_explainability.data import from_string, import_data, train_test_split from text_explainability.global_explanation import (KMedoids, LabelwiseKMedoids, LabelwiseMMDCritic, MMDCritic, TokenFrequency, TokenInformation) from text_explainability.local_explanation import (LIME, Anchor, KernelSHAP, LocalRules, LocalTree) from text_explainability.model import import_model from text_explainability.utils import (character_detokenizer, character_tokenizer, default_detokenizer, default_tokenizer, word_detokenizer, word_tokenizer)","title":"Module text_explainability"},{"location":"reference/text_explainability/#sub-modules","text":"text_explainability.data text_explainability.decorators text_explainability.generation text_explainability.global_explanation text_explainability.local_explanation text_explainability.model text_explainability.ui text_explainability.utils","title":"Sub-modules"},{"location":"reference/text_explainability/decorators/","text":"Module text_explainability.decorators Function decorators to ensure functions are fool-proof en readable. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 \"\"\"Function decorators to ensure functions are fool-proof en readable.\"\"\" import inspect from functools import partial, wraps from .data import from_string from .utils import default_tokenizer def text_instance(func=None, *, tokenize: bool = False): \"\"\"Decorator to convert an accidentally passed string to a TextInstance.\"\"\" if func is None: return partial(text_instance, tokenize=tokenize) def str_to_text_instance(arg): if isinstance(arg, str): arg = from_string(arg) if tokenize and not arg.tokenized: arg.tokenized = default_tokenizer(arg.data) return arg @wraps(func) def inner(*args, **kwargs): possible_args = [i for i, t in enumerate(inspect.signature(func).parameters.values()) if 'TextInstance' in str(t)] if possible_args: args = tuple(str_to_text_instance(a) if j in possible_args else a for j, a in enumerate(list(args))) return func(*args, **kwargs) return inner Functions text_instance 1 2 3 4 5 def text_instance ( func = None , * , tokenize : bool = False ) Decorator to convert an accidentally passed string to a TextInstance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def text_instance(func=None, *, tokenize: bool = False): \"\"\"Decorator to convert an accidentally passed string to a TextInstance.\"\"\" if func is None: return partial(text_instance, tokenize=tokenize) def str_to_text_instance(arg): if isinstance(arg, str): arg = from_string(arg) if tokenize and not arg.tokenized: arg.tokenized = default_tokenizer(arg.data) return arg @wraps(func) def inner(*args, **kwargs): possible_args = [i for i, t in enumerate(inspect.signature(func).parameters.values()) if 'TextInstance' in str(t)] if possible_args: args = tuple(str_to_text_instance(a) if j in possible_args else a for j, a in enumerate(list(args))) return func(*args, **kwargs) return inner","title":"Decorators"},{"location":"reference/text_explainability/decorators/#module-text_explainabilitydecorators","text":"Function decorators to ensure functions are fool-proof en readable. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 \"\"\"Function decorators to ensure functions are fool-proof en readable.\"\"\" import inspect from functools import partial, wraps from .data import from_string from .utils import default_tokenizer def text_instance(func=None, *, tokenize: bool = False): \"\"\"Decorator to convert an accidentally passed string to a TextInstance.\"\"\" if func is None: return partial(text_instance, tokenize=tokenize) def str_to_text_instance(arg): if isinstance(arg, str): arg = from_string(arg) if tokenize and not arg.tokenized: arg.tokenized = default_tokenizer(arg.data) return arg @wraps(func) def inner(*args, **kwargs): possible_args = [i for i, t in enumerate(inspect.signature(func).parameters.values()) if 'TextInstance' in str(t)] if possible_args: args = tuple(str_to_text_instance(a) if j in possible_args else a for j, a in enumerate(list(args))) return func(*args, **kwargs) return inner","title":"Module text_explainability.decorators"},{"location":"reference/text_explainability/decorators/#functions","text":"","title":"Functions"},{"location":"reference/text_explainability/decorators/#text_instance","text":"1 2 3 4 5 def text_instance ( func = None , * , tokenize : bool = False ) Decorator to convert an accidentally passed string to a TextInstance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def text_instance(func=None, *, tokenize: bool = False): \"\"\"Decorator to convert an accidentally passed string to a TextInstance.\"\"\" if func is None: return partial(text_instance, tokenize=tokenize) def str_to_text_instance(arg): if isinstance(arg, str): arg = from_string(arg) if tokenize and not arg.tokenized: arg.tokenized = default_tokenizer(arg.data) return arg @wraps(func) def inner(*args, **kwargs): possible_args = [i for i, t in enumerate(inspect.signature(func).parameters.values()) if 'TextInstance' in str(t)] if possible_args: args = tuple(str_to_text_instance(a) if j in possible_args else a for j, a in enumerate(list(args))) return func(*args, **kwargs) return inner","title":"text_instance"},{"location":"reference/text_explainability/global_explanation/","text":"Module text_explainability.global_explanation Global explanations explain the whole dataset or model behavior on that dataset. Todo: 1 2 3 * More support for sampling methods * add support for other tasks than classification (e.g. regression, multi-label classification) * partial dependence plots? https://scikit-learn.org/stable/modules/classes.html#module-sklearn.inspection View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 \"\"\"Global explanations explain the whole dataset or model behavior on that dataset. Todo: * More support for sampling methods * add support for other tasks than classification (e.g. regression, multi-label classification) * partial dependence plots? https://scikit-learn.org/stable/modules/classes.html#module-sklearn.inspection \"\"\" from typing import Any, FrozenSet, List, Optional, Sequence, Tuple, Union import numpy as np from genbase import Readable, SeedMixin, add_callargs, translate_list from instancelib import InstanceProvider from instancelib.instances.text import TextInstance from instancelib.labels import LabelProvider from instancelib.machinelearning import AbstractClassifier from sklearn.feature_selection import mutual_info_classif from .data.sampling import KMedoids as _Kmedoids from .data.sampling import LabelwiseKMedoids as _LabelwiseKMedoids from .data.sampling import LabelwiseMMDCritic as _LabelwiseMMDCritic from .data.sampling import MMDCritic as _MMDCritic from .data.sampling import PrototypeSampler from .generation.return_types import FeatureList, Instances try: from fastcountvectorizer import \\ FastCountVectorizer as \\ CountVectorizer # use fastcountvectorizer if available except ImportError: from sklearn.feature_extraction.text import CountVectorizer class GlobalExplanation(Readable, SeedMixin): def __init__(self, provider: InstanceProvider[TextInstance, Any, str, Any, str], seed: int = 0): \"\"\"Generic wrapper from global explanations (explain whole dataset or model). Args: provider (InstanceProvider[TextInstance, Any, str, Any, str]): Dataset to perform explanation on. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__() self.provider = provider self._seed = self._original_seed = seed def get_data(self) -> InstanceProvider: \"\"\"Easy access to data. Returns: InstanceProvider: Easily accessible dataset. \"\"\" return self.provider def predict(self, model: AbstractClassifier) -> Union[Sequence[FrozenSet[str]], np.ndarray]: \"\"\"Apply predict function of model to data. Args: model (AbstractClassifier): Model to apply predictions with. Returns: Union[Sequence[FrozenSet[str]], np.ndarray]: Labels for dataset according to model. \"\"\" return model.predict(self.get_data()) def get_instances_labels(self, model: Optional[AbstractClassifier], labelprovider: Optional[LabelProvider], explain_model: bool = True) -> Tuple[InstanceProvider, np.ndarray]: \"\"\"Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Args: model (Optional[AbstractClassifier]): Model to perform predictions with. labelprovider (Optional[LabelProvider]): Ground-truth labels. explain_model (bool, optional): Whether to explain using the `model` labels (True) or `labelprovider` labels (False). Defaults to True. Raises: ValueError: if explain_model = True provide a model, and if False provide a labelprovider. Returns: Tuple[InstanceProvider, np.ndarray]: Instances and corresponding labels \"\"\" if explain_model and model is None: raise ValueError('Provide a model to explain its predictions, or set `explain_predictions` to False') elif not explain_model and labelprovider is None: raise ValueError('Provide a labelprovider to explain ground-truth labels, ', 'or set `explain_predictions` to True') instances = self.get_data() labels = model.predict(instances) if explain_model \\ else [next(iter(labelprovider.get_labels(k))) for k in instances] if len(labels) > 0 and isinstance(labels[0], tuple) and isinstance(labels[0][-1], frozenset): labels = ['-'.join(list(x)) for _, x in labels] return instances, np.array(labels) def explain(self, *args, **kwargs): return self(*args, **kwargs) class TokenFrequency(GlobalExplanation): @add_callargs def __call__(self, model: Optional[AbstractClassifier] = None, labelprovider: Optional[LabelProvider] = None, explain_model: bool = True, labelwise: bool = True, k: Optional[int] = None, filter_words: List[str] = translate_list('stopwords'), lower: bool = True, **count_vectorizer_kwargs) -> FeatureList: \"\"\"Show the top-k number of tokens for each ground-truth or predicted label. Args: model (Optional[AbstractClassifier], optional): Predictive model to explain. Defaults to None. labelprovider (Optional[LabelProvider], optional): Ground-truth labels to explain. Defaults to None. explain_model (bool, optional): Whether to explain the model (True) or ground-truth labels (False). Defaults to True. labelwise (bool, optional): Whether to summarize the counts for each label seperately. Defaults to True. k (Optional[int], optional): Limit to the top-k words per label, or all words if None. Defaults to None. filter_words (List[str], optional): Words to filter out from top-k. Defaults to ['de', 'het', 'een']. lower (bool, optional): Whether to make all tokens lowercase. Defaults to True. **count_vectorizer_kwargs: Optional arguments passed to `CountVectorizer`/`FastCountVectorizer`. Returns: FeatureList: Each label with corresponding top words and their frequency \"\"\" type, subtype = 'global_explanation', 'token_frequency' callargs = count_vectorizer_kwargs.pop('__callargs__', None) instances, labels = self.get_instances_labels(model, labelprovider, explain_model=explain_model) def top_k_counts(instances_to_fit): cv = CountVectorizer(**count_vectorizer_kwargs) counts = cv.fit_transform([str.lower(d) for d in instances_to_fit] if lower else instances_to_fit) counts = np.ravel(counts.sum(axis=0)) return sorted([(w, counts[v]) for w, v in cv.vocabulary_.items() if k not in filter_words], key=lambda x: x[1], reverse=True)[:k] if labelwise: # TO-DO improve beyond classification, e.g. buckets for regression? label_names = np.unique(labels) label_ids = [i for i, _ in enumerate(label_names)] def counts_by_label(label): return zip(*top_k_counts([instances[instances.key_list[idx]].data for idx in np.where(labels == label)[0]])) used_features, scores = zip(*[counts_by_label(label) for label in label_names]) return FeatureList(labels=label_ids, labelset=label_names, used_features=dict(zip(label_ids, used_features)), scores=dict(zip(label_ids, scores)), type=type, subtype=subtype, callargs=callargs) used_features, scores = zip(*top_k_counts(instances.all_data())) return FeatureList(used_features=used_features, scores=scores, type=type, subtype=subtype, callargs=callargs) class TokenInformation(GlobalExplanation): @add_callargs def __call__(self, model: Optional[AbstractClassifier] = None, labelprovider: Optional[LabelProvider] = None, explain_model: bool = True, # labelwise: bool = True, k: Optional[int] = None, filter_words: List[str] = translate_list('stopwords'), lower: bool = True, **count_vectorizer_kwargs) -> FeatureList: \"\"\"Show the top-k token mutual information for a dataset or model. Args: model (Optional[AbstractClassifier], optional): Predictive model to explain. Defaults to None. labelprovider (Optional[LabelProvider], optional): Ground-truth labels to explain. Defaults to None. explain_model (bool, optional): Whether to explain the model (True) or ground-truth labels (False). Defaults to True. k (Optional[int], optional): Limit to the top-k words per label, or all words if None. Defaults to None. filter_words (List[str], optional): Words to filter out from top-k. Defaults to ['de', 'het', 'een']. lower (bool, optional): Whether to make all tokens lowercase. Defaults to True. **count_vectorizer_kwargs: Keyword arguments to pass onto `CountVectorizer`/`FastCountVectorizer`. Returns: FeatureList: k labels, sorted based on their mutual information with the output (predictive model labels or ground-truth labels) \"\"\" callargs = count_vectorizer_kwargs.pop('__callargs__', None) instances, labels = self.get_instances_labels(model, labelprovider, explain_model=explain_model) cv = CountVectorizer(**count_vectorizer_kwargs) counts = cv.fit_transform([str.lower(d) for d in instances.all_data()] if lower else instances.all_data()) # TO-DO improve beyond classification # see https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html # #sklearn.feature_selection.mutual_info_regression mif = mutual_info_classif(counts, labels, discrete_features=True, random_state=self.seed) feature_names = cv.get_feature_names() res = list(map(tuple, zip(feature_names, mif))) res_sorted = list(sorted([(w, v) for w, v in res if w not in filter_words], key=lambda x: x[1], reverse=True))[:k] used_features, scores = zip(*res_sorted) return FeatureList(used_features=used_features, scores=scores, type='global_explanation', subtype='token_information', method='mutual_information', callargs=callargs) class PrototypeWrapper: def __init__(self, prototype_sampler: PrototypeSampler, *args, method: Optional[str] = None, subtype: str = 'prototypes', **kwargs): self.prototype_sampler = prototype_sampler(*args, **kwargs) self.type = 'global_explanation' self.subtype = subtype self.method = method self.labelwise = False @add_callargs def __call__(self, *args, **kwargs) -> Instances: callargs = kwargs.pop('__callargs__', None) instances = self.prototype_sampler.__call__(*args, **kwargs) return Instances(instances=instances if isinstance(instances, dict) else {'prototypes': instances}, type=self.type, subtype=self.subtype, method=self.method, callargs=callargs, labelwise=self.labelwise) @add_callargs def prototypes(self, *args, **kwargs) -> Instances: callargs = kwargs.pop('__callargs__', None) return Instances(instances={'prototypes': self.prototype_sampler.prototypes(*args, **kwargs)}, type=self.type, subtype='prototypes', method=self.method, callargs=callargs, labelwise=self.labelwise) class KMedoids(PrototypeWrapper): def __init__(self, *args, **kwargs): super().__init__(_Kmedoids, *args, method='kmedoids', subtype='prototypes', **kwargs) class LabelwiseKMedoids(PrototypeWrapper): def __init__(self, *args, **kwargs): super().__init__(_LabelwiseKMedoids, *args, method='kmedoids', subtype='prototypes', **kwargs) self.labelwise = True class PrototypeCriticismWrapper(PrototypeWrapper): def __init__(self, prototype_sampler: PrototypeSampler, *args, method: Optional[str] = None, subtype: str = 'prototypes_&_criticisms', **kwargs): super().__init__(prototype_sampler, *args, method=method, subtype=subtype, **kwargs) @add_callargs def criticisms(self, *args, **kwargs) -> Instances: callargs = kwargs.pop('__callargs__', None) return Instances(instances={'criticisms': self.prototype_sampler.criticisms(*args, **kwargs)}, type=self.type, subtype='criticisms', method=self.method, callargs=callargs, labelwise=self.labelwise) class MMDCritic(PrototypeCriticismWrapper): def __init__(self, *args, **kwargs): super().__init__(_MMDCritic, *args, method='mmdcritic', subtype='prototypes_&_criticisms', **kwargs) class LabelwiseMMDCritic(PrototypeCriticismWrapper): def __init__(self, *args, **kwargs): super().__init__(_LabelwiseMMDCritic, *args, method='mmdcritic', subtype='prototypes_&_criticisms', **kwargs) self.labelwise = True Classes GlobalExplanation 1 2 3 4 class GlobalExplanation ( provider : instancelib . instances . base . InstanceProvider [ instancelib . instances . text . TextInstance , typing . Any , str , typing . Any , str ], seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 class GlobalExplanation(Readable, SeedMixin): def __init__(self, provider: InstanceProvider[TextInstance, Any, str, Any, str], seed: int = 0): \"\"\"Generic wrapper from global explanations (explain whole dataset or model). Args: provider (InstanceProvider[TextInstance, Any, str, Any, str]): Dataset to perform explanation on. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__() self.provider = provider self._seed = self._original_seed = seed def get_data(self) -> InstanceProvider: \"\"\"Easy access to data. Returns: InstanceProvider: Easily accessible dataset. \"\"\" return self.provider def predict(self, model: AbstractClassifier) -> Union[Sequence[FrozenSet[str]], np.ndarray]: \"\"\"Apply predict function of model to data. Args: model (AbstractClassifier): Model to apply predictions with. Returns: Union[Sequence[FrozenSet[str]], np.ndarray]: Labels for dataset according to model. \"\"\" return model.predict(self.get_data()) def get_instances_labels(self, model: Optional[AbstractClassifier], labelprovider: Optional[LabelProvider], explain_model: bool = True) -> Tuple[InstanceProvider, np.ndarray]: \"\"\"Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Args: model (Optional[AbstractClassifier]): Model to perform predictions with. labelprovider (Optional[LabelProvider]): Ground-truth labels. explain_model (bool, optional): Whether to explain using the `model` labels (True) or `labelprovider` labels (False). Defaults to True. Raises: ValueError: if explain_model = True provide a model, and if False provide a labelprovider. Returns: Tuple[InstanceProvider, np.ndarray]: Instances and corresponding labels \"\"\" if explain_model and model is None: raise ValueError('Provide a model to explain its predictions, or set `explain_predictions` to False') elif not explain_model and labelprovider is None: raise ValueError('Provide a labelprovider to explain ground-truth labels, ', 'or set `explain_predictions` to True') instances = self.get_data() labels = model.predict(instances) if explain_model \\ else [next(iter(labelprovider.get_labels(k))) for k in instances] if len(labels) > 0 and isinstance(labels[0], tuple) and isinstance(labels[0][-1], frozenset): labels = ['-'.join(list(x)) for _, x in labels] return instances, np.array(labels) def explain(self, *args, **kwargs): return self(*args, **kwargs) Ancestors (in MRO) genbase.Readable genbase.mixin.SeedMixin Descendants text_explainability.global_explanation.TokenFrequency text_explainability.global_explanation.TokenInformation Instance variables 1 seed Methods explain 1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs) get_data 1 2 3 def get_data ( self ) -> instancelib . instances . base . InstanceProvider Easy access to data. Returns: Type Description InstanceProvider Easily accessible dataset. View Source 1 2 3 4 5 6 7 8 9 10 11 def get_data(self) -> InstanceProvider: \"\"\"Easy access to data. Returns: InstanceProvider: Easily accessible dataset. \"\"\" return self.provider get_instances_labels 1 2 3 4 5 6 def get_instances_labels ( self , model : Optional [ instancelib . machinelearning . base . AbstractClassifier ], labelprovider : Optional [ instancelib . labels . base . LabelProvider ], explain_model : bool = True ) -> Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ] Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Parameters: Name Type Description Default model Optional[AbstractClassifier] Model to perform predictions with. None labelprovider Optional[LabelProvider] Ground-truth labels. None explain_model bool Whether to explain using the model labels (True) or labelprovider labels (False). Defaults to True. None Returns: Type Description Tuple[InstanceProvider, np.ndarray] Instances and corresponding labels Raises: Type Description ValueError if explain_model = True provide a model, and if False provide a labelprovider. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def get_instances_labels(self, model: Optional[AbstractClassifier], labelprovider: Optional[LabelProvider], explain_model: bool = True) -> Tuple[InstanceProvider, np.ndarray]: \"\"\"Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Args: model (Optional[AbstractClassifier]): Model to perform predictions with. labelprovider (Optional[LabelProvider]): Ground-truth labels. explain_model (bool, optional): Whether to explain using the `model` labels (True) or `labelprovider` labels (False). Defaults to True. Raises: ValueError: if explain_model = True provide a model, and if False provide a labelprovider. Returns: Tuple[InstanceProvider, np.ndarray]: Instances and corresponding labels \"\"\" if explain_model and model is None: raise ValueError('Provide a model to explain its predictions, or set `explain_predictions` to False') elif not explain_model and labelprovider is None: raise ValueError('Provide a labelprovider to explain ground-truth labels, ', 'or set `explain_predictions` to True') instances = self.get_data() labels = model.predict(instances) if explain_model \\ else [next(iter(labelprovider.get_labels(k))) for k in instances] if len(labels) > 0 and isinstance(labels[0], tuple) and isinstance(labels[0][-1], frozenset): labels = ['-'.join(list(x)) for _, x in labels] return instances, np.array(labels) predict 1 2 3 4 def predict ( self , model : instancelib . machinelearning . base . AbstractClassifier ) -> Union [ Sequence [ FrozenSet [ str ]], numpy . ndarray ] Apply predict function of model to data. Parameters: Name Type Description Default model AbstractClassifier Model to apply predictions with. None Returns: Type Description Union[Sequence[FrozenSet[str]], np.ndarray] Labels for dataset according to model. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def predict(self, model: AbstractClassifier) -> Union[Sequence[FrozenSet[str]], np.ndarray]: \"\"\"Apply predict function of model to data. Args: model (AbstractClassifier): Model to apply predictions with. Returns: Union[Sequence[FrozenSet[str]], np.ndarray]: Labels for dataset according to model. \"\"\" return model.predict(self.get_data()) reset_seed 1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self set_seed 1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed() KMedoids 1 2 3 4 class KMedoids ( * args , ** kwargs ) View Source 1 2 3 4 5 class KMedoids(PrototypeWrapper): def __init__(self, *args, **kwargs): super().__init__(_Kmedoids, *args, method='kmedoids', subtype='prototypes', **kwargs) Ancestors (in MRO) text_explainability.global_explanation.PrototypeWrapper Methods prototypes 1 2 3 4 def prototypes ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs) LabelwiseKMedoids 1 2 3 4 class LabelwiseKMedoids ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 class LabelwiseKMedoids(PrototypeWrapper): def __init__(self, *args, **kwargs): super().__init__(_LabelwiseKMedoids, *args, method='kmedoids', subtype='prototypes', **kwargs) self.labelwise = True Ancestors (in MRO) text_explainability.global_explanation.PrototypeWrapper Methods prototypes 1 2 3 4 def prototypes ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs) LabelwiseMMDCritic 1 2 3 4 class LabelwiseMMDCritic ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 class LabelwiseMMDCritic(PrototypeCriticismWrapper): def __init__(self, *args, **kwargs): super().__init__(_LabelwiseMMDCritic, *args, method='mmdcritic', subtype='prototypes_&_criticisms', **kwargs) self.labelwise = True Ancestors (in MRO) text_explainability.global_explanation.PrototypeCriticismWrapper text_explainability.global_explanation.PrototypeWrapper Methods criticisms 1 2 3 4 def criticisms ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs) prototypes 1 2 3 4 def prototypes ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs) MMDCritic 1 2 3 4 class MMDCritic ( * args , ** kwargs ) View Source 1 2 3 4 5 class MMDCritic(PrototypeCriticismWrapper): def __init__(self, *args, **kwargs): super().__init__(_MMDCritic, *args, method='mmdcritic', subtype='prototypes_&_criticisms', **kwargs) Ancestors (in MRO) text_explainability.global_explanation.PrototypeCriticismWrapper text_explainability.global_explanation.PrototypeWrapper Methods criticisms 1 2 3 4 def criticisms ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs) prototypes 1 2 3 4 def prototypes ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs) PrototypeCriticismWrapper 1 2 3 4 5 6 7 class PrototypeCriticismWrapper ( prototype_sampler : text_explainability . data . sampling . PrototypeSampler , * args , method : Optional [ str ] = None , subtype : str = 'prototypes_&_criticisms' , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class PrototypeCriticismWrapper(PrototypeWrapper): def __init__(self, prototype_sampler: PrototypeSampler, *args, method: Optional[str] = None, subtype: str = 'prototypes_&_criticisms', **kwargs): super().__init__(prototype_sampler, *args, method=method, subtype=subtype, **kwargs) @add_callargs def criticisms(self, *args, **kwargs) -> Instances: callargs = kwargs.pop('__callargs__', None) return Instances(instances={'criticisms': self.prototype_sampler.criticisms(*args, **kwargs)}, type=self.type, subtype='criticisms', method=self.method, callargs=callargs, labelwise=self.labelwise) Ancestors (in MRO) text_explainability.global_explanation.PrototypeWrapper Descendants text_explainability.global_explanation.MMDCritic text_explainability.global_explanation.LabelwiseMMDCritic Methods criticisms 1 2 3 4 def criticisms ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs) prototypes 1 2 3 4 def prototypes ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs) PrototypeWrapper 1 2 3 4 5 6 7 class PrototypeWrapper ( prototype_sampler : text_explainability . data . sampling . PrototypeSampler , * args , method : Optional [ str ] = None , subtype : str = 'prototypes' , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class PrototypeWrapper: def __init__(self, prototype_sampler: PrototypeSampler, *args, method: Optional[str] = None, subtype: str = 'prototypes', **kwargs): self.prototype_sampler = prototype_sampler(*args, **kwargs) self.type = 'global_explanation' self.subtype = subtype self.method = method self.labelwise = False @add_callargs def __call__(self, *args, **kwargs) -> Instances: callargs = kwargs.pop('__callargs__', None) instances = self.prototype_sampler.__call__(*args, **kwargs) return Instances(instances=instances if isinstance(instances, dict) else {'prototypes': instances}, type=self.type, subtype=self.subtype, method=self.method, callargs=callargs, labelwise=self.labelwise) @add_callargs def prototypes(self, *args, **kwargs) -> Instances: callargs = kwargs.pop('__callargs__', None) return Instances(instances={'prototypes': self.prototype_sampler.prototypes(*args, **kwargs)}, type=self.type, subtype='prototypes', method=self.method, callargs=callargs, labelwise=self.labelwise) Descendants text_explainability.global_explanation.KMedoids text_explainability.global_explanation.LabelwiseKMedoids text_explainability.global_explanation.PrototypeCriticismWrapper Methods prototypes 1 2 3 4 def prototypes ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs) TokenFrequency 1 2 3 4 class TokenFrequency ( provider : instancelib . instances . base . InstanceProvider [ instancelib . instances . text . TextInstance , typing . Any , str , typing . Any , str ], seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 class TokenFrequency(GlobalExplanation): @add_callargs def __call__(self, model: Optional[AbstractClassifier] = None, labelprovider: Optional[LabelProvider] = None, explain_model: bool = True, labelwise: bool = True, k: Optional[int] = None, filter_words: List[str] = translate_list('stopwords'), lower: bool = True, **count_vectorizer_kwargs) -> FeatureList: \"\"\"Show the top-k number of tokens for each ground-truth or predicted label. Args: model (Optional[AbstractClassifier], optional): Predictive model to explain. Defaults to None. labelprovider (Optional[LabelProvider], optional): Ground-truth labels to explain. Defaults to None. explain_model (bool, optional): Whether to explain the model (True) or ground-truth labels (False). Defaults to True. labelwise (bool, optional): Whether to summarize the counts for each label seperately. Defaults to True. k (Optional[int], optional): Limit to the top-k words per label, or all words if None. Defaults to None. filter_words (List[str], optional): Words to filter out from top-k. Defaults to ['de', 'het', 'een']. lower (bool, optional): Whether to make all tokens lowercase. Defaults to True. **count_vectorizer_kwargs: Optional arguments passed to `CountVectorizer`/`FastCountVectorizer`. Returns: FeatureList: Each label with corresponding top words and their frequency \"\"\" type, subtype = 'global_explanation', 'token_frequency' callargs = count_vectorizer_kwargs.pop('__callargs__', None) instances, labels = self.get_instances_labels(model, labelprovider, explain_model=explain_model) def top_k_counts(instances_to_fit): cv = CountVectorizer(**count_vectorizer_kwargs) counts = cv.fit_transform([str.lower(d) for d in instances_to_fit] if lower else instances_to_fit) counts = np.ravel(counts.sum(axis=0)) return sorted([(w, counts[v]) for w, v in cv.vocabulary_.items() if k not in filter_words], key=lambda x: x[1], reverse=True)[:k] if labelwise: # TO-DO improve beyond classification, e.g. buckets for regression? label_names = np.unique(labels) label_ids = [i for i, _ in enumerate(label_names)] def counts_by_label(label): return zip(*top_k_counts([instances[instances.key_list[idx]].data for idx in np.where(labels == label)[0]])) used_features, scores = zip(*[counts_by_label(label) for label in label_names]) return FeatureList(labels=label_ids, labelset=label_names, used_features=dict(zip(label_ids, used_features)), scores=dict(zip(label_ids, scores)), type=type, subtype=subtype, callargs=callargs) used_features, scores = zip(*top_k_counts(instances.all_data())) return FeatureList(used_features=used_features, scores=scores, type=type, subtype=subtype, callargs=callargs) Ancestors (in MRO) text_explainability.global_explanation.GlobalExplanation genbase.Readable genbase.mixin.SeedMixin Instance variables 1 seed Methods explain 1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs) get_data 1 2 3 def get_data ( self ) -> instancelib . instances . base . InstanceProvider Easy access to data. Returns: Type Description InstanceProvider Easily accessible dataset. View Source 1 2 3 4 5 6 7 8 9 10 11 def get_data(self) -> InstanceProvider: \"\"\"Easy access to data. Returns: InstanceProvider: Easily accessible dataset. \"\"\" return self.provider get_instances_labels 1 2 3 4 5 6 def get_instances_labels ( self , model : Optional [ instancelib . machinelearning . base . AbstractClassifier ], labelprovider : Optional [ instancelib . labels . base . LabelProvider ], explain_model : bool = True ) -> Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ] Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Parameters: Name Type Description Default model Optional[AbstractClassifier] Model to perform predictions with. None labelprovider Optional[LabelProvider] Ground-truth labels. None explain_model bool Whether to explain using the model labels (True) or labelprovider labels (False). Defaults to True. None Returns: Type Description Tuple[InstanceProvider, np.ndarray] Instances and corresponding labels Raises: Type Description ValueError if explain_model = True provide a model, and if False provide a labelprovider. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def get_instances_labels(self, model: Optional[AbstractClassifier], labelprovider: Optional[LabelProvider], explain_model: bool = True) -> Tuple[InstanceProvider, np.ndarray]: \"\"\"Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Args: model (Optional[AbstractClassifier]): Model to perform predictions with. labelprovider (Optional[LabelProvider]): Ground-truth labels. explain_model (bool, optional): Whether to explain using the `model` labels (True) or `labelprovider` labels (False). Defaults to True. Raises: ValueError: if explain_model = True provide a model, and if False provide a labelprovider. Returns: Tuple[InstanceProvider, np.ndarray]: Instances and corresponding labels \"\"\" if explain_model and model is None: raise ValueError('Provide a model to explain its predictions, or set `explain_predictions` to False') elif not explain_model and labelprovider is None: raise ValueError('Provide a labelprovider to explain ground-truth labels, ', 'or set `explain_predictions` to True') instances = self.get_data() labels = model.predict(instances) if explain_model \\ else [next(iter(labelprovider.get_labels(k))) for k in instances] if len(labels) > 0 and isinstance(labels[0], tuple) and isinstance(labels[0][-1], frozenset): labels = ['-'.join(list(x)) for _, x in labels] return instances, np.array(labels) predict 1 2 3 4 def predict ( self , model : instancelib . machinelearning . base . AbstractClassifier ) -> Union [ Sequence [ FrozenSet [ str ]], numpy . ndarray ] Apply predict function of model to data. Parameters: Name Type Description Default model AbstractClassifier Model to apply predictions with. None Returns: Type Description Union[Sequence[FrozenSet[str]], np.ndarray] Labels for dataset according to model. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def predict(self, model: AbstractClassifier) -> Union[Sequence[FrozenSet[str]], np.ndarray]: \"\"\"Apply predict function of model to data. Args: model (AbstractClassifier): Model to apply predictions with. Returns: Union[Sequence[FrozenSet[str]], np.ndarray]: Labels for dataset according to model. \"\"\" return model.predict(self.get_data()) reset_seed 1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self set_seed 1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed() TokenInformation 1 2 3 4 class TokenInformation ( provider : instancelib . instances . base . InstanceProvider [ instancelib . instances . text . TextInstance , typing . Any , str , typing . Any , str ], seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class TokenInformation(GlobalExplanation): @add_callargs def __call__(self, model: Optional[AbstractClassifier] = None, labelprovider: Optional[LabelProvider] = None, explain_model: bool = True, # labelwise: bool = True, k: Optional[int] = None, filter_words: List[str] = translate_list('stopwords'), lower: bool = True, **count_vectorizer_kwargs) -> FeatureList: \"\"\"Show the top-k token mutual information for a dataset or model. Args: model (Optional[AbstractClassifier], optional): Predictive model to explain. Defaults to None. labelprovider (Optional[LabelProvider], optional): Ground-truth labels to explain. Defaults to None. explain_model (bool, optional): Whether to explain the model (True) or ground-truth labels (False). Defaults to True. k (Optional[int], optional): Limit to the top-k words per label, or all words if None. Defaults to None. filter_words (List[str], optional): Words to filter out from top-k. Defaults to ['de', 'het', 'een']. lower (bool, optional): Whether to make all tokens lowercase. Defaults to True. **count_vectorizer_kwargs: Keyword arguments to pass onto `CountVectorizer`/`FastCountVectorizer`. Returns: FeatureList: k labels, sorted based on their mutual information with the output (predictive model labels or ground-truth labels) \"\"\" callargs = count_vectorizer_kwargs.pop('__callargs__', None) instances, labels = self.get_instances_labels(model, labelprovider, explain_model=explain_model) cv = CountVectorizer(**count_vectorizer_kwargs) counts = cv.fit_transform([str.lower(d) for d in instances.all_data()] if lower else instances.all_data()) # TO-DO improve beyond classification # see https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html # #sklearn.feature_selection.mutual_info_regression mif = mutual_info_classif(counts, labels, discrete_features=True, random_state=self.seed) feature_names = cv.get_feature_names() res = list(map(tuple, zip(feature_names, mif))) res_sorted = list(sorted([(w, v) for w, v in res if w not in filter_words], key=lambda x: x[1], reverse=True))[:k] used_features, scores = zip(*res_sorted) return FeatureList(used_features=used_features, scores=scores, type='global_explanation', subtype='token_information', method='mutual_information', callargs=callargs) Ancestors (in MRO) text_explainability.global_explanation.GlobalExplanation genbase.Readable genbase.mixin.SeedMixin Instance variables 1 seed Methods explain 1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs) get_data 1 2 3 def get_data ( self ) -> instancelib . instances . base . InstanceProvider Easy access to data. Returns: Type Description InstanceProvider Easily accessible dataset. View Source 1 2 3 4 5 6 7 8 9 10 11 def get_data(self) -> InstanceProvider: \"\"\"Easy access to data. Returns: InstanceProvider: Easily accessible dataset. \"\"\" return self.provider get_instances_labels 1 2 3 4 5 6 def get_instances_labels ( self , model : Optional [ instancelib . machinelearning . base . AbstractClassifier ], labelprovider : Optional [ instancelib . labels . base . LabelProvider ], explain_model : bool = True ) -> Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ] Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Parameters: Name Type Description Default model Optional[AbstractClassifier] Model to perform predictions with. None labelprovider Optional[LabelProvider] Ground-truth labels. None explain_model bool Whether to explain using the model labels (True) or labelprovider labels (False). Defaults to True. None Returns: Type Description Tuple[InstanceProvider, np.ndarray] Instances and corresponding labels Raises: Type Description ValueError if explain_model = True provide a model, and if False provide a labelprovider. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def get_instances_labels(self, model: Optional[AbstractClassifier], labelprovider: Optional[LabelProvider], explain_model: bool = True) -> Tuple[InstanceProvider, np.ndarray]: \"\"\"Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Args: model (Optional[AbstractClassifier]): Model to perform predictions with. labelprovider (Optional[LabelProvider]): Ground-truth labels. explain_model (bool, optional): Whether to explain using the `model` labels (True) or `labelprovider` labels (False). Defaults to True. Raises: ValueError: if explain_model = True provide a model, and if False provide a labelprovider. Returns: Tuple[InstanceProvider, np.ndarray]: Instances and corresponding labels \"\"\" if explain_model and model is None: raise ValueError('Provide a model to explain its predictions, or set `explain_predictions` to False') elif not explain_model and labelprovider is None: raise ValueError('Provide a labelprovider to explain ground-truth labels, ', 'or set `explain_predictions` to True') instances = self.get_data() labels = model.predict(instances) if explain_model \\ else [next(iter(labelprovider.get_labels(k))) for k in instances] if len(labels) > 0 and isinstance(labels[0], tuple) and isinstance(labels[0][-1], frozenset): labels = ['-'.join(list(x)) for _, x in labels] return instances, np.array(labels) predict 1 2 3 4 def predict ( self , model : instancelib . machinelearning . base . AbstractClassifier ) -> Union [ Sequence [ FrozenSet [ str ]], numpy . ndarray ] Apply predict function of model to data. Parameters: Name Type Description Default model AbstractClassifier Model to apply predictions with. None Returns: Type Description Union[Sequence[FrozenSet[str]], np.ndarray] Labels for dataset according to model. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def predict(self, model: AbstractClassifier) -> Union[Sequence[FrozenSet[str]], np.ndarray]: \"\"\"Apply predict function of model to data. Args: model (AbstractClassifier): Model to apply predictions with. Returns: Union[Sequence[FrozenSet[str]], np.ndarray]: Labels for dataset according to model. \"\"\" return model.predict(self.get_data()) reset_seed 1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self set_seed 1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed()","title":"Global Explanation"},{"location":"reference/text_explainability/global_explanation/#module-text_explainabilityglobal_explanation","text":"Global explanations explain the whole dataset or model behavior on that dataset. Todo: 1 2 3 * More support for sampling methods * add support for other tasks than classification (e.g. regression, multi-label classification) * partial dependence plots? https://scikit-learn.org/stable/modules/classes.html#module-sklearn.inspection View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 \"\"\"Global explanations explain the whole dataset or model behavior on that dataset. Todo: * More support for sampling methods * add support for other tasks than classification (e.g. regression, multi-label classification) * partial dependence plots? https://scikit-learn.org/stable/modules/classes.html#module-sklearn.inspection \"\"\" from typing import Any, FrozenSet, List, Optional, Sequence, Tuple, Union import numpy as np from genbase import Readable, SeedMixin, add_callargs, translate_list from instancelib import InstanceProvider from instancelib.instances.text import TextInstance from instancelib.labels import LabelProvider from instancelib.machinelearning import AbstractClassifier from sklearn.feature_selection import mutual_info_classif from .data.sampling import KMedoids as _Kmedoids from .data.sampling import LabelwiseKMedoids as _LabelwiseKMedoids from .data.sampling import LabelwiseMMDCritic as _LabelwiseMMDCritic from .data.sampling import MMDCritic as _MMDCritic from .data.sampling import PrototypeSampler from .generation.return_types import FeatureList, Instances try: from fastcountvectorizer import \\ FastCountVectorizer as \\ CountVectorizer # use fastcountvectorizer if available except ImportError: from sklearn.feature_extraction.text import CountVectorizer class GlobalExplanation(Readable, SeedMixin): def __init__(self, provider: InstanceProvider[TextInstance, Any, str, Any, str], seed: int = 0): \"\"\"Generic wrapper from global explanations (explain whole dataset or model). Args: provider (InstanceProvider[TextInstance, Any, str, Any, str]): Dataset to perform explanation on. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__() self.provider = provider self._seed = self._original_seed = seed def get_data(self) -> InstanceProvider: \"\"\"Easy access to data. Returns: InstanceProvider: Easily accessible dataset. \"\"\" return self.provider def predict(self, model: AbstractClassifier) -> Union[Sequence[FrozenSet[str]], np.ndarray]: \"\"\"Apply predict function of model to data. Args: model (AbstractClassifier): Model to apply predictions with. Returns: Union[Sequence[FrozenSet[str]], np.ndarray]: Labels for dataset according to model. \"\"\" return model.predict(self.get_data()) def get_instances_labels(self, model: Optional[AbstractClassifier], labelprovider: Optional[LabelProvider], explain_model: bool = True) -> Tuple[InstanceProvider, np.ndarray]: \"\"\"Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Args: model (Optional[AbstractClassifier]): Model to perform predictions with. labelprovider (Optional[LabelProvider]): Ground-truth labels. explain_model (bool, optional): Whether to explain using the `model` labels (True) or `labelprovider` labels (False). Defaults to True. Raises: ValueError: if explain_model = True provide a model, and if False provide a labelprovider. Returns: Tuple[InstanceProvider, np.ndarray]: Instances and corresponding labels \"\"\" if explain_model and model is None: raise ValueError('Provide a model to explain its predictions, or set `explain_predictions` to False') elif not explain_model and labelprovider is None: raise ValueError('Provide a labelprovider to explain ground-truth labels, ', 'or set `explain_predictions` to True') instances = self.get_data() labels = model.predict(instances) if explain_model \\ else [next(iter(labelprovider.get_labels(k))) for k in instances] if len(labels) > 0 and isinstance(labels[0], tuple) and isinstance(labels[0][-1], frozenset): labels = ['-'.join(list(x)) for _, x in labels] return instances, np.array(labels) def explain(self, *args, **kwargs): return self(*args, **kwargs) class TokenFrequency(GlobalExplanation): @add_callargs def __call__(self, model: Optional[AbstractClassifier] = None, labelprovider: Optional[LabelProvider] = None, explain_model: bool = True, labelwise: bool = True, k: Optional[int] = None, filter_words: List[str] = translate_list('stopwords'), lower: bool = True, **count_vectorizer_kwargs) -> FeatureList: \"\"\"Show the top-k number of tokens for each ground-truth or predicted label. Args: model (Optional[AbstractClassifier], optional): Predictive model to explain. Defaults to None. labelprovider (Optional[LabelProvider], optional): Ground-truth labels to explain. Defaults to None. explain_model (bool, optional): Whether to explain the model (True) or ground-truth labels (False). Defaults to True. labelwise (bool, optional): Whether to summarize the counts for each label seperately. Defaults to True. k (Optional[int], optional): Limit to the top-k words per label, or all words if None. Defaults to None. filter_words (List[str], optional): Words to filter out from top-k. Defaults to ['de', 'het', 'een']. lower (bool, optional): Whether to make all tokens lowercase. Defaults to True. **count_vectorizer_kwargs: Optional arguments passed to `CountVectorizer`/`FastCountVectorizer`. Returns: FeatureList: Each label with corresponding top words and their frequency \"\"\" type, subtype = 'global_explanation', 'token_frequency' callargs = count_vectorizer_kwargs.pop('__callargs__', None) instances, labels = self.get_instances_labels(model, labelprovider, explain_model=explain_model) def top_k_counts(instances_to_fit): cv = CountVectorizer(**count_vectorizer_kwargs) counts = cv.fit_transform([str.lower(d) for d in instances_to_fit] if lower else instances_to_fit) counts = np.ravel(counts.sum(axis=0)) return sorted([(w, counts[v]) for w, v in cv.vocabulary_.items() if k not in filter_words], key=lambda x: x[1], reverse=True)[:k] if labelwise: # TO-DO improve beyond classification, e.g. buckets for regression? label_names = np.unique(labels) label_ids = [i for i, _ in enumerate(label_names)] def counts_by_label(label): return zip(*top_k_counts([instances[instances.key_list[idx]].data for idx in np.where(labels == label)[0]])) used_features, scores = zip(*[counts_by_label(label) for label in label_names]) return FeatureList(labels=label_ids, labelset=label_names, used_features=dict(zip(label_ids, used_features)), scores=dict(zip(label_ids, scores)), type=type, subtype=subtype, callargs=callargs) used_features, scores = zip(*top_k_counts(instances.all_data())) return FeatureList(used_features=used_features, scores=scores, type=type, subtype=subtype, callargs=callargs) class TokenInformation(GlobalExplanation): @add_callargs def __call__(self, model: Optional[AbstractClassifier] = None, labelprovider: Optional[LabelProvider] = None, explain_model: bool = True, # labelwise: bool = True, k: Optional[int] = None, filter_words: List[str] = translate_list('stopwords'), lower: bool = True, **count_vectorizer_kwargs) -> FeatureList: \"\"\"Show the top-k token mutual information for a dataset or model. Args: model (Optional[AbstractClassifier], optional): Predictive model to explain. Defaults to None. labelprovider (Optional[LabelProvider], optional): Ground-truth labels to explain. Defaults to None. explain_model (bool, optional): Whether to explain the model (True) or ground-truth labels (False). Defaults to True. k (Optional[int], optional): Limit to the top-k words per label, or all words if None. Defaults to None. filter_words (List[str], optional): Words to filter out from top-k. Defaults to ['de', 'het', 'een']. lower (bool, optional): Whether to make all tokens lowercase. Defaults to True. **count_vectorizer_kwargs: Keyword arguments to pass onto `CountVectorizer`/`FastCountVectorizer`. Returns: FeatureList: k labels, sorted based on their mutual information with the output (predictive model labels or ground-truth labels) \"\"\" callargs = count_vectorizer_kwargs.pop('__callargs__', None) instances, labels = self.get_instances_labels(model, labelprovider, explain_model=explain_model) cv = CountVectorizer(**count_vectorizer_kwargs) counts = cv.fit_transform([str.lower(d) for d in instances.all_data()] if lower else instances.all_data()) # TO-DO improve beyond classification # see https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html # #sklearn.feature_selection.mutual_info_regression mif = mutual_info_classif(counts, labels, discrete_features=True, random_state=self.seed) feature_names = cv.get_feature_names() res = list(map(tuple, zip(feature_names, mif))) res_sorted = list(sorted([(w, v) for w, v in res if w not in filter_words], key=lambda x: x[1], reverse=True))[:k] used_features, scores = zip(*res_sorted) return FeatureList(used_features=used_features, scores=scores, type='global_explanation', subtype='token_information', method='mutual_information', callargs=callargs) class PrototypeWrapper: def __init__(self, prototype_sampler: PrototypeSampler, *args, method: Optional[str] = None, subtype: str = 'prototypes', **kwargs): self.prototype_sampler = prototype_sampler(*args, **kwargs) self.type = 'global_explanation' self.subtype = subtype self.method = method self.labelwise = False @add_callargs def __call__(self, *args, **kwargs) -> Instances: callargs = kwargs.pop('__callargs__', None) instances = self.prototype_sampler.__call__(*args, **kwargs) return Instances(instances=instances if isinstance(instances, dict) else {'prototypes': instances}, type=self.type, subtype=self.subtype, method=self.method, callargs=callargs, labelwise=self.labelwise) @add_callargs def prototypes(self, *args, **kwargs) -> Instances: callargs = kwargs.pop('__callargs__', None) return Instances(instances={'prototypes': self.prototype_sampler.prototypes(*args, **kwargs)}, type=self.type, subtype='prototypes', method=self.method, callargs=callargs, labelwise=self.labelwise) class KMedoids(PrototypeWrapper): def __init__(self, *args, **kwargs): super().__init__(_Kmedoids, *args, method='kmedoids', subtype='prototypes', **kwargs) class LabelwiseKMedoids(PrototypeWrapper): def __init__(self, *args, **kwargs): super().__init__(_LabelwiseKMedoids, *args, method='kmedoids', subtype='prototypes', **kwargs) self.labelwise = True class PrototypeCriticismWrapper(PrototypeWrapper): def __init__(self, prototype_sampler: PrototypeSampler, *args, method: Optional[str] = None, subtype: str = 'prototypes_&_criticisms', **kwargs): super().__init__(prototype_sampler, *args, method=method, subtype=subtype, **kwargs) @add_callargs def criticisms(self, *args, **kwargs) -> Instances: callargs = kwargs.pop('__callargs__', None) return Instances(instances={'criticisms': self.prototype_sampler.criticisms(*args, **kwargs)}, type=self.type, subtype='criticisms', method=self.method, callargs=callargs, labelwise=self.labelwise) class MMDCritic(PrototypeCriticismWrapper): def __init__(self, *args, **kwargs): super().__init__(_MMDCritic, *args, method='mmdcritic', subtype='prototypes_&_criticisms', **kwargs) class LabelwiseMMDCritic(PrototypeCriticismWrapper): def __init__(self, *args, **kwargs): super().__init__(_LabelwiseMMDCritic, *args, method='mmdcritic', subtype='prototypes_&_criticisms', **kwargs) self.labelwise = True","title":"Module text_explainability.global_explanation"},{"location":"reference/text_explainability/global_explanation/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/global_explanation/#globalexplanation","text":"1 2 3 4 class GlobalExplanation ( provider : instancelib . instances . base . InstanceProvider [ instancelib . instances . text . TextInstance , typing . Any , str , typing . Any , str ], seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 class GlobalExplanation(Readable, SeedMixin): def __init__(self, provider: InstanceProvider[TextInstance, Any, str, Any, str], seed: int = 0): \"\"\"Generic wrapper from global explanations (explain whole dataset or model). Args: provider (InstanceProvider[TextInstance, Any, str, Any, str]): Dataset to perform explanation on. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__() self.provider = provider self._seed = self._original_seed = seed def get_data(self) -> InstanceProvider: \"\"\"Easy access to data. Returns: InstanceProvider: Easily accessible dataset. \"\"\" return self.provider def predict(self, model: AbstractClassifier) -> Union[Sequence[FrozenSet[str]], np.ndarray]: \"\"\"Apply predict function of model to data. Args: model (AbstractClassifier): Model to apply predictions with. Returns: Union[Sequence[FrozenSet[str]], np.ndarray]: Labels for dataset according to model. \"\"\" return model.predict(self.get_data()) def get_instances_labels(self, model: Optional[AbstractClassifier], labelprovider: Optional[LabelProvider], explain_model: bool = True) -> Tuple[InstanceProvider, np.ndarray]: \"\"\"Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Args: model (Optional[AbstractClassifier]): Model to perform predictions with. labelprovider (Optional[LabelProvider]): Ground-truth labels. explain_model (bool, optional): Whether to explain using the `model` labels (True) or `labelprovider` labels (False). Defaults to True. Raises: ValueError: if explain_model = True provide a model, and if False provide a labelprovider. Returns: Tuple[InstanceProvider, np.ndarray]: Instances and corresponding labels \"\"\" if explain_model and model is None: raise ValueError('Provide a model to explain its predictions, or set `explain_predictions` to False') elif not explain_model and labelprovider is None: raise ValueError('Provide a labelprovider to explain ground-truth labels, ', 'or set `explain_predictions` to True') instances = self.get_data() labels = model.predict(instances) if explain_model \\ else [next(iter(labelprovider.get_labels(k))) for k in instances] if len(labels) > 0 and isinstance(labels[0], tuple) and isinstance(labels[0][-1], frozenset): labels = ['-'.join(list(x)) for _, x in labels] return instances, np.array(labels) def explain(self, *args, **kwargs): return self(*args, **kwargs)","title":"GlobalExplanation"},{"location":"reference/text_explainability/global_explanation/#ancestors-in-mro","text":"genbase.Readable genbase.mixin.SeedMixin","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/global_explanation/#descendants","text":"text_explainability.global_explanation.TokenFrequency text_explainability.global_explanation.TokenInformation","title":"Descendants"},{"location":"reference/text_explainability/global_explanation/#instance-variables","text":"1 seed","title":"Instance variables"},{"location":"reference/text_explainability/global_explanation/#methods","text":"","title":"Methods"},{"location":"reference/text_explainability/global_explanation/#explain","text":"1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs)","title":"explain"},{"location":"reference/text_explainability/global_explanation/#get_data","text":"1 2 3 def get_data ( self ) -> instancelib . instances . base . InstanceProvider Easy access to data. Returns: Type Description InstanceProvider Easily accessible dataset. View Source 1 2 3 4 5 6 7 8 9 10 11 def get_data(self) -> InstanceProvider: \"\"\"Easy access to data. Returns: InstanceProvider: Easily accessible dataset. \"\"\" return self.provider","title":"get_data"},{"location":"reference/text_explainability/global_explanation/#get_instances_labels","text":"1 2 3 4 5 6 def get_instances_labels ( self , model : Optional [ instancelib . machinelearning . base . AbstractClassifier ], labelprovider : Optional [ instancelib . labels . base . LabelProvider ], explain_model : bool = True ) -> Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ] Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Parameters: Name Type Description Default model Optional[AbstractClassifier] Model to perform predictions with. None labelprovider Optional[LabelProvider] Ground-truth labels. None explain_model bool Whether to explain using the model labels (True) or labelprovider labels (False). Defaults to True. None Returns: Type Description Tuple[InstanceProvider, np.ndarray] Instances and corresponding labels Raises: Type Description ValueError if explain_model = True provide a model, and if False provide a labelprovider. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def get_instances_labels(self, model: Optional[AbstractClassifier], labelprovider: Optional[LabelProvider], explain_model: bool = True) -> Tuple[InstanceProvider, np.ndarray]: \"\"\"Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Args: model (Optional[AbstractClassifier]): Model to perform predictions with. labelprovider (Optional[LabelProvider]): Ground-truth labels. explain_model (bool, optional): Whether to explain using the `model` labels (True) or `labelprovider` labels (False). Defaults to True. Raises: ValueError: if explain_model = True provide a model, and if False provide a labelprovider. Returns: Tuple[InstanceProvider, np.ndarray]: Instances and corresponding labels \"\"\" if explain_model and model is None: raise ValueError('Provide a model to explain its predictions, or set `explain_predictions` to False') elif not explain_model and labelprovider is None: raise ValueError('Provide a labelprovider to explain ground-truth labels, ', 'or set `explain_predictions` to True') instances = self.get_data() labels = model.predict(instances) if explain_model \\ else [next(iter(labelprovider.get_labels(k))) for k in instances] if len(labels) > 0 and isinstance(labels[0], tuple) and isinstance(labels[0][-1], frozenset): labels = ['-'.join(list(x)) for _, x in labels] return instances, np.array(labels)","title":"get_instances_labels"},{"location":"reference/text_explainability/global_explanation/#predict","text":"1 2 3 4 def predict ( self , model : instancelib . machinelearning . base . AbstractClassifier ) -> Union [ Sequence [ FrozenSet [ str ]], numpy . ndarray ] Apply predict function of model to data. Parameters: Name Type Description Default model AbstractClassifier Model to apply predictions with. None Returns: Type Description Union[Sequence[FrozenSet[str]], np.ndarray] Labels for dataset according to model. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def predict(self, model: AbstractClassifier) -> Union[Sequence[FrozenSet[str]], np.ndarray]: \"\"\"Apply predict function of model to data. Args: model (AbstractClassifier): Model to apply predictions with. Returns: Union[Sequence[FrozenSet[str]], np.ndarray]: Labels for dataset according to model. \"\"\" return model.predict(self.get_data())","title":"predict"},{"location":"reference/text_explainability/global_explanation/#reset_seed","text":"1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self","title":"reset_seed"},{"location":"reference/text_explainability/global_explanation/#set_seed","text":"1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed()","title":"set_seed"},{"location":"reference/text_explainability/global_explanation/#kmedoids","text":"1 2 3 4 class KMedoids ( * args , ** kwargs ) View Source 1 2 3 4 5 class KMedoids(PrototypeWrapper): def __init__(self, *args, **kwargs): super().__init__(_Kmedoids, *args, method='kmedoids', subtype='prototypes', **kwargs)","title":"KMedoids"},{"location":"reference/text_explainability/global_explanation/#ancestors-in-mro_1","text":"text_explainability.global_explanation.PrototypeWrapper","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/global_explanation/#methods_1","text":"","title":"Methods"},{"location":"reference/text_explainability/global_explanation/#prototypes","text":"1 2 3 4 def prototypes ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs)","title":"prototypes"},{"location":"reference/text_explainability/global_explanation/#labelwisekmedoids","text":"1 2 3 4 class LabelwiseKMedoids ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 class LabelwiseKMedoids(PrototypeWrapper): def __init__(self, *args, **kwargs): super().__init__(_LabelwiseKMedoids, *args, method='kmedoids', subtype='prototypes', **kwargs) self.labelwise = True","title":"LabelwiseKMedoids"},{"location":"reference/text_explainability/global_explanation/#ancestors-in-mro_2","text":"text_explainability.global_explanation.PrototypeWrapper","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/global_explanation/#methods_2","text":"","title":"Methods"},{"location":"reference/text_explainability/global_explanation/#prototypes_1","text":"1 2 3 4 def prototypes ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs)","title":"prototypes"},{"location":"reference/text_explainability/global_explanation/#labelwisemmdcritic","text":"1 2 3 4 class LabelwiseMMDCritic ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 class LabelwiseMMDCritic(PrototypeCriticismWrapper): def __init__(self, *args, **kwargs): super().__init__(_LabelwiseMMDCritic, *args, method='mmdcritic', subtype='prototypes_&_criticisms', **kwargs) self.labelwise = True","title":"LabelwiseMMDCritic"},{"location":"reference/text_explainability/global_explanation/#ancestors-in-mro_3","text":"text_explainability.global_explanation.PrototypeCriticismWrapper text_explainability.global_explanation.PrototypeWrapper","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/global_explanation/#methods_3","text":"","title":"Methods"},{"location":"reference/text_explainability/global_explanation/#criticisms","text":"1 2 3 4 def criticisms ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs)","title":"criticisms"},{"location":"reference/text_explainability/global_explanation/#prototypes_2","text":"1 2 3 4 def prototypes ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs)","title":"prototypes"},{"location":"reference/text_explainability/global_explanation/#mmdcritic","text":"1 2 3 4 class MMDCritic ( * args , ** kwargs ) View Source 1 2 3 4 5 class MMDCritic(PrototypeCriticismWrapper): def __init__(self, *args, **kwargs): super().__init__(_MMDCritic, *args, method='mmdcritic', subtype='prototypes_&_criticisms', **kwargs)","title":"MMDCritic"},{"location":"reference/text_explainability/global_explanation/#ancestors-in-mro_4","text":"text_explainability.global_explanation.PrototypeCriticismWrapper text_explainability.global_explanation.PrototypeWrapper","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/global_explanation/#methods_4","text":"","title":"Methods"},{"location":"reference/text_explainability/global_explanation/#criticisms_1","text":"1 2 3 4 def criticisms ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs)","title":"criticisms"},{"location":"reference/text_explainability/global_explanation/#prototypes_3","text":"1 2 3 4 def prototypes ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs)","title":"prototypes"},{"location":"reference/text_explainability/global_explanation/#prototypecriticismwrapper","text":"1 2 3 4 5 6 7 class PrototypeCriticismWrapper ( prototype_sampler : text_explainability . data . sampling . PrototypeSampler , * args , method : Optional [ str ] = None , subtype : str = 'prototypes_&_criticisms' , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class PrototypeCriticismWrapper(PrototypeWrapper): def __init__(self, prototype_sampler: PrototypeSampler, *args, method: Optional[str] = None, subtype: str = 'prototypes_&_criticisms', **kwargs): super().__init__(prototype_sampler, *args, method=method, subtype=subtype, **kwargs) @add_callargs def criticisms(self, *args, **kwargs) -> Instances: callargs = kwargs.pop('__callargs__', None) return Instances(instances={'criticisms': self.prototype_sampler.criticisms(*args, **kwargs)}, type=self.type, subtype='criticisms', method=self.method, callargs=callargs, labelwise=self.labelwise)","title":"PrototypeCriticismWrapper"},{"location":"reference/text_explainability/global_explanation/#ancestors-in-mro_5","text":"text_explainability.global_explanation.PrototypeWrapper","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/global_explanation/#descendants_1","text":"text_explainability.global_explanation.MMDCritic text_explainability.global_explanation.LabelwiseMMDCritic","title":"Descendants"},{"location":"reference/text_explainability/global_explanation/#methods_5","text":"","title":"Methods"},{"location":"reference/text_explainability/global_explanation/#criticisms_2","text":"1 2 3 4 def criticisms ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs)","title":"criticisms"},{"location":"reference/text_explainability/global_explanation/#prototypes_4","text":"1 2 3 4 def prototypes ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs)","title":"prototypes"},{"location":"reference/text_explainability/global_explanation/#prototypewrapper","text":"1 2 3 4 5 6 7 class PrototypeWrapper ( prototype_sampler : text_explainability . data . sampling . PrototypeSampler , * args , method : Optional [ str ] = None , subtype : str = 'prototypes' , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class PrototypeWrapper: def __init__(self, prototype_sampler: PrototypeSampler, *args, method: Optional[str] = None, subtype: str = 'prototypes', **kwargs): self.prototype_sampler = prototype_sampler(*args, **kwargs) self.type = 'global_explanation' self.subtype = subtype self.method = method self.labelwise = False @add_callargs def __call__(self, *args, **kwargs) -> Instances: callargs = kwargs.pop('__callargs__', None) instances = self.prototype_sampler.__call__(*args, **kwargs) return Instances(instances=instances if isinstance(instances, dict) else {'prototypes': instances}, type=self.type, subtype=self.subtype, method=self.method, callargs=callargs, labelwise=self.labelwise) @add_callargs def prototypes(self, *args, **kwargs) -> Instances: callargs = kwargs.pop('__callargs__', None) return Instances(instances={'prototypes': self.prototype_sampler.prototypes(*args, **kwargs)}, type=self.type, subtype='prototypes', method=self.method, callargs=callargs, labelwise=self.labelwise)","title":"PrototypeWrapper"},{"location":"reference/text_explainability/global_explanation/#descendants_2","text":"text_explainability.global_explanation.KMedoids text_explainability.global_explanation.LabelwiseKMedoids text_explainability.global_explanation.PrototypeCriticismWrapper","title":"Descendants"},{"location":"reference/text_explainability/global_explanation/#methods_6","text":"","title":"Methods"},{"location":"reference/text_explainability/global_explanation/#prototypes_5","text":"1 2 3 4 def prototypes ( * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def inner(*args, **kwargs): ba = inspect.signature(function).bind(*args, **kwargs) ba.apply_defaults() kw = next((k for k, v in ba.signature.parameters.items() if k == '__callargs__' or v.kind == inspect._ParameterKind.VAR_KEYWORD), None) # Do not decorate the function if we are unable to pass __callargs__ as an argument if kw is None: return function(*ba.args, **ba.kwargs) # Construct __callargs__ (including introspection of the self argument) callargs = {'__name__': function.__name__, **dict(recursive_to_dict(ba.arguments))} if hasattr(function, '__self__') and 'self' not in callargs.keys(): callargs['self'] = function.__self__ if 'self' in callargs.keys(): self = callargs.pop('self') callargs['self'] = self.to_config() if hasattr(self, 'to_config') and hasattr(self, '_dict') \\ else dict(recursive_to_dict(self)) if '__name__' not in callargs['self'] and hasattr(self, '__class__') or hasattr(self, '__name__'): callargs['self']['__name__'] = self.__class__.__name__ if hasattr(self, '__class__') \\ else self.__name__ callargs.pop('__class__', None) return function(*ba.args, __callargs__=callargs, **ba.kwargs)","title":"prototypes"},{"location":"reference/text_explainability/global_explanation/#tokenfrequency","text":"1 2 3 4 class TokenFrequency ( provider : instancelib . instances . base . InstanceProvider [ instancelib . instances . text . TextInstance , typing . Any , str , typing . Any , str ], seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 class TokenFrequency(GlobalExplanation): @add_callargs def __call__(self, model: Optional[AbstractClassifier] = None, labelprovider: Optional[LabelProvider] = None, explain_model: bool = True, labelwise: bool = True, k: Optional[int] = None, filter_words: List[str] = translate_list('stopwords'), lower: bool = True, **count_vectorizer_kwargs) -> FeatureList: \"\"\"Show the top-k number of tokens for each ground-truth or predicted label. Args: model (Optional[AbstractClassifier], optional): Predictive model to explain. Defaults to None. labelprovider (Optional[LabelProvider], optional): Ground-truth labels to explain. Defaults to None. explain_model (bool, optional): Whether to explain the model (True) or ground-truth labels (False). Defaults to True. labelwise (bool, optional): Whether to summarize the counts for each label seperately. Defaults to True. k (Optional[int], optional): Limit to the top-k words per label, or all words if None. Defaults to None. filter_words (List[str], optional): Words to filter out from top-k. Defaults to ['de', 'het', 'een']. lower (bool, optional): Whether to make all tokens lowercase. Defaults to True. **count_vectorizer_kwargs: Optional arguments passed to `CountVectorizer`/`FastCountVectorizer`. Returns: FeatureList: Each label with corresponding top words and their frequency \"\"\" type, subtype = 'global_explanation', 'token_frequency' callargs = count_vectorizer_kwargs.pop('__callargs__', None) instances, labels = self.get_instances_labels(model, labelprovider, explain_model=explain_model) def top_k_counts(instances_to_fit): cv = CountVectorizer(**count_vectorizer_kwargs) counts = cv.fit_transform([str.lower(d) for d in instances_to_fit] if lower else instances_to_fit) counts = np.ravel(counts.sum(axis=0)) return sorted([(w, counts[v]) for w, v in cv.vocabulary_.items() if k not in filter_words], key=lambda x: x[1], reverse=True)[:k] if labelwise: # TO-DO improve beyond classification, e.g. buckets for regression? label_names = np.unique(labels) label_ids = [i for i, _ in enumerate(label_names)] def counts_by_label(label): return zip(*top_k_counts([instances[instances.key_list[idx]].data for idx in np.where(labels == label)[0]])) used_features, scores = zip(*[counts_by_label(label) for label in label_names]) return FeatureList(labels=label_ids, labelset=label_names, used_features=dict(zip(label_ids, used_features)), scores=dict(zip(label_ids, scores)), type=type, subtype=subtype, callargs=callargs) used_features, scores = zip(*top_k_counts(instances.all_data())) return FeatureList(used_features=used_features, scores=scores, type=type, subtype=subtype, callargs=callargs)","title":"TokenFrequency"},{"location":"reference/text_explainability/global_explanation/#ancestors-in-mro_6","text":"text_explainability.global_explanation.GlobalExplanation genbase.Readable genbase.mixin.SeedMixin","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/global_explanation/#instance-variables_1","text":"1 seed","title":"Instance variables"},{"location":"reference/text_explainability/global_explanation/#methods_7","text":"","title":"Methods"},{"location":"reference/text_explainability/global_explanation/#explain_1","text":"1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs)","title":"explain"},{"location":"reference/text_explainability/global_explanation/#get_data_1","text":"1 2 3 def get_data ( self ) -> instancelib . instances . base . InstanceProvider Easy access to data. Returns: Type Description InstanceProvider Easily accessible dataset. View Source 1 2 3 4 5 6 7 8 9 10 11 def get_data(self) -> InstanceProvider: \"\"\"Easy access to data. Returns: InstanceProvider: Easily accessible dataset. \"\"\" return self.provider","title":"get_data"},{"location":"reference/text_explainability/global_explanation/#get_instances_labels_1","text":"1 2 3 4 5 6 def get_instances_labels ( self , model : Optional [ instancelib . machinelearning . base . AbstractClassifier ], labelprovider : Optional [ instancelib . labels . base . LabelProvider ], explain_model : bool = True ) -> Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ] Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Parameters: Name Type Description Default model Optional[AbstractClassifier] Model to perform predictions with. None labelprovider Optional[LabelProvider] Ground-truth labels. None explain_model bool Whether to explain using the model labels (True) or labelprovider labels (False). Defaults to True. None Returns: Type Description Tuple[InstanceProvider, np.ndarray] Instances and corresponding labels Raises: Type Description ValueError if explain_model = True provide a model, and if False provide a labelprovider. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def get_instances_labels(self, model: Optional[AbstractClassifier], labelprovider: Optional[LabelProvider], explain_model: bool = True) -> Tuple[InstanceProvider, np.ndarray]: \"\"\"Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Args: model (Optional[AbstractClassifier]): Model to perform predictions with. labelprovider (Optional[LabelProvider]): Ground-truth labels. explain_model (bool, optional): Whether to explain using the `model` labels (True) or `labelprovider` labels (False). Defaults to True. Raises: ValueError: if explain_model = True provide a model, and if False provide a labelprovider. Returns: Tuple[InstanceProvider, np.ndarray]: Instances and corresponding labels \"\"\" if explain_model and model is None: raise ValueError('Provide a model to explain its predictions, or set `explain_predictions` to False') elif not explain_model and labelprovider is None: raise ValueError('Provide a labelprovider to explain ground-truth labels, ', 'or set `explain_predictions` to True') instances = self.get_data() labels = model.predict(instances) if explain_model \\ else [next(iter(labelprovider.get_labels(k))) for k in instances] if len(labels) > 0 and isinstance(labels[0], tuple) and isinstance(labels[0][-1], frozenset): labels = ['-'.join(list(x)) for _, x in labels] return instances, np.array(labels)","title":"get_instances_labels"},{"location":"reference/text_explainability/global_explanation/#predict_1","text":"1 2 3 4 def predict ( self , model : instancelib . machinelearning . base . AbstractClassifier ) -> Union [ Sequence [ FrozenSet [ str ]], numpy . ndarray ] Apply predict function of model to data. Parameters: Name Type Description Default model AbstractClassifier Model to apply predictions with. None Returns: Type Description Union[Sequence[FrozenSet[str]], np.ndarray] Labels for dataset according to model. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def predict(self, model: AbstractClassifier) -> Union[Sequence[FrozenSet[str]], np.ndarray]: \"\"\"Apply predict function of model to data. Args: model (AbstractClassifier): Model to apply predictions with. Returns: Union[Sequence[FrozenSet[str]], np.ndarray]: Labels for dataset according to model. \"\"\" return model.predict(self.get_data())","title":"predict"},{"location":"reference/text_explainability/global_explanation/#reset_seed_1","text":"1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self","title":"reset_seed"},{"location":"reference/text_explainability/global_explanation/#set_seed_1","text":"1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed()","title":"set_seed"},{"location":"reference/text_explainability/global_explanation/#tokeninformation","text":"1 2 3 4 class TokenInformation ( provider : instancelib . instances . base . InstanceProvider [ instancelib . instances . text . TextInstance , typing . Any , str , typing . Any , str ], seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class TokenInformation(GlobalExplanation): @add_callargs def __call__(self, model: Optional[AbstractClassifier] = None, labelprovider: Optional[LabelProvider] = None, explain_model: bool = True, # labelwise: bool = True, k: Optional[int] = None, filter_words: List[str] = translate_list('stopwords'), lower: bool = True, **count_vectorizer_kwargs) -> FeatureList: \"\"\"Show the top-k token mutual information for a dataset or model. Args: model (Optional[AbstractClassifier], optional): Predictive model to explain. Defaults to None. labelprovider (Optional[LabelProvider], optional): Ground-truth labels to explain. Defaults to None. explain_model (bool, optional): Whether to explain the model (True) or ground-truth labels (False). Defaults to True. k (Optional[int], optional): Limit to the top-k words per label, or all words if None. Defaults to None. filter_words (List[str], optional): Words to filter out from top-k. Defaults to ['de', 'het', 'een']. lower (bool, optional): Whether to make all tokens lowercase. Defaults to True. **count_vectorizer_kwargs: Keyword arguments to pass onto `CountVectorizer`/`FastCountVectorizer`. Returns: FeatureList: k labels, sorted based on their mutual information with the output (predictive model labels or ground-truth labels) \"\"\" callargs = count_vectorizer_kwargs.pop('__callargs__', None) instances, labels = self.get_instances_labels(model, labelprovider, explain_model=explain_model) cv = CountVectorizer(**count_vectorizer_kwargs) counts = cv.fit_transform([str.lower(d) for d in instances.all_data()] if lower else instances.all_data()) # TO-DO improve beyond classification # see https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html # #sklearn.feature_selection.mutual_info_regression mif = mutual_info_classif(counts, labels, discrete_features=True, random_state=self.seed) feature_names = cv.get_feature_names() res = list(map(tuple, zip(feature_names, mif))) res_sorted = list(sorted([(w, v) for w, v in res if w not in filter_words], key=lambda x: x[1], reverse=True))[:k] used_features, scores = zip(*res_sorted) return FeatureList(used_features=used_features, scores=scores, type='global_explanation', subtype='token_information', method='mutual_information', callargs=callargs)","title":"TokenInformation"},{"location":"reference/text_explainability/global_explanation/#ancestors-in-mro_7","text":"text_explainability.global_explanation.GlobalExplanation genbase.Readable genbase.mixin.SeedMixin","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/global_explanation/#instance-variables_2","text":"1 seed","title":"Instance variables"},{"location":"reference/text_explainability/global_explanation/#methods_8","text":"","title":"Methods"},{"location":"reference/text_explainability/global_explanation/#explain_2","text":"1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs)","title":"explain"},{"location":"reference/text_explainability/global_explanation/#get_data_2","text":"1 2 3 def get_data ( self ) -> instancelib . instances . base . InstanceProvider Easy access to data. Returns: Type Description InstanceProvider Easily accessible dataset. View Source 1 2 3 4 5 6 7 8 9 10 11 def get_data(self) -> InstanceProvider: \"\"\"Easy access to data. Returns: InstanceProvider: Easily accessible dataset. \"\"\" return self.provider","title":"get_data"},{"location":"reference/text_explainability/global_explanation/#get_instances_labels_2","text":"1 2 3 4 5 6 def get_instances_labels ( self , model : Optional [ instancelib . machinelearning . base . AbstractClassifier ], labelprovider : Optional [ instancelib . labels . base . LabelProvider ], explain_model : bool = True ) -> Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ] Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Parameters: Name Type Description Default model Optional[AbstractClassifier] Model to perform predictions with. None labelprovider Optional[LabelProvider] Ground-truth labels. None explain_model bool Whether to explain using the model labels (True) or labelprovider labels (False). Defaults to True. None Returns: Type Description Tuple[InstanceProvider, np.ndarray] Instances and corresponding labels Raises: Type Description ValueError if explain_model = True provide a model, and if False provide a labelprovider. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def get_instances_labels(self, model: Optional[AbstractClassifier], labelprovider: Optional[LabelProvider], explain_model: bool = True) -> Tuple[InstanceProvider, np.ndarray]: \"\"\"Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Args: model (Optional[AbstractClassifier]): Model to perform predictions with. labelprovider (Optional[LabelProvider]): Ground-truth labels. explain_model (bool, optional): Whether to explain using the `model` labels (True) or `labelprovider` labels (False). Defaults to True. Raises: ValueError: if explain_model = True provide a model, and if False provide a labelprovider. Returns: Tuple[InstanceProvider, np.ndarray]: Instances and corresponding labels \"\"\" if explain_model and model is None: raise ValueError('Provide a model to explain its predictions, or set `explain_predictions` to False') elif not explain_model and labelprovider is None: raise ValueError('Provide a labelprovider to explain ground-truth labels, ', 'or set `explain_predictions` to True') instances = self.get_data() labels = model.predict(instances) if explain_model \\ else [next(iter(labelprovider.get_labels(k))) for k in instances] if len(labels) > 0 and isinstance(labels[0], tuple) and isinstance(labels[0][-1], frozenset): labels = ['-'.join(list(x)) for _, x in labels] return instances, np.array(labels)","title":"get_instances_labels"},{"location":"reference/text_explainability/global_explanation/#predict_2","text":"1 2 3 4 def predict ( self , model : instancelib . machinelearning . base . AbstractClassifier ) -> Union [ Sequence [ FrozenSet [ str ]], numpy . ndarray ] Apply predict function of model to data. Parameters: Name Type Description Default model AbstractClassifier Model to apply predictions with. None Returns: Type Description Union[Sequence[FrozenSet[str]], np.ndarray] Labels for dataset according to model. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def predict(self, model: AbstractClassifier) -> Union[Sequence[FrozenSet[str]], np.ndarray]: \"\"\"Apply predict function of model to data. Args: model (AbstractClassifier): Model to apply predictions with. Returns: Union[Sequence[FrozenSet[str]], np.ndarray]: Labels for dataset according to model. \"\"\" return model.predict(self.get_data())","title":"predict"},{"location":"reference/text_explainability/global_explanation/#reset_seed_2","text":"1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self","title":"reset_seed"},{"location":"reference/text_explainability/global_explanation/#set_seed_2","text":"1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed()","title":"set_seed"},{"location":"reference/text_explainability/local_explanation/","text":"Module text_explainability.local_explanation Local explanations explain why a model made a prediction for a single instance. Todo: 1 2 * Implement Anchors * Various bugfixes View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 \"\"\"Local explanations explain why a model made a prediction for a single instance. Todo: * Implement Anchors * Various bugfixes \"\"\" import math import sys from typing import Callable, Optional, Sequence, Tuple, Union import numpy as np import six from genbase import Readable, SeedMixin, add_callargs from instancelib import (AbstractEnvironment, InstanceProvider, LabelProvider, MemoryLabelProvider, TextEnvironment) from instancelib.instances.text import TextInstance, TextInstanceProvider from instancelib.machinelearning import AbstractClassifier from sklearn.linear_model import Ridge from sklearn.tree import DecisionTreeClassifier from .data.augmentation import LeaveOut, LocalTokenPertubator from .data.weights import exponential_kernel, pairwise_distances from .decorators import text_instance from .generation.feature_selection import FeatureSelector from .generation.return_types import FeatureAttribution, Rules from .generation.surrogate import LinearSurrogate, RuleSurrogate, TreeSurrogate from .generation.target_encoding import FactFoilEncoder from .utils import binarize, default_detokenizer sys.modules['sklearn.externals.six'] = six # ensure backward compatibility from skrules import SkopeRules # noqa: E402 def default_env(env: Optional[AbstractEnvironment] = None) -> AbstractEnvironment: \"\"\"If no environment is supplied, an empty Enviroment is created for text data. Args: env (Optional[AbstractEnvironment], optional): If a environment is supplied, it is used, otherwise. Returns: AbstractEnvironment: The default/supplied environment. \"\"\" if env is not None: return env empty_dataset = TextInstanceProvider([]) empty_labels = MemoryLabelProvider([], {}) empty_env = TextEnvironment(empty_dataset, empty_labels) return empty_env class LocalExplanation(Readable, SeedMixin): def __init__(self, env: Optional[AbstractEnvironment] = None, augmenter: Optional[LocalTokenPertubator] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, seed: int = 0): \"\"\"Generate explanation for a single decision. Args: env (Optional[AbstractEnvironment], optional): Environment to save local perturbations in. Defaults to None. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__() self.env = default_env(env) if augmenter is None: augmenter = LeaveOut(env=self.env, detokenizer=default_detokenizer) if isinstance(labelset, LabelProvider) and hasattr(labelset, 'labelset'): labelset = list(labelset.labelset) elif labelset is None and self.env is not None: if hasattr(self.env.labels, 'labelset'): labelset = list(self.env.labels.labelset) self.labelset = labelset self.augmenter = augmenter self._seed = self._original_seed = seed @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed def explain(self, *args, **kwargs): return self(*args, **kwargs) class WeightedExplanation: def __init__(self, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25): \"\"\"Add weights to neighborhood data. Args: kernel (Optional[Callable], optional): Kernel to determine similarity of perturbed instances to original instance (if set to None defaults to `data.weights.exponential_kernel`). Defaults to None. kernel_width (Union[int, float], optional): Hyperparameter for similarity function of kernel. Defaults to 25. \"\"\" if kernel is None: kernel = exponential_kernel self.kernel_fn = lambda d: kernel(d, kernel_width) def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric)) class LIME(LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, local_model: Optional[LinearSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, augmenter: Optional[LocalTokenPertubator] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, seed: int = 0): \"\"\"Local Interpretable Model-Agnostic Explanations (`LIME`_). Implementation of local linear surrogate model on (weighted) perturbed text data, to get feature attribution scores for an example instance. Args: env (Optional[AbstractEnvironment]): Environment to save local perturbations in. Defaults to None. local_model (Optional[LinearSurrogate], optional): Local linear model. Defaults to None. kernel (Optional[Callable], optional): Kernel to determine similarity of perturbed instances to original instance. Defaults to None. kernel_width (Union[int, float], optional): Hyperparameter for similarity function of kernel. Defaults to 25. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _LIME: https://github.com/marcotcr/lime \"\"\" LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = LinearSurrogate(Ridge(alpha=1, fit_intercept=True, random_state=self.seed)) self.local_model = local_model @add_callargs @text_instance(tokenize=True) def __call__(self, sample: TextInstance, model: AbstractClassifier, labels: Optional[Union[Sequence[int], Sequence[str]]] = None, n_samples: int = 50, n_features: int = 10, feature_selection_method: str = 'auto', weigh_samples: bool = True, distance_metric: str = 'cosine', **kwargs) -> FeatureAttribution: \"\"\"Calculate feature attribution scores using `LIME Text`_. Args: sample (TextInstance): Instance to explain. model (AbstractClassifier): Model to explain. labels (Optional[Union[Sequence[int], Sequence[str]]], optional): [description]. Defaults to None. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. n_features (int, optional): Maximum number of features to include (explanation length). Defaults to 10. feature_selection_method (str, optional): Method for limiting number of features, either `forward_selection`, `highest_weights` or `auto`. Defaults to 'auto'. weigh_samples (bool, optional): Whether to locally weigh samples based on their similarity to the original instance. Defaults to True. distance_metric (str, optional): Distance metric for local weighting. Defaults to 'cosine'. Raises: ValueError: Can only provide labels from labelset if self.labelset is not None Returns: FeatureAttribution: [description] .. _LIME Text: https://github.com/marcotcr/lime/blob/master/lime/lime_text.py \"\"\" if labels is not None: if isinstance(labels, (int, str)): labels = [labels] n_labels = sum(1 for _ in iter(labels)) if n_labels > 0 and isinstance(next(iter(labels)), str): if self.labelset is None: raise ValueError('Can only provide label names when such a list exists in self.labelset') labels = [self.labelset.index(label) for label in labels] # Generate neighborhood samples provider, original_id, perturbed, y = self.augment_sample(sample, model, sequential=False, contiguous=False, n_samples=n_samples) perturbed = binarize(perturbed) # flatten all n replacements into one if weigh_samples: weights = self.weigh_samples(perturbed, metric=distance_metric) if feature_selection_method == 'auto': feature_selection_method = 'forward_selection' if n_features <= 6 else 'highest_weights' feature_importances, used_features = [], {} if labels is None: labels = np.arange(y.shape[1]) for label in labels: # Look at output for label y_label = y[:, label].copy() # Get the most important features features = FeatureSelector(self.local_model)(perturbed, y_label, weights=weights, n_features=n_features, method=feature_selection_method) # Fit explanation model self.local_model.alpha_reset() self.local_model.fit(perturbed[:, features], y_label, weights=weights) feature_importances.append(self.local_model.feature_importances) used_features[label] = features return FeatureAttribution(provider=provider, original_id=original_id, scores=feature_importances, used_features=used_features, labels=labels, labelset=self.labelset, type='local_explanation', method='lime', callargs=kwargs.pop('__callargs__', None)) class KernelSHAP(LocalExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: LocalTokenPertubator = None, seed: int = 0): \"\"\"Calculates `Shapley values`_ for an instance to explain, assuming the model is a black-box. Calculates Shapley values (local, additive feature attribution scores) for an instance to explain, by calculating the average contribution of changing combinations of feature values. Args: env (Optional[AbstractEnvironment], optional): Environment to save local perturbations in. Defaults to None. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _Shapley values: https://github.com/slundberg/shap \"\"\" super().__init__(env=env, augmenter=augmenter, labelset=labelset, seed=seed) @staticmethod def select_features(X: np.ndarray, y: np.ndarray, default_features: int = 1, l1_reg: Union[int, float, str] = 'auto') -> np.ndarray: \"\"\"Select features for data X and corresponding output y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. default_features (int, optional): Default number of features, when returning all features. Defaults to 1. l1_reg (Union[int, float, str], optional): Method for regularization, either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Raises: Exception: Unknown value for `l1_reg` Returns: np.ndarray: Feature indices to include. \"\"\" feature_selector = FeatureSelector() nonzero = np.arange(default_features) if isinstance(l1_reg, str) and l1_reg.startswith('n_features('): l1_reg = int(l1_reg[len('n_features('):-1]) if isinstance(l1_reg, int): nonzero = feature_selector(X, y, n_features=l1_reg, method='l1_reg') elif isinstance(l1_reg, float): nonzero = feature_selector(X, y, method='l1_reg', alpha=l1_reg) elif l1_reg in ['auto', 'aic', 'bic']: if l1_reg == 'auto': l1_reg = 'aic' nonzero = feature_selector(X, y, method=l1_reg) else: raise Exception(f'Unknown value \"{l1_reg}\" for l1_reg') return nonzero @add_callargs @text_instance(tokenize=True) def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: Optional[int] = None, l1_reg: Union[int, float, str] = 'auto', **kwargs) -> FeatureAttribution: \"\"\"Calculate feature attribution scores using `KernelShap`_. Args: sample (TextInstance): Instance to explain. model (AbstractClassifier): Model to explain. n_samples (Optional[int], optional): Number of neighborhood samples to generate (if None defaults to `2 * sample_len + 2 ** 11`). Defaults to None. l1_reg (Union[int, float, str], optional): Method for regularization (limiting number of features), either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Returns: FeatureAttribution: [description] .. _KernelShap: https://github.com/slundberg/shap/blob/master/shap/explainers/_kernel.py \"\"\" sample_len = len(sample.tokenized) if n_samples is None: n_samples = 2 * sample_len + 2 ** 11 n_samples = min(n_samples, 2 ** 30) provider, original_id, perturbed, y = self.augment_sample(sample, model, sequential=True, contiguous=False, n_samples=n_samples, add_background_instance=True) # TODO: exclude non-varying feature groups y_null, y = y[-1], y[1:-1] y -= y_null used_features = np.arange(perturbed.shape[1]) phi = np.zeros([sample_len, y.shape[1]]) phi_var = np.zeros(sample_len) if perturbed.shape[1] == 1: phi = np.mean(y - y_null, axis=0).reshape(1, -1) elif perturbed.shape[1] > 1: # Weigh samples M = perturbed.shape[1] Z = np.sum(perturbed[1:-1], axis=1).astype(int) weight_vector = np.array([(M - 1) / (math.comb(M, m) * m * (M - m)) for m in range(1, M)]) weight_vector = np.append(weight_vector, [np.mean(weight_vector)]) # TODO: replace hotfix weight_vector /= np.sum(weight_vector) kernel_weights = weight_vector[Z - 1] nonzero = KernelSHAP.select_features(perturbed[1:-1], y, default_features=sample_len, l1_reg=l1_reg) used_features = nonzero phi_var = np.ones(sample_len) if len(used_features) > 0: X = perturbed[1:-1] X_W = np.dot(X.T, np.diag(kernel_weights)) try: tmp2 = np.linalg.inv(np.dot(X_W, X)) except np.linalg.LinAlgError: tmp2 = np.linalg.pinv(np.dot(X_W, X)) phi = np.dot(tmp2, np.dot(X_W, y)).T return FeatureAttribution(provider=provider, original_id=original_id, scores=phi, scores_stddev=phi_var, base_score=y_null, used_features=used_features, labels=np.arange(y.shape[1]), labelset=self.labelset, type='local_explanation', method='kernel_shap', callargs=kwargs.pop('__callargs__', None)) class Anchor(LocalExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, seed: int = 0): super().__init__(env=env, augmenter=augmenter, labelset=labelset, seed=seed) @staticmethod def kl_bernoulli(p, q): p = float(min(1 - 1e-15, max(1e-15, p))) q = float(min(1 - 1e-15, max(1e-15, q))) return (p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))) @staticmethod def dlow_bernoulli(p, level): lm = max(min(1, p - np.sqrt(level / 2.0)), 0.0) qm = (p + lm) / 2.0 if Anchor.kl_bernoulli(p, qm) > level: lm = qm return lm def generate_candidates(self,): pass def best_candidate(self): pass @staticmethod def beam_search(provider, perturbed: np.ndarray, model, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None, batch_size: int = 20): # TODO: add value checking to decorator if beam_size < 1: raise ValueError(f'{beam_size=} should be at least 1.') if not (0.0 <= min_confidence <= 1.0): raise ValueError(f'{min_confidence=} should be in range [0, 1].') if not (0.0 <= delta <= 1.0): raise ValueError(f'{delta=} should be in range [0, 1].') if not (0.0 <= epsilon <= 1.0): raise ValueError(f'{epsilon=} should be in range [0, 1].') if batch_size < 2: raise ValueError(f'{batch_size=} should be at least 2.') for batch in provider.instance_chunker(batch_size): y = list(model.predict_proba_raw(batch))[-1][-1] # todo: only look at probs of one class y_true, y = y[0], y[1:] # noqa: F841 beta = np.log(1.0 / delta) mean = y.mean() lb = Anchor.dlow_bernoulli(mean, beta / perturbed.shape[0]) if not(mean > min_confidence and lb < min_confidence - epsilon): break raise NotImplementedError('[WIP] Implementing anchor/anchor_base.py') @add_callargs @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: int = 100, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None, **kwargs): raise NotImplementedError('Only partially implemented') # https://github.com/marcotcr/anchor/blob/master/anchor/anchor_text.py # https://github.com/marcotcr/anchor/blob/master/anchor/anchor_base.py provider, original_id, perturbed = self.augment_sample(sample, None, sequential=False, contiguous=False, n_samples=n_samples, predict=False) perturbed = binarize(perturbed[1:]) # flatten all n replacements into one y_true = np.argmax(model.predict_proba([provider[0]])[0][-1]) # noqa: F841 # Use beam from https://homes.cs.washington.edu/~marcotcr/aaai18.pdf (Algorithm 2) anchor = Anchor.beam_search(provider, # noqa: F841 perturbed, model, beam_size=beam_size, min_confidence=min_confidence, delta=delta, epsilon=epsilon, max_anchor_size=max_anchor_size, batch_size=n_samples // 10 if n_samples >= 1000 else n_samples // 5) pass class LocalTree(LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[TreeSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = TreeSurrogate(DecisionTreeClassifier(random_state=self.seed)) self.local_model = local_model self.explanation_type = explanation_type @add_callargs @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', max_rule_size: Optional[int] = None, **sample_kwargs): callargs = sample_kwargs.pop('__callargs__', None) provider, original_id, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.max_rule_size = max_rule_size self.local_model.fit(perturbed, y, weights=weights) return Rules(provider=provider, original_id=original_id, rules=self.local_model, labelset=self.labelset, sampled=True, type='local_explanation', method='local_tree', callargs=callargs) class FactFoilMixin: def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y) class FoilTree(FactFoilMixin, LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[TreeSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = TreeSurrogate(DecisionTreeClassifier(random_state=self.seed)) self.local_model = local_model self.explanation_type = explanation_type @add_callargs @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, foil_fn: Union[FactFoilEncoder, int, str], n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', max_rule_size: Optional[int] = None, **sample_kwargs): callargs = sample_kwargs.pop('__callargs__', None) provider, original_id, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one # Encode foil as 0 and rest as 1 labelset = self.labelset if self.labelset else model y_ = self.to_fact_foil(y, labelset, foil_fn) weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.max_rule_size = max_rule_size self.local_model.fit(perturbed, y_, weights=weights) # TODO: pass to which label the Foil Tree applies return Rules(provider=provider, original_id=original_id, rules=self.local_model, labelset=labelset, sampled=True, type='local_explanation', method='foil_tree', callargs=callargs) class LocalRules(FactFoilMixin, LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[RuleSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = RuleSurrogate(SkopeRules(max_depth_duplication=2, n_estimators=30, random_state=self.seed)) self.local_model = local_model self.explanation_type = explanation_type @add_callargs @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, foil_fn: Union[FactFoilEncoder, int, str], n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', **sample_kwargs): callargs = sample_kwargs.pop('__callargs__', None) provider, original_id, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one # Encode foil as 0 and rest as 1 labelset = self.labelset if self.labelset else model y_ = self.to_fact_foil(y, labelset, foil_fn) weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.fit(perturbed, y_, weights=weights) return Rules(provider, original_id=original_id, rules=self.local_model, labelset=labelset, sampled=True, type='local_explanation', method='local_rules', callargs=callargs) Functions default_env 1 2 3 def default_env ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None ) -> instancelib . environment . base . AbstractEnvironment If no environment is supplied, an empty Enviroment is created for text data. Parameters: Name Type Description Default env Optional[AbstractEnvironment] If a environment is supplied, it is used, otherwise. None Returns: Type Description AbstractEnvironment The default/supplied environment. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def default_env(env: Optional[AbstractEnvironment] = None) -> AbstractEnvironment: \"\"\"If no environment is supplied, an empty Enviroment is created for text data. Args: env (Optional[AbstractEnvironment], optional): If a environment is supplied, it is used, otherwise. Returns: AbstractEnvironment: The default/supplied environment. \"\"\" if env is not None: return env empty_dataset = TextInstanceProvider([]) empty_labels = MemoryLabelProvider([], {}) empty_env = TextEnvironment(empty_dataset, empty_labels) return empty_env Classes Anchor 1 2 3 4 5 6 class Anchor ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 class Anchor(LocalExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, seed: int = 0): super().__init__(env=env, augmenter=augmenter, labelset=labelset, seed=seed) @staticmethod def kl_bernoulli(p, q): p = float(min(1 - 1e-15, max(1e-15, p))) q = float(min(1 - 1e-15, max(1e-15, q))) return (p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))) @staticmethod def dlow_bernoulli(p, level): lm = max(min(1, p - np.sqrt(level / 2.0)), 0.0) qm = (p + lm) / 2.0 if Anchor.kl_bernoulli(p, qm) > level: lm = qm return lm def generate_candidates(self,): pass def best_candidate(self): pass @staticmethod def beam_search(provider, perturbed: np.ndarray, model, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None, batch_size: int = 20): # TODO: add value checking to decorator if beam_size < 1: raise ValueError(f'{beam_size=} should be at least 1.') if not (0.0 <= min_confidence <= 1.0): raise ValueError(f'{min_confidence=} should be in range [0, 1].') if not (0.0 <= delta <= 1.0): raise ValueError(f'{delta=} should be in range [0, 1].') if not (0.0 <= epsilon <= 1.0): raise ValueError(f'{epsilon=} should be in range [0, 1].') if batch_size < 2: raise ValueError(f'{batch_size=} should be at least 2.') for batch in provider.instance_chunker(batch_size): y = list(model.predict_proba_raw(batch))[-1][-1] # todo: only look at probs of one class y_true, y = y[0], y[1:] # noqa: F841 beta = np.log(1.0 / delta) mean = y.mean() lb = Anchor.dlow_bernoulli(mean, beta / perturbed.shape[0]) if not(mean > min_confidence and lb < min_confidence - epsilon): break raise NotImplementedError('[WIP] Implementing anchor/anchor_base.py') @add_callargs @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: int = 100, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None, **kwargs): raise NotImplementedError('Only partially implemented') # https://github.com/marcotcr/anchor/blob/master/anchor/anchor_text.py # https://github.com/marcotcr/anchor/blob/master/anchor/anchor_base.py provider, original_id, perturbed = self.augment_sample(sample, None, sequential=False, contiguous=False, n_samples=n_samples, predict=False) perturbed = binarize(perturbed[1:]) # flatten all n replacements into one y_true = np.argmax(model.predict_proba([provider[0]])[0][-1]) # noqa: F841 # Use beam from https://homes.cs.washington.edu/~marcotcr/aaai18.pdf (Algorithm 2) anchor = Anchor.beam_search(provider, # noqa: F841 perturbed, model, beam_size=beam_size, min_confidence=min_confidence, delta=delta, epsilon=epsilon, max_anchor_size=max_anchor_size, batch_size=n_samples // 10 if n_samples >= 1000 else n_samples // 5) pass Ancestors (in MRO) text_explainability.local_explanation.LocalExplanation genbase.Readable genbase.mixin.SeedMixin Static methods beam_search 1 2 3 4 5 6 7 8 9 10 11 def beam_search ( provider , perturbed : numpy . ndarray , model , beam_size : int = 1 , min_confidence : float = 0.95 , delta : float = 0.05 , epsilon : float = 0.1 , max_anchor_size : Optional [ int ] = None , batch_size : int = 20 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @staticmethod def beam_search(provider, perturbed: np.ndarray, model, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None, batch_size: int = 20): # TODO: add value checking to decorator if beam_size < 1: raise ValueError(f'{beam_size=} should be at least 1.') if not (0.0 <= min_confidence <= 1.0): raise ValueError(f'{min_confidence=} should be in range [0, 1].') if not (0.0 <= delta <= 1.0): raise ValueError(f'{delta=} should be in range [0, 1].') if not (0.0 <= epsilon <= 1.0): raise ValueError(f'{epsilon=} should be in range [0, 1].') if batch_size < 2: raise ValueError(f'{batch_size=} should be at least 2.') for batch in provider.instance_chunker(batch_size): y = list(model.predict_proba_raw(batch))[-1][-1] # todo: only look at probs of one class y_true, y = y[0], y[1:] # noqa: F841 beta = np.log(1.0 / delta) mean = y.mean() lb = Anchor.dlow_bernoulli(mean, beta / perturbed.shape[0]) if not(mean > min_confidence and lb < min_confidence - epsilon): break raise NotImplementedError('[WIP] Implementing anchor/anchor_base.py') dlow_bernoulli 1 2 3 4 def dlow_bernoulli ( p , level ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 @staticmethod def dlow_bernoulli(p, level): lm = max(min(1, p - np.sqrt(level / 2.0)), 0.0) qm = (p + lm) / 2.0 if Anchor.kl_bernoulli(p, qm) > level: lm = qm return lm kl_bernoulli 1 2 3 4 def kl_bernoulli ( p , q ) View Source 1 2 3 4 5 6 7 8 9 10 11 @staticmethod def kl_bernoulli(p, q): p = float(min(1 - 1e-15, max(1e-15, p))) q = float(min(1 - 1e-15, max(1e-15, q))) return (p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))) Instance variables 1 seed Methods augment_sample 1 2 3 4 5 6 7 8 9 10 11 12 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False , ** kwargs ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed best_candidate 1 2 3 def best_candidate ( self ) View Source 1 2 3 def best_candidate(self): pass explain 1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs) generate_candidates 1 2 3 def generate_candidates ( self ) View Source 1 2 3 def generate_candidates(self,): pass reset_seed 1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self set_seed 1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed() FactFoilMixin 1 2 3 4 5 class FactFoilMixin ( / , * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 class FactFoilMixin: def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y) Descendants text_explainability.local_explanation.FoilTree text_explainability.local_explanation.LocalRules Methods to_fact_foil 1 2 3 4 5 6 def to_fact_foil ( self , y , labelset , foil_fn : Union [ text_explainability . generation . target_encoding . FactFoilEncoder , int , str ] ) View Source 1 2 3 4 5 6 7 8 9 10 11 def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y) FoilTree 1 2 3 4 5 6 7 8 9 10 class FoilTree ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , local_model : Optional [ text_explainability . generation . surrogate . TreeSurrogate ] = None , kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 , explanation_type : str = 'multiclass' , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 class FoilTree(FactFoilMixin, LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[TreeSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = TreeSurrogate(DecisionTreeClassifier(random_state=self.seed)) self.local_model = local_model self.explanation_type = explanation_type @add_callargs @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, foil_fn: Union[FactFoilEncoder, int, str], n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', max_rule_size: Optional[int] = None, **sample_kwargs): callargs = sample_kwargs.pop('__callargs__', None) provider, original_id, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one # Encode foil as 0 and rest as 1 labelset = self.labelset if self.labelset else model y_ = self.to_fact_foil(y, labelset, foil_fn) weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.max_rule_size = max_rule_size self.local_model.fit(perturbed, y_, weights=weights) # TODO: pass to which label the Foil Tree applies return Rules(provider=provider, original_id=original_id, rules=self.local_model, labelset=labelset, sampled=True, type='local_explanation', method='foil_tree', callargs=callargs) Ancestors (in MRO) text_explainability.local_explanation.FactFoilMixin text_explainability.local_explanation.LocalExplanation genbase.Readable genbase.mixin.SeedMixin text_explainability.local_explanation.WeightedExplanation Instance variables 1 seed Methods augment_sample 1 2 3 4 5 6 7 8 9 10 11 12 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False , ** kwargs ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed explain 1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs) reset_seed 1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self set_seed 1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed() to_fact_foil 1 2 3 4 5 6 def to_fact_foil ( self , y , labelset , foil_fn : Union [ text_explainability . generation . target_encoding . FactFoilEncoder , int , str ] ) View Source 1 2 3 4 5 6 7 8 9 10 11 def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y) weigh_samples 1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric)) KernelSHAP 1 2 3 4 5 6 class KernelSHAP ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : text_explainability . data . augmentation . LocalTokenPertubator = None , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 class KernelSHAP(LocalExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: LocalTokenPertubator = None, seed: int = 0): \"\"\"Calculates `Shapley values`_ for an instance to explain, assuming the model is a black-box. Calculates Shapley values (local, additive feature attribution scores) for an instance to explain, by calculating the average contribution of changing combinations of feature values. Args: env (Optional[AbstractEnvironment], optional): Environment to save local perturbations in. Defaults to None. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _Shapley values: https://github.com/slundberg/shap \"\"\" super().__init__(env=env, augmenter=augmenter, labelset=labelset, seed=seed) @staticmethod def select_features(X: np.ndarray, y: np.ndarray, default_features: int = 1, l1_reg: Union[int, float, str] = 'auto') -> np.ndarray: \"\"\"Select features for data X and corresponding output y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. default_features (int, optional): Default number of features, when returning all features. Defaults to 1. l1_reg (Union[int, float, str], optional): Method for regularization, either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Raises: Exception: Unknown value for `l1_reg` Returns: np.ndarray: Feature indices to include. \"\"\" feature_selector = FeatureSelector() nonzero = np.arange(default_features) if isinstance(l1_reg, str) and l1_reg.startswith('n_features('): l1_reg = int(l1_reg[len('n_features('):-1]) if isinstance(l1_reg, int): nonzero = feature_selector(X, y, n_features=l1_reg, method='l1_reg') elif isinstance(l1_reg, float): nonzero = feature_selector(X, y, method='l1_reg', alpha=l1_reg) elif l1_reg in ['auto', 'aic', 'bic']: if l1_reg == 'auto': l1_reg = 'aic' nonzero = feature_selector(X, y, method=l1_reg) else: raise Exception(f'Unknown value \"{l1_reg}\" for l1_reg') return nonzero @add_callargs @text_instance(tokenize=True) def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: Optional[int] = None, l1_reg: Union[int, float, str] = 'auto', **kwargs) -> FeatureAttribution: \"\"\"Calculate feature attribution scores using `KernelShap`_. Args: sample (TextInstance): Instance to explain. model (AbstractClassifier): Model to explain. n_samples (Optional[int], optional): Number of neighborhood samples to generate (if None defaults to `2 * sample_len + 2 ** 11`). Defaults to None. l1_reg (Union[int, float, str], optional): Method for regularization (limiting number of features), either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Returns: FeatureAttribution: [description] .. _KernelShap: https://github.com/slundberg/shap/blob/master/shap/explainers/_kernel.py \"\"\" sample_len = len(sample.tokenized) if n_samples is None: n_samples = 2 * sample_len + 2 ** 11 n_samples = min(n_samples, 2 ** 30) provider, original_id, perturbed, y = self.augment_sample(sample, model, sequential=True, contiguous=False, n_samples=n_samples, add_background_instance=True) # TODO: exclude non-varying feature groups y_null, y = y[-1], y[1:-1] y -= y_null used_features = np.arange(perturbed.shape[1]) phi = np.zeros([sample_len, y.shape[1]]) phi_var = np.zeros(sample_len) if perturbed.shape[1] == 1: phi = np.mean(y - y_null, axis=0).reshape(1, -1) elif perturbed.shape[1] > 1: # Weigh samples M = perturbed.shape[1] Z = np.sum(perturbed[1:-1], axis=1).astype(int) weight_vector = np.array([(M - 1) / (math.comb(M, m) * m * (M - m)) for m in range(1, M)]) weight_vector = np.append(weight_vector, [np.mean(weight_vector)]) # TODO: replace hotfix weight_vector /= np.sum(weight_vector) kernel_weights = weight_vector[Z - 1] nonzero = KernelSHAP.select_features(perturbed[1:-1], y, default_features=sample_len, l1_reg=l1_reg) used_features = nonzero phi_var = np.ones(sample_len) if len(used_features) > 0: X = perturbed[1:-1] X_W = np.dot(X.T, np.diag(kernel_weights)) try: tmp2 = np.linalg.inv(np.dot(X_W, X)) except np.linalg.LinAlgError: tmp2 = np.linalg.pinv(np.dot(X_W, X)) phi = np.dot(tmp2, np.dot(X_W, y)).T return FeatureAttribution(provider=provider, original_id=original_id, scores=phi, scores_stddev=phi_var, base_score=y_null, used_features=used_features, labels=np.arange(y.shape[1]), labelset=self.labelset, type='local_explanation', method='kernel_shap', callargs=kwargs.pop('__callargs__', None)) Ancestors (in MRO) text_explainability.local_explanation.LocalExplanation genbase.Readable genbase.mixin.SeedMixin Static methods select_features 1 2 3 4 5 6 def select_features ( X : numpy . ndarray , y : numpy . ndarray , default_features : int = 1 , l1_reg : Union [ int , float , str ] = 'auto' ) -> numpy . ndarray Select features for data X and corresponding output y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. default_features (int, optional): Default number of features, when returning all features. Defaults to 1. l1_reg (Union[int, float, str], optional): Method for regularization, either auto , n_features({int}) , {int} , {float} , aic or bic . Defaults to 'auto'. Raises: Exception: Unknown value for l1_reg Returns: np.ndarray: Feature indices to include. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @staticmethod def select_features(X: np.ndarray, y: np.ndarray, default_features: int = 1, l1_reg: Union[int, float, str] = 'auto') -> np.ndarray: \"\"\"Select features for data X and corresponding output y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. default_features (int, optional): Default number of features, when returning all features. Defaults to 1. l1_reg (Union[int, float, str], optional): Method for regularization, either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Raises: Exception: Unknown value for `l1_reg` Returns: np.ndarray: Feature indices to include. \"\"\" feature_selector = FeatureSelector() nonzero = np.arange(default_features) if isinstance(l1_reg, str) and l1_reg.startswith('n_features('): l1_reg = int(l1_reg[len('n_features('):-1]) if isinstance(l1_reg, int): nonzero = feature_selector(X, y, n_features=l1_reg, method='l1_reg') elif isinstance(l1_reg, float): nonzero = feature_selector(X, y, method='l1_reg', alpha=l1_reg) elif l1_reg in ['auto', 'aic', 'bic']: if l1_reg == 'auto': l1_reg = 'aic' nonzero = feature_selector(X, y, method=l1_reg) else: raise Exception(f'Unknown value \"{l1_reg}\" for l1_reg') return nonzero Instance variables 1 seed Methods augment_sample 1 2 3 4 5 6 7 8 9 10 11 12 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False , ** kwargs ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed explain 1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs) reset_seed 1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self set_seed 1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed() LIME 1 2 3 4 5 6 7 8 9 class LIME ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , local_model : Optional [ text_explainability . generation . surrogate . LinearSurrogate ] = None , kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 class LIME(LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, local_model: Optional[LinearSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, augmenter: Optional[LocalTokenPertubator] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, seed: int = 0): \"\"\"Local Interpretable Model-Agnostic Explanations (`LIME`_). Implementation of local linear surrogate model on (weighted) perturbed text data, to get feature attribution scores for an example instance. Args: env (Optional[AbstractEnvironment]): Environment to save local perturbations in. Defaults to None. local_model (Optional[LinearSurrogate], optional): Local linear model. Defaults to None. kernel (Optional[Callable], optional): Kernel to determine similarity of perturbed instances to original instance. Defaults to None. kernel_width (Union[int, float], optional): Hyperparameter for similarity function of kernel. Defaults to 25. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _LIME: https://github.com/marcotcr/lime \"\"\" LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = LinearSurrogate(Ridge(alpha=1, fit_intercept=True, random_state=self.seed)) self.local_model = local_model @add_callargs @text_instance(tokenize=True) def __call__(self, sample: TextInstance, model: AbstractClassifier, labels: Optional[Union[Sequence[int], Sequence[str]]] = None, n_samples: int = 50, n_features: int = 10, feature_selection_method: str = 'auto', weigh_samples: bool = True, distance_metric: str = 'cosine', **kwargs) -> FeatureAttribution: \"\"\"Calculate feature attribution scores using `LIME Text`_. Args: sample (TextInstance): Instance to explain. model (AbstractClassifier): Model to explain. labels (Optional[Union[Sequence[int], Sequence[str]]], optional): [description]. Defaults to None. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. n_features (int, optional): Maximum number of features to include (explanation length). Defaults to 10. feature_selection_method (str, optional): Method for limiting number of features, either `forward_selection`, `highest_weights` or `auto`. Defaults to 'auto'. weigh_samples (bool, optional): Whether to locally weigh samples based on their similarity to the original instance. Defaults to True. distance_metric (str, optional): Distance metric for local weighting. Defaults to 'cosine'. Raises: ValueError: Can only provide labels from labelset if self.labelset is not None Returns: FeatureAttribution: [description] .. _LIME Text: https://github.com/marcotcr/lime/blob/master/lime/lime_text.py \"\"\" if labels is not None: if isinstance(labels, (int, str)): labels = [labels] n_labels = sum(1 for _ in iter(labels)) if n_labels > 0 and isinstance(next(iter(labels)), str): if self.labelset is None: raise ValueError('Can only provide label names when such a list exists in self.labelset') labels = [self.labelset.index(label) for label in labels] # Generate neighborhood samples provider, original_id, perturbed, y = self.augment_sample(sample, model, sequential=False, contiguous=False, n_samples=n_samples) perturbed = binarize(perturbed) # flatten all n replacements into one if weigh_samples: weights = self.weigh_samples(perturbed, metric=distance_metric) if feature_selection_method == 'auto': feature_selection_method = 'forward_selection' if n_features <= 6 else 'highest_weights' feature_importances, used_features = [], {} if labels is None: labels = np.arange(y.shape[1]) for label in labels: # Look at output for label y_label = y[:, label].copy() # Get the most important features features = FeatureSelector(self.local_model)(perturbed, y_label, weights=weights, n_features=n_features, method=feature_selection_method) # Fit explanation model self.local_model.alpha_reset() self.local_model.fit(perturbed[:, features], y_label, weights=weights) feature_importances.append(self.local_model.feature_importances) used_features[label] = features return FeatureAttribution(provider=provider, original_id=original_id, scores=feature_importances, used_features=used_features, labels=labels, labelset=self.labelset, type='local_explanation', method='lime', callargs=kwargs.pop('__callargs__', None)) Ancestors (in MRO) text_explainability.local_explanation.LocalExplanation genbase.Readable genbase.mixin.SeedMixin text_explainability.local_explanation.WeightedExplanation Instance variables 1 seed Methods augment_sample 1 2 3 4 5 6 7 8 9 10 11 12 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False , ** kwargs ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed explain 1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs) reset_seed 1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self set_seed 1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed() weigh_samples 1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric)) LocalExplanation 1 2 3 4 5 6 class LocalExplanation ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 class LocalExplanation(Readable, SeedMixin): def __init__(self, env: Optional[AbstractEnvironment] = None, augmenter: Optional[LocalTokenPertubator] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, seed: int = 0): \"\"\"Generate explanation for a single decision. Args: env (Optional[AbstractEnvironment], optional): Environment to save local perturbations in. Defaults to None. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__() self.env = default_env(env) if augmenter is None: augmenter = LeaveOut(env=self.env, detokenizer=default_detokenizer) if isinstance(labelset, LabelProvider) and hasattr(labelset, 'labelset'): labelset = list(labelset.labelset) elif labelset is None and self.env is not None: if hasattr(self.env.labels, 'labelset'): labelset = list(self.env.labels.labelset) self.labelset = labelset self.augmenter = augmenter self._seed = self._original_seed = seed @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed def explain(self, *args, **kwargs): return self(*args, **kwargs) Ancestors (in MRO) genbase.Readable genbase.mixin.SeedMixin Descendants text_explainability.local_explanation.LIME text_explainability.local_explanation.KernelSHAP text_explainability.local_explanation.Anchor text_explainability.local_explanation.LocalTree text_explainability.local_explanation.FoilTree text_explainability.local_explanation.LocalRules Instance variables 1 seed Methods augment_sample 1 2 3 4 5 6 7 8 9 10 11 12 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False , ** kwargs ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed explain 1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs) reset_seed 1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self set_seed 1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed() LocalRules 1 2 3 4 5 6 7 8 9 10 class LocalRules ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , local_model : Optional [ text_explainability . generation . surrogate . RuleSurrogate ] = None , kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 , explanation_type : str = 'multiclass' , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class LocalRules(FactFoilMixin, LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[RuleSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = RuleSurrogate(SkopeRules(max_depth_duplication=2, n_estimators=30, random_state=self.seed)) self.local_model = local_model self.explanation_type = explanation_type @add_callargs @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, foil_fn: Union[FactFoilEncoder, int, str], n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', **sample_kwargs): callargs = sample_kwargs.pop('__callargs__', None) provider, original_id, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one # Encode foil as 0 and rest as 1 labelset = self.labelset if self.labelset else model y_ = self.to_fact_foil(y, labelset, foil_fn) weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.fit(perturbed, y_, weights=weights) return Rules(provider, original_id=original_id, rules=self.local_model, labelset=labelset, sampled=True, type='local_explanation', method='local_rules', callargs=callargs) Ancestors (in MRO) text_explainability.local_explanation.FactFoilMixin text_explainability.local_explanation.LocalExplanation genbase.Readable genbase.mixin.SeedMixin text_explainability.local_explanation.WeightedExplanation Instance variables 1 seed Methods augment_sample 1 2 3 4 5 6 7 8 9 10 11 12 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False , ** kwargs ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed explain 1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs) reset_seed 1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self set_seed 1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed() to_fact_foil 1 2 3 4 5 6 def to_fact_foil ( self , y , labelset , foil_fn : Union [ text_explainability . generation . target_encoding . FactFoilEncoder , int , str ] ) View Source 1 2 3 4 5 6 7 8 9 10 11 def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y) weigh_samples 1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric)) LocalTree 1 2 3 4 5 6 7 8 9 10 class LocalTree ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , local_model : Optional [ text_explainability . generation . surrogate . TreeSurrogate ] = None , kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 , explanation_type : str = 'multiclass' , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class LocalTree(LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[TreeSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = TreeSurrogate(DecisionTreeClassifier(random_state=self.seed)) self.local_model = local_model self.explanation_type = explanation_type @add_callargs @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', max_rule_size: Optional[int] = None, **sample_kwargs): callargs = sample_kwargs.pop('__callargs__', None) provider, original_id, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.max_rule_size = max_rule_size self.local_model.fit(perturbed, y, weights=weights) return Rules(provider=provider, original_id=original_id, rules=self.local_model, labelset=self.labelset, sampled=True, type='local_explanation', method='local_tree', callargs=callargs) Ancestors (in MRO) text_explainability.local_explanation.LocalExplanation genbase.Readable genbase.mixin.SeedMixin text_explainability.local_explanation.WeightedExplanation Instance variables 1 seed Methods augment_sample 1 2 3 4 5 6 7 8 9 10 11 12 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False , ** kwargs ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed explain 1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs) reset_seed 1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self set_seed 1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed() weigh_samples 1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric)) WeightedExplanation 1 2 3 4 class WeightedExplanation ( kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class WeightedExplanation: def __init__(self, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25): \"\"\"Add weights to neighborhood data. Args: kernel (Optional[Callable], optional): Kernel to determine similarity of perturbed instances to original instance (if set to None defaults to `data.weights.exponential_kernel`). Defaults to None. kernel_width (Union[int, float], optional): Hyperparameter for similarity function of kernel. Defaults to 25. \"\"\" if kernel is None: kernel = exponential_kernel self.kernel_fn = lambda d: kernel(d, kernel_width) def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric)) Descendants text_explainability.local_explanation.LIME text_explainability.local_explanation.LocalTree text_explainability.local_explanation.FoilTree text_explainability.local_explanation.LocalRules Methods weigh_samples 1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric))","title":"Local Explanation"},{"location":"reference/text_explainability/local_explanation/#module-text_explainabilitylocal_explanation","text":"Local explanations explain why a model made a prediction for a single instance. Todo: 1 2 * Implement Anchors * Various bugfixes View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 \"\"\"Local explanations explain why a model made a prediction for a single instance. Todo: * Implement Anchors * Various bugfixes \"\"\" import math import sys from typing import Callable, Optional, Sequence, Tuple, Union import numpy as np import six from genbase import Readable, SeedMixin, add_callargs from instancelib import (AbstractEnvironment, InstanceProvider, LabelProvider, MemoryLabelProvider, TextEnvironment) from instancelib.instances.text import TextInstance, TextInstanceProvider from instancelib.machinelearning import AbstractClassifier from sklearn.linear_model import Ridge from sklearn.tree import DecisionTreeClassifier from .data.augmentation import LeaveOut, LocalTokenPertubator from .data.weights import exponential_kernel, pairwise_distances from .decorators import text_instance from .generation.feature_selection import FeatureSelector from .generation.return_types import FeatureAttribution, Rules from .generation.surrogate import LinearSurrogate, RuleSurrogate, TreeSurrogate from .generation.target_encoding import FactFoilEncoder from .utils import binarize, default_detokenizer sys.modules['sklearn.externals.six'] = six # ensure backward compatibility from skrules import SkopeRules # noqa: E402 def default_env(env: Optional[AbstractEnvironment] = None) -> AbstractEnvironment: \"\"\"If no environment is supplied, an empty Enviroment is created for text data. Args: env (Optional[AbstractEnvironment], optional): If a environment is supplied, it is used, otherwise. Returns: AbstractEnvironment: The default/supplied environment. \"\"\" if env is not None: return env empty_dataset = TextInstanceProvider([]) empty_labels = MemoryLabelProvider([], {}) empty_env = TextEnvironment(empty_dataset, empty_labels) return empty_env class LocalExplanation(Readable, SeedMixin): def __init__(self, env: Optional[AbstractEnvironment] = None, augmenter: Optional[LocalTokenPertubator] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, seed: int = 0): \"\"\"Generate explanation for a single decision. Args: env (Optional[AbstractEnvironment], optional): Environment to save local perturbations in. Defaults to None. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__() self.env = default_env(env) if augmenter is None: augmenter = LeaveOut(env=self.env, detokenizer=default_detokenizer) if isinstance(labelset, LabelProvider) and hasattr(labelset, 'labelset'): labelset = list(labelset.labelset) elif labelset is None and self.env is not None: if hasattr(self.env.labels, 'labelset'): labelset = list(self.env.labels.labelset) self.labelset = labelset self.augmenter = augmenter self._seed = self._original_seed = seed @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed def explain(self, *args, **kwargs): return self(*args, **kwargs) class WeightedExplanation: def __init__(self, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25): \"\"\"Add weights to neighborhood data. Args: kernel (Optional[Callable], optional): Kernel to determine similarity of perturbed instances to original instance (if set to None defaults to `data.weights.exponential_kernel`). Defaults to None. kernel_width (Union[int, float], optional): Hyperparameter for similarity function of kernel. Defaults to 25. \"\"\" if kernel is None: kernel = exponential_kernel self.kernel_fn = lambda d: kernel(d, kernel_width) def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric)) class LIME(LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, local_model: Optional[LinearSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, augmenter: Optional[LocalTokenPertubator] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, seed: int = 0): \"\"\"Local Interpretable Model-Agnostic Explanations (`LIME`_). Implementation of local linear surrogate model on (weighted) perturbed text data, to get feature attribution scores for an example instance. Args: env (Optional[AbstractEnvironment]): Environment to save local perturbations in. Defaults to None. local_model (Optional[LinearSurrogate], optional): Local linear model. Defaults to None. kernel (Optional[Callable], optional): Kernel to determine similarity of perturbed instances to original instance. Defaults to None. kernel_width (Union[int, float], optional): Hyperparameter for similarity function of kernel. Defaults to 25. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _LIME: https://github.com/marcotcr/lime \"\"\" LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = LinearSurrogate(Ridge(alpha=1, fit_intercept=True, random_state=self.seed)) self.local_model = local_model @add_callargs @text_instance(tokenize=True) def __call__(self, sample: TextInstance, model: AbstractClassifier, labels: Optional[Union[Sequence[int], Sequence[str]]] = None, n_samples: int = 50, n_features: int = 10, feature_selection_method: str = 'auto', weigh_samples: bool = True, distance_metric: str = 'cosine', **kwargs) -> FeatureAttribution: \"\"\"Calculate feature attribution scores using `LIME Text`_. Args: sample (TextInstance): Instance to explain. model (AbstractClassifier): Model to explain. labels (Optional[Union[Sequence[int], Sequence[str]]], optional): [description]. Defaults to None. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. n_features (int, optional): Maximum number of features to include (explanation length). Defaults to 10. feature_selection_method (str, optional): Method for limiting number of features, either `forward_selection`, `highest_weights` or `auto`. Defaults to 'auto'. weigh_samples (bool, optional): Whether to locally weigh samples based on their similarity to the original instance. Defaults to True. distance_metric (str, optional): Distance metric for local weighting. Defaults to 'cosine'. Raises: ValueError: Can only provide labels from labelset if self.labelset is not None Returns: FeatureAttribution: [description] .. _LIME Text: https://github.com/marcotcr/lime/blob/master/lime/lime_text.py \"\"\" if labels is not None: if isinstance(labels, (int, str)): labels = [labels] n_labels = sum(1 for _ in iter(labels)) if n_labels > 0 and isinstance(next(iter(labels)), str): if self.labelset is None: raise ValueError('Can only provide label names when such a list exists in self.labelset') labels = [self.labelset.index(label) for label in labels] # Generate neighborhood samples provider, original_id, perturbed, y = self.augment_sample(sample, model, sequential=False, contiguous=False, n_samples=n_samples) perturbed = binarize(perturbed) # flatten all n replacements into one if weigh_samples: weights = self.weigh_samples(perturbed, metric=distance_metric) if feature_selection_method == 'auto': feature_selection_method = 'forward_selection' if n_features <= 6 else 'highest_weights' feature_importances, used_features = [], {} if labels is None: labels = np.arange(y.shape[1]) for label in labels: # Look at output for label y_label = y[:, label].copy() # Get the most important features features = FeatureSelector(self.local_model)(perturbed, y_label, weights=weights, n_features=n_features, method=feature_selection_method) # Fit explanation model self.local_model.alpha_reset() self.local_model.fit(perturbed[:, features], y_label, weights=weights) feature_importances.append(self.local_model.feature_importances) used_features[label] = features return FeatureAttribution(provider=provider, original_id=original_id, scores=feature_importances, used_features=used_features, labels=labels, labelset=self.labelset, type='local_explanation', method='lime', callargs=kwargs.pop('__callargs__', None)) class KernelSHAP(LocalExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: LocalTokenPertubator = None, seed: int = 0): \"\"\"Calculates `Shapley values`_ for an instance to explain, assuming the model is a black-box. Calculates Shapley values (local, additive feature attribution scores) for an instance to explain, by calculating the average contribution of changing combinations of feature values. Args: env (Optional[AbstractEnvironment], optional): Environment to save local perturbations in. Defaults to None. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _Shapley values: https://github.com/slundberg/shap \"\"\" super().__init__(env=env, augmenter=augmenter, labelset=labelset, seed=seed) @staticmethod def select_features(X: np.ndarray, y: np.ndarray, default_features: int = 1, l1_reg: Union[int, float, str] = 'auto') -> np.ndarray: \"\"\"Select features for data X and corresponding output y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. default_features (int, optional): Default number of features, when returning all features. Defaults to 1. l1_reg (Union[int, float, str], optional): Method for regularization, either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Raises: Exception: Unknown value for `l1_reg` Returns: np.ndarray: Feature indices to include. \"\"\" feature_selector = FeatureSelector() nonzero = np.arange(default_features) if isinstance(l1_reg, str) and l1_reg.startswith('n_features('): l1_reg = int(l1_reg[len('n_features('):-1]) if isinstance(l1_reg, int): nonzero = feature_selector(X, y, n_features=l1_reg, method='l1_reg') elif isinstance(l1_reg, float): nonzero = feature_selector(X, y, method='l1_reg', alpha=l1_reg) elif l1_reg in ['auto', 'aic', 'bic']: if l1_reg == 'auto': l1_reg = 'aic' nonzero = feature_selector(X, y, method=l1_reg) else: raise Exception(f'Unknown value \"{l1_reg}\" for l1_reg') return nonzero @add_callargs @text_instance(tokenize=True) def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: Optional[int] = None, l1_reg: Union[int, float, str] = 'auto', **kwargs) -> FeatureAttribution: \"\"\"Calculate feature attribution scores using `KernelShap`_. Args: sample (TextInstance): Instance to explain. model (AbstractClassifier): Model to explain. n_samples (Optional[int], optional): Number of neighborhood samples to generate (if None defaults to `2 * sample_len + 2 ** 11`). Defaults to None. l1_reg (Union[int, float, str], optional): Method for regularization (limiting number of features), either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Returns: FeatureAttribution: [description] .. _KernelShap: https://github.com/slundberg/shap/blob/master/shap/explainers/_kernel.py \"\"\" sample_len = len(sample.tokenized) if n_samples is None: n_samples = 2 * sample_len + 2 ** 11 n_samples = min(n_samples, 2 ** 30) provider, original_id, perturbed, y = self.augment_sample(sample, model, sequential=True, contiguous=False, n_samples=n_samples, add_background_instance=True) # TODO: exclude non-varying feature groups y_null, y = y[-1], y[1:-1] y -= y_null used_features = np.arange(perturbed.shape[1]) phi = np.zeros([sample_len, y.shape[1]]) phi_var = np.zeros(sample_len) if perturbed.shape[1] == 1: phi = np.mean(y - y_null, axis=0).reshape(1, -1) elif perturbed.shape[1] > 1: # Weigh samples M = perturbed.shape[1] Z = np.sum(perturbed[1:-1], axis=1).astype(int) weight_vector = np.array([(M - 1) / (math.comb(M, m) * m * (M - m)) for m in range(1, M)]) weight_vector = np.append(weight_vector, [np.mean(weight_vector)]) # TODO: replace hotfix weight_vector /= np.sum(weight_vector) kernel_weights = weight_vector[Z - 1] nonzero = KernelSHAP.select_features(perturbed[1:-1], y, default_features=sample_len, l1_reg=l1_reg) used_features = nonzero phi_var = np.ones(sample_len) if len(used_features) > 0: X = perturbed[1:-1] X_W = np.dot(X.T, np.diag(kernel_weights)) try: tmp2 = np.linalg.inv(np.dot(X_W, X)) except np.linalg.LinAlgError: tmp2 = np.linalg.pinv(np.dot(X_W, X)) phi = np.dot(tmp2, np.dot(X_W, y)).T return FeatureAttribution(provider=provider, original_id=original_id, scores=phi, scores_stddev=phi_var, base_score=y_null, used_features=used_features, labels=np.arange(y.shape[1]), labelset=self.labelset, type='local_explanation', method='kernel_shap', callargs=kwargs.pop('__callargs__', None)) class Anchor(LocalExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, seed: int = 0): super().__init__(env=env, augmenter=augmenter, labelset=labelset, seed=seed) @staticmethod def kl_bernoulli(p, q): p = float(min(1 - 1e-15, max(1e-15, p))) q = float(min(1 - 1e-15, max(1e-15, q))) return (p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))) @staticmethod def dlow_bernoulli(p, level): lm = max(min(1, p - np.sqrt(level / 2.0)), 0.0) qm = (p + lm) / 2.0 if Anchor.kl_bernoulli(p, qm) > level: lm = qm return lm def generate_candidates(self,): pass def best_candidate(self): pass @staticmethod def beam_search(provider, perturbed: np.ndarray, model, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None, batch_size: int = 20): # TODO: add value checking to decorator if beam_size < 1: raise ValueError(f'{beam_size=} should be at least 1.') if not (0.0 <= min_confidence <= 1.0): raise ValueError(f'{min_confidence=} should be in range [0, 1].') if not (0.0 <= delta <= 1.0): raise ValueError(f'{delta=} should be in range [0, 1].') if not (0.0 <= epsilon <= 1.0): raise ValueError(f'{epsilon=} should be in range [0, 1].') if batch_size < 2: raise ValueError(f'{batch_size=} should be at least 2.') for batch in provider.instance_chunker(batch_size): y = list(model.predict_proba_raw(batch))[-1][-1] # todo: only look at probs of one class y_true, y = y[0], y[1:] # noqa: F841 beta = np.log(1.0 / delta) mean = y.mean() lb = Anchor.dlow_bernoulli(mean, beta / perturbed.shape[0]) if not(mean > min_confidence and lb < min_confidence - epsilon): break raise NotImplementedError('[WIP] Implementing anchor/anchor_base.py') @add_callargs @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: int = 100, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None, **kwargs): raise NotImplementedError('Only partially implemented') # https://github.com/marcotcr/anchor/blob/master/anchor/anchor_text.py # https://github.com/marcotcr/anchor/blob/master/anchor/anchor_base.py provider, original_id, perturbed = self.augment_sample(sample, None, sequential=False, contiguous=False, n_samples=n_samples, predict=False) perturbed = binarize(perturbed[1:]) # flatten all n replacements into one y_true = np.argmax(model.predict_proba([provider[0]])[0][-1]) # noqa: F841 # Use beam from https://homes.cs.washington.edu/~marcotcr/aaai18.pdf (Algorithm 2) anchor = Anchor.beam_search(provider, # noqa: F841 perturbed, model, beam_size=beam_size, min_confidence=min_confidence, delta=delta, epsilon=epsilon, max_anchor_size=max_anchor_size, batch_size=n_samples // 10 if n_samples >= 1000 else n_samples // 5) pass class LocalTree(LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[TreeSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = TreeSurrogate(DecisionTreeClassifier(random_state=self.seed)) self.local_model = local_model self.explanation_type = explanation_type @add_callargs @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', max_rule_size: Optional[int] = None, **sample_kwargs): callargs = sample_kwargs.pop('__callargs__', None) provider, original_id, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.max_rule_size = max_rule_size self.local_model.fit(perturbed, y, weights=weights) return Rules(provider=provider, original_id=original_id, rules=self.local_model, labelset=self.labelset, sampled=True, type='local_explanation', method='local_tree', callargs=callargs) class FactFoilMixin: def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y) class FoilTree(FactFoilMixin, LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[TreeSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = TreeSurrogate(DecisionTreeClassifier(random_state=self.seed)) self.local_model = local_model self.explanation_type = explanation_type @add_callargs @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, foil_fn: Union[FactFoilEncoder, int, str], n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', max_rule_size: Optional[int] = None, **sample_kwargs): callargs = sample_kwargs.pop('__callargs__', None) provider, original_id, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one # Encode foil as 0 and rest as 1 labelset = self.labelset if self.labelset else model y_ = self.to_fact_foil(y, labelset, foil_fn) weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.max_rule_size = max_rule_size self.local_model.fit(perturbed, y_, weights=weights) # TODO: pass to which label the Foil Tree applies return Rules(provider=provider, original_id=original_id, rules=self.local_model, labelset=labelset, sampled=True, type='local_explanation', method='foil_tree', callargs=callargs) class LocalRules(FactFoilMixin, LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[RuleSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = RuleSurrogate(SkopeRules(max_depth_duplication=2, n_estimators=30, random_state=self.seed)) self.local_model = local_model self.explanation_type = explanation_type @add_callargs @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, foil_fn: Union[FactFoilEncoder, int, str], n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', **sample_kwargs): callargs = sample_kwargs.pop('__callargs__', None) provider, original_id, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one # Encode foil as 0 and rest as 1 labelset = self.labelset if self.labelset else model y_ = self.to_fact_foil(y, labelset, foil_fn) weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.fit(perturbed, y_, weights=weights) return Rules(provider, original_id=original_id, rules=self.local_model, labelset=labelset, sampled=True, type='local_explanation', method='local_rules', callargs=callargs)","title":"Module text_explainability.local_explanation"},{"location":"reference/text_explainability/local_explanation/#functions","text":"","title":"Functions"},{"location":"reference/text_explainability/local_explanation/#default_env","text":"1 2 3 def default_env ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None ) -> instancelib . environment . base . AbstractEnvironment If no environment is supplied, an empty Enviroment is created for text data. Parameters: Name Type Description Default env Optional[AbstractEnvironment] If a environment is supplied, it is used, otherwise. None Returns: Type Description AbstractEnvironment The default/supplied environment. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def default_env(env: Optional[AbstractEnvironment] = None) -> AbstractEnvironment: \"\"\"If no environment is supplied, an empty Enviroment is created for text data. Args: env (Optional[AbstractEnvironment], optional): If a environment is supplied, it is used, otherwise. Returns: AbstractEnvironment: The default/supplied environment. \"\"\" if env is not None: return env empty_dataset = TextInstanceProvider([]) empty_labels = MemoryLabelProvider([], {}) empty_env = TextEnvironment(empty_dataset, empty_labels) return empty_env","title":"default_env"},{"location":"reference/text_explainability/local_explanation/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/local_explanation/#anchor","text":"1 2 3 4 5 6 class Anchor ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 class Anchor(LocalExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, seed: int = 0): super().__init__(env=env, augmenter=augmenter, labelset=labelset, seed=seed) @staticmethod def kl_bernoulli(p, q): p = float(min(1 - 1e-15, max(1e-15, p))) q = float(min(1 - 1e-15, max(1e-15, q))) return (p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))) @staticmethod def dlow_bernoulli(p, level): lm = max(min(1, p - np.sqrt(level / 2.0)), 0.0) qm = (p + lm) / 2.0 if Anchor.kl_bernoulli(p, qm) > level: lm = qm return lm def generate_candidates(self,): pass def best_candidate(self): pass @staticmethod def beam_search(provider, perturbed: np.ndarray, model, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None, batch_size: int = 20): # TODO: add value checking to decorator if beam_size < 1: raise ValueError(f'{beam_size=} should be at least 1.') if not (0.0 <= min_confidence <= 1.0): raise ValueError(f'{min_confidence=} should be in range [0, 1].') if not (0.0 <= delta <= 1.0): raise ValueError(f'{delta=} should be in range [0, 1].') if not (0.0 <= epsilon <= 1.0): raise ValueError(f'{epsilon=} should be in range [0, 1].') if batch_size < 2: raise ValueError(f'{batch_size=} should be at least 2.') for batch in provider.instance_chunker(batch_size): y = list(model.predict_proba_raw(batch))[-1][-1] # todo: only look at probs of one class y_true, y = y[0], y[1:] # noqa: F841 beta = np.log(1.0 / delta) mean = y.mean() lb = Anchor.dlow_bernoulli(mean, beta / perturbed.shape[0]) if not(mean > min_confidence and lb < min_confidence - epsilon): break raise NotImplementedError('[WIP] Implementing anchor/anchor_base.py') @add_callargs @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: int = 100, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None, **kwargs): raise NotImplementedError('Only partially implemented') # https://github.com/marcotcr/anchor/blob/master/anchor/anchor_text.py # https://github.com/marcotcr/anchor/blob/master/anchor/anchor_base.py provider, original_id, perturbed = self.augment_sample(sample, None, sequential=False, contiguous=False, n_samples=n_samples, predict=False) perturbed = binarize(perturbed[1:]) # flatten all n replacements into one y_true = np.argmax(model.predict_proba([provider[0]])[0][-1]) # noqa: F841 # Use beam from https://homes.cs.washington.edu/~marcotcr/aaai18.pdf (Algorithm 2) anchor = Anchor.beam_search(provider, # noqa: F841 perturbed, model, beam_size=beam_size, min_confidence=min_confidence, delta=delta, epsilon=epsilon, max_anchor_size=max_anchor_size, batch_size=n_samples // 10 if n_samples >= 1000 else n_samples // 5) pass","title":"Anchor"},{"location":"reference/text_explainability/local_explanation/#ancestors-in-mro","text":"text_explainability.local_explanation.LocalExplanation genbase.Readable genbase.mixin.SeedMixin","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/local_explanation/#static-methods","text":"","title":"Static methods"},{"location":"reference/text_explainability/local_explanation/#beam_search","text":"1 2 3 4 5 6 7 8 9 10 11 def beam_search ( provider , perturbed : numpy . ndarray , model , beam_size : int = 1 , min_confidence : float = 0.95 , delta : float = 0.05 , epsilon : float = 0.1 , max_anchor_size : Optional [ int ] = None , batch_size : int = 20 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @staticmethod def beam_search(provider, perturbed: np.ndarray, model, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None, batch_size: int = 20): # TODO: add value checking to decorator if beam_size < 1: raise ValueError(f'{beam_size=} should be at least 1.') if not (0.0 <= min_confidence <= 1.0): raise ValueError(f'{min_confidence=} should be in range [0, 1].') if not (0.0 <= delta <= 1.0): raise ValueError(f'{delta=} should be in range [0, 1].') if not (0.0 <= epsilon <= 1.0): raise ValueError(f'{epsilon=} should be in range [0, 1].') if batch_size < 2: raise ValueError(f'{batch_size=} should be at least 2.') for batch in provider.instance_chunker(batch_size): y = list(model.predict_proba_raw(batch))[-1][-1] # todo: only look at probs of one class y_true, y = y[0], y[1:] # noqa: F841 beta = np.log(1.0 / delta) mean = y.mean() lb = Anchor.dlow_bernoulli(mean, beta / perturbed.shape[0]) if not(mean > min_confidence and lb < min_confidence - epsilon): break raise NotImplementedError('[WIP] Implementing anchor/anchor_base.py')","title":"beam_search"},{"location":"reference/text_explainability/local_explanation/#dlow_bernoulli","text":"1 2 3 4 def dlow_bernoulli ( p , level ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 @staticmethod def dlow_bernoulli(p, level): lm = max(min(1, p - np.sqrt(level / 2.0)), 0.0) qm = (p + lm) / 2.0 if Anchor.kl_bernoulli(p, qm) > level: lm = qm return lm","title":"dlow_bernoulli"},{"location":"reference/text_explainability/local_explanation/#kl_bernoulli","text":"1 2 3 4 def kl_bernoulli ( p , q ) View Source 1 2 3 4 5 6 7 8 9 10 11 @staticmethod def kl_bernoulli(p, q): p = float(min(1 - 1e-15, max(1e-15, p))) q = float(min(1 - 1e-15, max(1e-15, q))) return (p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q)))","title":"kl_bernoulli"},{"location":"reference/text_explainability/local_explanation/#instance-variables","text":"1 seed","title":"Instance variables"},{"location":"reference/text_explainability/local_explanation/#methods","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#augment_sample","text":"1 2 3 4 5 6 7 8 9 10 11 12 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False , ** kwargs ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed","title":"augment_sample"},{"location":"reference/text_explainability/local_explanation/#best_candidate","text":"1 2 3 def best_candidate ( self ) View Source 1 2 3 def best_candidate(self): pass","title":"best_candidate"},{"location":"reference/text_explainability/local_explanation/#explain","text":"1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs)","title":"explain"},{"location":"reference/text_explainability/local_explanation/#generate_candidates","text":"1 2 3 def generate_candidates ( self ) View Source 1 2 3 def generate_candidates(self,): pass","title":"generate_candidates"},{"location":"reference/text_explainability/local_explanation/#reset_seed","text":"1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self","title":"reset_seed"},{"location":"reference/text_explainability/local_explanation/#set_seed","text":"1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed()","title":"set_seed"},{"location":"reference/text_explainability/local_explanation/#factfoilmixin","text":"1 2 3 4 5 class FactFoilMixin ( / , * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 class FactFoilMixin: def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y)","title":"FactFoilMixin"},{"location":"reference/text_explainability/local_explanation/#descendants","text":"text_explainability.local_explanation.FoilTree text_explainability.local_explanation.LocalRules","title":"Descendants"},{"location":"reference/text_explainability/local_explanation/#methods_1","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#to_fact_foil","text":"1 2 3 4 5 6 def to_fact_foil ( self , y , labelset , foil_fn : Union [ text_explainability . generation . target_encoding . FactFoilEncoder , int , str ] ) View Source 1 2 3 4 5 6 7 8 9 10 11 def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y)","title":"to_fact_foil"},{"location":"reference/text_explainability/local_explanation/#foiltree","text":"1 2 3 4 5 6 7 8 9 10 class FoilTree ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , local_model : Optional [ text_explainability . generation . surrogate . TreeSurrogate ] = None , kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 , explanation_type : str = 'multiclass' , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 class FoilTree(FactFoilMixin, LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[TreeSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = TreeSurrogate(DecisionTreeClassifier(random_state=self.seed)) self.local_model = local_model self.explanation_type = explanation_type @add_callargs @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, foil_fn: Union[FactFoilEncoder, int, str], n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', max_rule_size: Optional[int] = None, **sample_kwargs): callargs = sample_kwargs.pop('__callargs__', None) provider, original_id, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one # Encode foil as 0 and rest as 1 labelset = self.labelset if self.labelset else model y_ = self.to_fact_foil(y, labelset, foil_fn) weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.max_rule_size = max_rule_size self.local_model.fit(perturbed, y_, weights=weights) # TODO: pass to which label the Foil Tree applies return Rules(provider=provider, original_id=original_id, rules=self.local_model, labelset=labelset, sampled=True, type='local_explanation', method='foil_tree', callargs=callargs)","title":"FoilTree"},{"location":"reference/text_explainability/local_explanation/#ancestors-in-mro_1","text":"text_explainability.local_explanation.FactFoilMixin text_explainability.local_explanation.LocalExplanation genbase.Readable genbase.mixin.SeedMixin text_explainability.local_explanation.WeightedExplanation","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/local_explanation/#instance-variables_1","text":"1 seed","title":"Instance variables"},{"location":"reference/text_explainability/local_explanation/#methods_2","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#augment_sample_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False , ** kwargs ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed","title":"augment_sample"},{"location":"reference/text_explainability/local_explanation/#explain_1","text":"1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs)","title":"explain"},{"location":"reference/text_explainability/local_explanation/#reset_seed_1","text":"1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self","title":"reset_seed"},{"location":"reference/text_explainability/local_explanation/#set_seed_1","text":"1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed()","title":"set_seed"},{"location":"reference/text_explainability/local_explanation/#to_fact_foil_1","text":"1 2 3 4 5 6 def to_fact_foil ( self , y , labelset , foil_fn : Union [ text_explainability . generation . target_encoding . FactFoilEncoder , int , str ] ) View Source 1 2 3 4 5 6 7 8 9 10 11 def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y)","title":"to_fact_foil"},{"location":"reference/text_explainability/local_explanation/#weigh_samples","text":"1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric))","title":"weigh_samples"},{"location":"reference/text_explainability/local_explanation/#kernelshap","text":"1 2 3 4 5 6 class KernelSHAP ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : text_explainability . data . augmentation . LocalTokenPertubator = None , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 class KernelSHAP(LocalExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: LocalTokenPertubator = None, seed: int = 0): \"\"\"Calculates `Shapley values`_ for an instance to explain, assuming the model is a black-box. Calculates Shapley values (local, additive feature attribution scores) for an instance to explain, by calculating the average contribution of changing combinations of feature values. Args: env (Optional[AbstractEnvironment], optional): Environment to save local perturbations in. Defaults to None. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _Shapley values: https://github.com/slundberg/shap \"\"\" super().__init__(env=env, augmenter=augmenter, labelset=labelset, seed=seed) @staticmethod def select_features(X: np.ndarray, y: np.ndarray, default_features: int = 1, l1_reg: Union[int, float, str] = 'auto') -> np.ndarray: \"\"\"Select features for data X and corresponding output y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. default_features (int, optional): Default number of features, when returning all features. Defaults to 1. l1_reg (Union[int, float, str], optional): Method for regularization, either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Raises: Exception: Unknown value for `l1_reg` Returns: np.ndarray: Feature indices to include. \"\"\" feature_selector = FeatureSelector() nonzero = np.arange(default_features) if isinstance(l1_reg, str) and l1_reg.startswith('n_features('): l1_reg = int(l1_reg[len('n_features('):-1]) if isinstance(l1_reg, int): nonzero = feature_selector(X, y, n_features=l1_reg, method='l1_reg') elif isinstance(l1_reg, float): nonzero = feature_selector(X, y, method='l1_reg', alpha=l1_reg) elif l1_reg in ['auto', 'aic', 'bic']: if l1_reg == 'auto': l1_reg = 'aic' nonzero = feature_selector(X, y, method=l1_reg) else: raise Exception(f'Unknown value \"{l1_reg}\" for l1_reg') return nonzero @add_callargs @text_instance(tokenize=True) def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: Optional[int] = None, l1_reg: Union[int, float, str] = 'auto', **kwargs) -> FeatureAttribution: \"\"\"Calculate feature attribution scores using `KernelShap`_. Args: sample (TextInstance): Instance to explain. model (AbstractClassifier): Model to explain. n_samples (Optional[int], optional): Number of neighborhood samples to generate (if None defaults to `2 * sample_len + 2 ** 11`). Defaults to None. l1_reg (Union[int, float, str], optional): Method for regularization (limiting number of features), either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Returns: FeatureAttribution: [description] .. _KernelShap: https://github.com/slundberg/shap/blob/master/shap/explainers/_kernel.py \"\"\" sample_len = len(sample.tokenized) if n_samples is None: n_samples = 2 * sample_len + 2 ** 11 n_samples = min(n_samples, 2 ** 30) provider, original_id, perturbed, y = self.augment_sample(sample, model, sequential=True, contiguous=False, n_samples=n_samples, add_background_instance=True) # TODO: exclude non-varying feature groups y_null, y = y[-1], y[1:-1] y -= y_null used_features = np.arange(perturbed.shape[1]) phi = np.zeros([sample_len, y.shape[1]]) phi_var = np.zeros(sample_len) if perturbed.shape[1] == 1: phi = np.mean(y - y_null, axis=0).reshape(1, -1) elif perturbed.shape[1] > 1: # Weigh samples M = perturbed.shape[1] Z = np.sum(perturbed[1:-1], axis=1).astype(int) weight_vector = np.array([(M - 1) / (math.comb(M, m) * m * (M - m)) for m in range(1, M)]) weight_vector = np.append(weight_vector, [np.mean(weight_vector)]) # TODO: replace hotfix weight_vector /= np.sum(weight_vector) kernel_weights = weight_vector[Z - 1] nonzero = KernelSHAP.select_features(perturbed[1:-1], y, default_features=sample_len, l1_reg=l1_reg) used_features = nonzero phi_var = np.ones(sample_len) if len(used_features) > 0: X = perturbed[1:-1] X_W = np.dot(X.T, np.diag(kernel_weights)) try: tmp2 = np.linalg.inv(np.dot(X_W, X)) except np.linalg.LinAlgError: tmp2 = np.linalg.pinv(np.dot(X_W, X)) phi = np.dot(tmp2, np.dot(X_W, y)).T return FeatureAttribution(provider=provider, original_id=original_id, scores=phi, scores_stddev=phi_var, base_score=y_null, used_features=used_features, labels=np.arange(y.shape[1]), labelset=self.labelset, type='local_explanation', method='kernel_shap', callargs=kwargs.pop('__callargs__', None))","title":"KernelSHAP"},{"location":"reference/text_explainability/local_explanation/#ancestors-in-mro_2","text":"text_explainability.local_explanation.LocalExplanation genbase.Readable genbase.mixin.SeedMixin","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/local_explanation/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/text_explainability/local_explanation/#select_features","text":"1 2 3 4 5 6 def select_features ( X : numpy . ndarray , y : numpy . ndarray , default_features : int = 1 , l1_reg : Union [ int , float , str ] = 'auto' ) -> numpy . ndarray Select features for data X and corresponding output y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. default_features (int, optional): Default number of features, when returning all features. Defaults to 1. l1_reg (Union[int, float, str], optional): Method for regularization, either auto , n_features({int}) , {int} , {float} , aic or bic . Defaults to 'auto'. Raises: Exception: Unknown value for l1_reg Returns: np.ndarray: Feature indices to include. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @staticmethod def select_features(X: np.ndarray, y: np.ndarray, default_features: int = 1, l1_reg: Union[int, float, str] = 'auto') -> np.ndarray: \"\"\"Select features for data X and corresponding output y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. default_features (int, optional): Default number of features, when returning all features. Defaults to 1. l1_reg (Union[int, float, str], optional): Method for regularization, either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Raises: Exception: Unknown value for `l1_reg` Returns: np.ndarray: Feature indices to include. \"\"\" feature_selector = FeatureSelector() nonzero = np.arange(default_features) if isinstance(l1_reg, str) and l1_reg.startswith('n_features('): l1_reg = int(l1_reg[len('n_features('):-1]) if isinstance(l1_reg, int): nonzero = feature_selector(X, y, n_features=l1_reg, method='l1_reg') elif isinstance(l1_reg, float): nonzero = feature_selector(X, y, method='l1_reg', alpha=l1_reg) elif l1_reg in ['auto', 'aic', 'bic']: if l1_reg == 'auto': l1_reg = 'aic' nonzero = feature_selector(X, y, method=l1_reg) else: raise Exception(f'Unknown value \"{l1_reg}\" for l1_reg') return nonzero","title":"select_features"},{"location":"reference/text_explainability/local_explanation/#instance-variables_2","text":"1 seed","title":"Instance variables"},{"location":"reference/text_explainability/local_explanation/#methods_3","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#augment_sample_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False , ** kwargs ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed","title":"augment_sample"},{"location":"reference/text_explainability/local_explanation/#explain_2","text":"1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs)","title":"explain"},{"location":"reference/text_explainability/local_explanation/#reset_seed_2","text":"1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self","title":"reset_seed"},{"location":"reference/text_explainability/local_explanation/#set_seed_2","text":"1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed()","title":"set_seed"},{"location":"reference/text_explainability/local_explanation/#lime","text":"1 2 3 4 5 6 7 8 9 class LIME ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , local_model : Optional [ text_explainability . generation . surrogate . LinearSurrogate ] = None , kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 class LIME(LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, local_model: Optional[LinearSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, augmenter: Optional[LocalTokenPertubator] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, seed: int = 0): \"\"\"Local Interpretable Model-Agnostic Explanations (`LIME`_). Implementation of local linear surrogate model on (weighted) perturbed text data, to get feature attribution scores for an example instance. Args: env (Optional[AbstractEnvironment]): Environment to save local perturbations in. Defaults to None. local_model (Optional[LinearSurrogate], optional): Local linear model. Defaults to None. kernel (Optional[Callable], optional): Kernel to determine similarity of perturbed instances to original instance. Defaults to None. kernel_width (Union[int, float], optional): Hyperparameter for similarity function of kernel. Defaults to 25. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _LIME: https://github.com/marcotcr/lime \"\"\" LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = LinearSurrogate(Ridge(alpha=1, fit_intercept=True, random_state=self.seed)) self.local_model = local_model @add_callargs @text_instance(tokenize=True) def __call__(self, sample: TextInstance, model: AbstractClassifier, labels: Optional[Union[Sequence[int], Sequence[str]]] = None, n_samples: int = 50, n_features: int = 10, feature_selection_method: str = 'auto', weigh_samples: bool = True, distance_metric: str = 'cosine', **kwargs) -> FeatureAttribution: \"\"\"Calculate feature attribution scores using `LIME Text`_. Args: sample (TextInstance): Instance to explain. model (AbstractClassifier): Model to explain. labels (Optional[Union[Sequence[int], Sequence[str]]], optional): [description]. Defaults to None. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. n_features (int, optional): Maximum number of features to include (explanation length). Defaults to 10. feature_selection_method (str, optional): Method for limiting number of features, either `forward_selection`, `highest_weights` or `auto`. Defaults to 'auto'. weigh_samples (bool, optional): Whether to locally weigh samples based on their similarity to the original instance. Defaults to True. distance_metric (str, optional): Distance metric for local weighting. Defaults to 'cosine'. Raises: ValueError: Can only provide labels from labelset if self.labelset is not None Returns: FeatureAttribution: [description] .. _LIME Text: https://github.com/marcotcr/lime/blob/master/lime/lime_text.py \"\"\" if labels is not None: if isinstance(labels, (int, str)): labels = [labels] n_labels = sum(1 for _ in iter(labels)) if n_labels > 0 and isinstance(next(iter(labels)), str): if self.labelset is None: raise ValueError('Can only provide label names when such a list exists in self.labelset') labels = [self.labelset.index(label) for label in labels] # Generate neighborhood samples provider, original_id, perturbed, y = self.augment_sample(sample, model, sequential=False, contiguous=False, n_samples=n_samples) perturbed = binarize(perturbed) # flatten all n replacements into one if weigh_samples: weights = self.weigh_samples(perturbed, metric=distance_metric) if feature_selection_method == 'auto': feature_selection_method = 'forward_selection' if n_features <= 6 else 'highest_weights' feature_importances, used_features = [], {} if labels is None: labels = np.arange(y.shape[1]) for label in labels: # Look at output for label y_label = y[:, label].copy() # Get the most important features features = FeatureSelector(self.local_model)(perturbed, y_label, weights=weights, n_features=n_features, method=feature_selection_method) # Fit explanation model self.local_model.alpha_reset() self.local_model.fit(perturbed[:, features], y_label, weights=weights) feature_importances.append(self.local_model.feature_importances) used_features[label] = features return FeatureAttribution(provider=provider, original_id=original_id, scores=feature_importances, used_features=used_features, labels=labels, labelset=self.labelset, type='local_explanation', method='lime', callargs=kwargs.pop('__callargs__', None))","title":"LIME"},{"location":"reference/text_explainability/local_explanation/#ancestors-in-mro_3","text":"text_explainability.local_explanation.LocalExplanation genbase.Readable genbase.mixin.SeedMixin text_explainability.local_explanation.WeightedExplanation","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/local_explanation/#instance-variables_3","text":"1 seed","title":"Instance variables"},{"location":"reference/text_explainability/local_explanation/#methods_4","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#augment_sample_3","text":"1 2 3 4 5 6 7 8 9 10 11 12 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False , ** kwargs ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed","title":"augment_sample"},{"location":"reference/text_explainability/local_explanation/#explain_3","text":"1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs)","title":"explain"},{"location":"reference/text_explainability/local_explanation/#reset_seed_3","text":"1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self","title":"reset_seed"},{"location":"reference/text_explainability/local_explanation/#set_seed_3","text":"1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed()","title":"set_seed"},{"location":"reference/text_explainability/local_explanation/#weigh_samples_1","text":"1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric))","title":"weigh_samples"},{"location":"reference/text_explainability/local_explanation/#localexplanation","text":"1 2 3 4 5 6 class LocalExplanation ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 class LocalExplanation(Readable, SeedMixin): def __init__(self, env: Optional[AbstractEnvironment] = None, augmenter: Optional[LocalTokenPertubator] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, seed: int = 0): \"\"\"Generate explanation for a single decision. Args: env (Optional[AbstractEnvironment], optional): Environment to save local perturbations in. Defaults to None. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__() self.env = default_env(env) if augmenter is None: augmenter = LeaveOut(env=self.env, detokenizer=default_detokenizer) if isinstance(labelset, LabelProvider) and hasattr(labelset, 'labelset'): labelset = list(labelset.labelset) elif labelset is None and self.env is not None: if hasattr(self.env.labels, 'labelset'): labelset = list(self.env.labels.labelset) self.labelset = labelset self.augmenter = augmenter self._seed = self._original_seed = seed @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed def explain(self, *args, **kwargs): return self(*args, **kwargs)","title":"LocalExplanation"},{"location":"reference/text_explainability/local_explanation/#ancestors-in-mro_4","text":"genbase.Readable genbase.mixin.SeedMixin","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/local_explanation/#descendants_1","text":"text_explainability.local_explanation.LIME text_explainability.local_explanation.KernelSHAP text_explainability.local_explanation.Anchor text_explainability.local_explanation.LocalTree text_explainability.local_explanation.FoilTree text_explainability.local_explanation.LocalRules","title":"Descendants"},{"location":"reference/text_explainability/local_explanation/#instance-variables_4","text":"1 seed","title":"Instance variables"},{"location":"reference/text_explainability/local_explanation/#methods_5","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#augment_sample_4","text":"1 2 3 4 5 6 7 8 9 10 11 12 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False , ** kwargs ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed","title":"augment_sample"},{"location":"reference/text_explainability/local_explanation/#explain_4","text":"1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs)","title":"explain"},{"location":"reference/text_explainability/local_explanation/#reset_seed_4","text":"1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self","title":"reset_seed"},{"location":"reference/text_explainability/local_explanation/#set_seed_4","text":"1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed()","title":"set_seed"},{"location":"reference/text_explainability/local_explanation/#localrules","text":"1 2 3 4 5 6 7 8 9 10 class LocalRules ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , local_model : Optional [ text_explainability . generation . surrogate . RuleSurrogate ] = None , kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 , explanation_type : str = 'multiclass' , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class LocalRules(FactFoilMixin, LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[RuleSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = RuleSurrogate(SkopeRules(max_depth_duplication=2, n_estimators=30, random_state=self.seed)) self.local_model = local_model self.explanation_type = explanation_type @add_callargs @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, foil_fn: Union[FactFoilEncoder, int, str], n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', **sample_kwargs): callargs = sample_kwargs.pop('__callargs__', None) provider, original_id, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one # Encode foil as 0 and rest as 1 labelset = self.labelset if self.labelset else model y_ = self.to_fact_foil(y, labelset, foil_fn) weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.fit(perturbed, y_, weights=weights) return Rules(provider, original_id=original_id, rules=self.local_model, labelset=labelset, sampled=True, type='local_explanation', method='local_rules', callargs=callargs)","title":"LocalRules"},{"location":"reference/text_explainability/local_explanation/#ancestors-in-mro_5","text":"text_explainability.local_explanation.FactFoilMixin text_explainability.local_explanation.LocalExplanation genbase.Readable genbase.mixin.SeedMixin text_explainability.local_explanation.WeightedExplanation","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/local_explanation/#instance-variables_5","text":"1 seed","title":"Instance variables"},{"location":"reference/text_explainability/local_explanation/#methods_6","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#augment_sample_5","text":"1 2 3 4 5 6 7 8 9 10 11 12 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False , ** kwargs ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed","title":"augment_sample"},{"location":"reference/text_explainability/local_explanation/#explain_5","text":"1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs)","title":"explain"},{"location":"reference/text_explainability/local_explanation/#reset_seed_5","text":"1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self","title":"reset_seed"},{"location":"reference/text_explainability/local_explanation/#set_seed_5","text":"1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed()","title":"set_seed"},{"location":"reference/text_explainability/local_explanation/#to_fact_foil_2","text":"1 2 3 4 5 6 def to_fact_foil ( self , y , labelset , foil_fn : Union [ text_explainability . generation . target_encoding . FactFoilEncoder , int , str ] ) View Source 1 2 3 4 5 6 7 8 9 10 11 def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y)","title":"to_fact_foil"},{"location":"reference/text_explainability/local_explanation/#weigh_samples_2","text":"1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric))","title":"weigh_samples"},{"location":"reference/text_explainability/local_explanation/#localtree","text":"1 2 3 4 5 6 7 8 9 10 class LocalTree ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , local_model : Optional [ text_explainability . generation . surrogate . TreeSurrogate ] = None , kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 , explanation_type : str = 'multiclass' , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class LocalTree(LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[TreeSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = TreeSurrogate(DecisionTreeClassifier(random_state=self.seed)) self.local_model = local_model self.explanation_type = explanation_type @add_callargs @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', max_rule_size: Optional[int] = None, **sample_kwargs): callargs = sample_kwargs.pop('__callargs__', None) provider, original_id, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.max_rule_size = max_rule_size self.local_model.fit(perturbed, y, weights=weights) return Rules(provider=provider, original_id=original_id, rules=self.local_model, labelset=self.labelset, sampled=True, type='local_explanation', method='local_tree', callargs=callargs)","title":"LocalTree"},{"location":"reference/text_explainability/local_explanation/#ancestors-in-mro_6","text":"text_explainability.local_explanation.LocalExplanation genbase.Readable genbase.mixin.SeedMixin text_explainability.local_explanation.WeightedExplanation","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/local_explanation/#instance-variables_6","text":"1 seed","title":"Instance variables"},{"location":"reference/text_explainability/local_explanation/#methods_7","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#augment_sample_6","text":"1 2 3 4 5 6 7 8 9 10 11 12 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False , ** kwargs ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False, **kwargs, ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) original_id = next(iter(provider)) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for _, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, original_id, perturbed, y return provider, original_id, perturbed","title":"augment_sample"},{"location":"reference/text_explainability/local_explanation/#explain_6","text":"1 2 3 4 5 def explain ( self , * args , ** kwargs ) View Source 1 2 3 def explain(self, *args, **kwargs): return self(*args, **kwargs)","title":"explain"},{"location":"reference/text_explainability/local_explanation/#reset_seed_6","text":"1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self","title":"reset_seed"},{"location":"reference/text_explainability/local_explanation/#set_seed_6","text":"1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed()","title":"set_seed"},{"location":"reference/text_explainability/local_explanation/#weigh_samples_3","text":"1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric))","title":"weigh_samples"},{"location":"reference/text_explainability/local_explanation/#weightedexplanation","text":"1 2 3 4 class WeightedExplanation ( kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class WeightedExplanation: def __init__(self, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25): \"\"\"Add weights to neighborhood data. Args: kernel (Optional[Callable], optional): Kernel to determine similarity of perturbed instances to original instance (if set to None defaults to `data.weights.exponential_kernel`). Defaults to None. kernel_width (Union[int, float], optional): Hyperparameter for similarity function of kernel. Defaults to 25. \"\"\" if kernel is None: kernel = exponential_kernel self.kernel_fn = lambda d: kernel(d, kernel_width) def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric))","title":"WeightedExplanation"},{"location":"reference/text_explainability/local_explanation/#descendants_2","text":"text_explainability.local_explanation.LIME text_explainability.local_explanation.LocalTree text_explainability.local_explanation.FoilTree text_explainability.local_explanation.LocalRules","title":"Descendants"},{"location":"reference/text_explainability/local_explanation/#methods_8","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#weigh_samples_4","text":"1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric))","title":"weigh_samples"},{"location":"reference/text_explainability/model/","text":"Module text_explainability.model None None View Source 1 2 3 from genbase.model import import_model __all__ = ['import_model'] Functions import_model 1 2 3 4 5 6 def import_model ( model , environment : Optional [ instancelib . environment . base . Environment ] = None , train : Union [ int , float , str , instancelib . instances . base . InstanceProvider ] = 'train' , label_map : Optional [ Dict [ ~ LT , ~ LT ]] = None ) -> instancelib . machinelearning . base . AbstractClassifier Import a model from file or from a Python object. Parameters: Name Type Description Default model None Model or path to model to import. None environment Optional[Environment] Environment corresponding to model (with dataset and ground-truth labels), used for importing models and/or training them. None train Union[int, float, str, InstanceProvider] Train split size, name in environment or provider. Defaults to 'train'. None label_map Optional[Dict[LT, LT]] Conversion of label IDs to named labels. Defaults to None. None Returns: Type Description AbstractClassifier Instancelib wrapped model. Raises: Type Description ImportError Unable to import model or file. NotImplementedError Type of model is not yet supported. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def import_model(model, environment: Optional[Environment] = None, train: Union[int, float, str, InstanceProvider] = 'train', label_map: Optional[Dict[LT, LT]] = None) -> AbstractClassifier: \"\"\"Import a model from file or from a Python object. Examples: Make a scikit-learn text classifier and train it on SST2 >>> from genbase import import_data, import_model >>> from datasets import load_dataset >>> ds = import_data(load_dataset('glue', 'sst2'), data_cols='sentence', label_cols='label') >>> from sklearn.pipeline import Pipeline >>> from sklearn.naive_bayes import MultinomialNB >>> from sklearn.feature_extraction.text import TfidfVectorizer >>> pipeline = Pipeline([('tfidf', TfidfVectorizer()), ... ('clf', MultinomialNB())]) >>> import_model(pipeline, ds, train='train') Load a pretrained ONNX model downloaded from https://github.com/mpbron/instancelib-onnx/blob/main/example_models/data-model.onnx >>> from genbase import import_model >>> import_model('data-model.onnx', label_map={0: 'Bedrijfsnieuws', 1: 'Games', 2: 'Smartphones'}) Args: model: Model or path to model to import. environment (Optional[Environment], optional): Environment corresponding to model (with dataset and ground-truth labels), used for importing models and/or training them. train (Union[int, float, str, InstanceProvider], optional): Train split size, name in environment or provider. Defaults to 'train'. label_map (Optional[Dict[LT, LT]], optional): Conversion of label IDs to named labels. Defaults to None. Raises: ImportError: Unable to import model or file. NotImplementedError: Type of model is not yet supported. Returns: AbstractClassifier: Instancelib wrapped model. \"\"\" if label_map is None and environment is not None: label_map = list(environment.labels.labelset) if isinstance(label_map, dict): label_map = {str(k): v for k, v in label_map.items()} if isinstance(model, str): if not Path(model).exists(): raise ImportError(f'Unable to locate file \"{model}\"') file_type = get_file_type(model) if file_type == '.pkl': import pickle # nosec info('Unpickling model (warning: be sure you trust a source before unpickling a model!)') model = pickle.load(model) # nosec elif file_type == '.onnx': if not package_available('ilonnx'): raise ImportError('To import ONNX files install `instancelib-onnx`!') import ilonnx if label_map is None: info('Improve the informativeness of your predictions by providing the label_map') return ilonnx.build_data_model(model, classes=label_map) elif isinstance(model, AbstractClassifier): return model _no_train_msg = '' if environment is not None: if isinstance(train, (float, int)): _no_train_msg = 'Splitting dataset into train/test set...' environment = train_test_split(environment, train_size=train) train = environment['train'] elif train in environment.keys(): _no_train_msg = f'Using named_provider \"{train}\" as the training set...' train = environment[train] elif not isinstance(train, InstanceProvider): _no_train_msg = 'No training set provided, defaulting to all data as training set...' train = environment.dataset if sklearn_model(model): if not sklearn_fitted(model): if environment is None: raise ImportError('Untrained scikit-learn models require an environment to import!') info('Model is not fitted yet, fitting model!') if _no_train_msg: info(_no_train_msg) if is_classifier(model): model = SkLearnDataClassifier.build(model, environment) model.fit_provider(train, environment.labels) return model else: NotImplementedError('Only classifiers are currently supported!') else: if is_classifier(model): classes = label_map if environment is None else environment return SkLearnDataClassifier.build_from_model(model, classes=classes) else: raise NotImplementedError('Only classifiers are currently supported!') elif 'torch' in str(type(model)): raise ImportError('Convert your PyTorch model with ONNX (https://pytorch.org/docs/stable/onnx.html)' + ' before importing it with instancelib-onnx.') elif 'keras' in str(type(model)): raise ImportError('Convert your Keras model with ONNX (https://github.com/onnx/keras-onnx)' + ' before importing it with instancelib-onnx.') elif 'tensorflow' in str(type(model)): raise ImportError('Convert your Tensorflow model with ONNX (https://github.com/onnx/tensorflow-onnx)' + ' before importing it with instancelib-onnx.') raise NotImplementedError(f'Unknown type of model \"{model}!\"')","title":"Model"},{"location":"reference/text_explainability/model/#module-text_explainabilitymodel","text":"None None View Source 1 2 3 from genbase.model import import_model __all__ = ['import_model']","title":"Module text_explainability.model"},{"location":"reference/text_explainability/model/#functions","text":"","title":"Functions"},{"location":"reference/text_explainability/model/#import_model","text":"1 2 3 4 5 6 def import_model ( model , environment : Optional [ instancelib . environment . base . Environment ] = None , train : Union [ int , float , str , instancelib . instances . base . InstanceProvider ] = 'train' , label_map : Optional [ Dict [ ~ LT , ~ LT ]] = None ) -> instancelib . machinelearning . base . AbstractClassifier Import a model from file or from a Python object. Parameters: Name Type Description Default model None Model or path to model to import. None environment Optional[Environment] Environment corresponding to model (with dataset and ground-truth labels), used for importing models and/or training them. None train Union[int, float, str, InstanceProvider] Train split size, name in environment or provider. Defaults to 'train'. None label_map Optional[Dict[LT, LT]] Conversion of label IDs to named labels. Defaults to None. None Returns: Type Description AbstractClassifier Instancelib wrapped model. Raises: Type Description ImportError Unable to import model or file. NotImplementedError Type of model is not yet supported. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def import_model(model, environment: Optional[Environment] = None, train: Union[int, float, str, InstanceProvider] = 'train', label_map: Optional[Dict[LT, LT]] = None) -> AbstractClassifier: \"\"\"Import a model from file or from a Python object. Examples: Make a scikit-learn text classifier and train it on SST2 >>> from genbase import import_data, import_model >>> from datasets import load_dataset >>> ds = import_data(load_dataset('glue', 'sst2'), data_cols='sentence', label_cols='label') >>> from sklearn.pipeline import Pipeline >>> from sklearn.naive_bayes import MultinomialNB >>> from sklearn.feature_extraction.text import TfidfVectorizer >>> pipeline = Pipeline([('tfidf', TfidfVectorizer()), ... ('clf', MultinomialNB())]) >>> import_model(pipeline, ds, train='train') Load a pretrained ONNX model downloaded from https://github.com/mpbron/instancelib-onnx/blob/main/example_models/data-model.onnx >>> from genbase import import_model >>> import_model('data-model.onnx', label_map={0: 'Bedrijfsnieuws', 1: 'Games', 2: 'Smartphones'}) Args: model: Model or path to model to import. environment (Optional[Environment], optional): Environment corresponding to model (with dataset and ground-truth labels), used for importing models and/or training them. train (Union[int, float, str, InstanceProvider], optional): Train split size, name in environment or provider. Defaults to 'train'. label_map (Optional[Dict[LT, LT]], optional): Conversion of label IDs to named labels. Defaults to None. Raises: ImportError: Unable to import model or file. NotImplementedError: Type of model is not yet supported. Returns: AbstractClassifier: Instancelib wrapped model. \"\"\" if label_map is None and environment is not None: label_map = list(environment.labels.labelset) if isinstance(label_map, dict): label_map = {str(k): v for k, v in label_map.items()} if isinstance(model, str): if not Path(model).exists(): raise ImportError(f'Unable to locate file \"{model}\"') file_type = get_file_type(model) if file_type == '.pkl': import pickle # nosec info('Unpickling model (warning: be sure you trust a source before unpickling a model!)') model = pickle.load(model) # nosec elif file_type == '.onnx': if not package_available('ilonnx'): raise ImportError('To import ONNX files install `instancelib-onnx`!') import ilonnx if label_map is None: info('Improve the informativeness of your predictions by providing the label_map') return ilonnx.build_data_model(model, classes=label_map) elif isinstance(model, AbstractClassifier): return model _no_train_msg = '' if environment is not None: if isinstance(train, (float, int)): _no_train_msg = 'Splitting dataset into train/test set...' environment = train_test_split(environment, train_size=train) train = environment['train'] elif train in environment.keys(): _no_train_msg = f'Using named_provider \"{train}\" as the training set...' train = environment[train] elif not isinstance(train, InstanceProvider): _no_train_msg = 'No training set provided, defaulting to all data as training set...' train = environment.dataset if sklearn_model(model): if not sklearn_fitted(model): if environment is None: raise ImportError('Untrained scikit-learn models require an environment to import!') info('Model is not fitted yet, fitting model!') if _no_train_msg: info(_no_train_msg) if is_classifier(model): model = SkLearnDataClassifier.build(model, environment) model.fit_provider(train, environment.labels) return model else: NotImplementedError('Only classifiers are currently supported!') else: if is_classifier(model): classes = label_map if environment is None else environment return SkLearnDataClassifier.build_from_model(model, classes=classes) else: raise NotImplementedError('Only classifiers are currently supported!') elif 'torch' in str(type(model)): raise ImportError('Convert your PyTorch model with ONNX (https://pytorch.org/docs/stable/onnx.html)' + ' before importing it with instancelib-onnx.') elif 'keras' in str(type(model)): raise ImportError('Convert your Keras model with ONNX (https://github.com/onnx/keras-onnx)' + ' before importing it with instancelib-onnx.') elif 'tensorflow' in str(type(model)): raise ImportError('Convert your Tensorflow model with ONNX (https://github.com/onnx/tensorflow-onnx)' + ' before importing it with instancelib-onnx.') raise NotImplementedError(f'Unknown type of model \"{model}!\"')","title":"import_model"},{"location":"reference/text_explainability/utils/","text":"Module text_explainability.utils Utility functions. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 \"\"\"Utility functions.\"\"\" import re import string from typing import Iterable, Sequence import numpy as np def word_tokenizer(input: str, exclude_curly_brackets: bool = False) -> Sequence[str]: \"\"\"Simple regex tokenizer.\"\"\" pattern = r\"\\{.+?\\}|\\w+|[^\\w\\s]+\" if exclude_curly_brackets else r\"\\w+|[^\\w\\s]+\" return re.findall(pattern, input) def word_detokenizer(input: Iterable[str]) -> str: \"\"\"Simple regex detokenizer, ideally resulting in `i = detokenizer(tokenizer(i))`.\"\"\" out = \" \".join(input).replace(\"`` \", '\"') \\ .replace(\" ''\", '\"') \\ .replace('. . .', '...') \\ .replace(\" ( \", \" (\") \\ .replace(\" ) \", \") \") out = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", out) out = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", out) out = re.sub(r'(\\s+[0-9]+):\\s+([0-9]+\\s+)', r\"\\1:\\2\", out) return out.replace(\" ` \", \" '\").strip() def character_tokenizer(input: str) -> Sequence[str]: \"\"\"Convert a string into a list of characters.\"\"\" return list(input) def character_detokenizer(input: Iterable[str]) -> str: \"\"\"Convert a list of characters into a string.\"\"\" return ''.join(input) def binarize(X: np.ndarray): return (X > 0).astype(int) default_tokenizer = word_tokenizer default_detokenizer = word_detokenizer PUNCTUATION = list(string.punctuation) + ['...'] Variables 1 PUNCTUATION Functions binarize 1 2 3 def binarize ( X : numpy . ndarray ) View Source 1 2 3 def binarize(X: np.ndarray): return (X > 0).astype(int) character_detokenizer 1 2 3 def character_detokenizer ( input : Iterable [ str ] ) -> str Convert a list of characters into a string. View Source 1 2 3 4 5 def character_detokenizer(input: Iterable[str]) -> str: \"\"\"Convert a list of characters into a string.\"\"\" return ''.join(input) character_tokenizer 1 2 3 def character_tokenizer ( input : str ) -> Sequence [ str ] Convert a string into a list of characters. View Source 1 2 3 4 5 def character_tokenizer(input: str) -> Sequence[str]: \"\"\"Convert a string into a list of characters.\"\"\" return list(input) default_detokenizer 1 2 3 def default_detokenizer ( input : Iterable [ str ] ) -> str Simple regex detokenizer, ideally resulting in i = detokenizer(tokenizer(i)) . View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def word_detokenizer(input: Iterable[str]) -> str: \"\"\"Simple regex detokenizer, ideally resulting in `i = detokenizer(tokenizer(i))`.\"\"\" out = \" \".join(input).replace(\"`` \", '\"') \\ .replace(\" ''\", '\"') \\ .replace('. . .', '...') \\ .replace(\" ( \", \" (\") \\ .replace(\" ) \", \") \") out = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", out) out = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", out) out = re.sub(r'(\\s+[0-9]+):\\s+([0-9]+\\s+)', r\"\\1:\\2\", out) return out.replace(\" ` \", \" '\").strip() default_tokenizer 1 2 3 4 def default_tokenizer ( input : str , exclude_curly_brackets : bool = False ) -> Sequence [ str ] Simple regex tokenizer. View Source 1 2 3 4 5 6 7 def word_tokenizer(input: str, exclude_curly_brackets: bool = False) -> Sequence[str]: \"\"\"Simple regex tokenizer.\"\"\" pattern = r\"\\{.+?\\}|\\w+|[^\\w\\s]+\" if exclude_curly_brackets else r\"\\w+|[^\\w\\s]+\" return re.findall(pattern, input) word_detokenizer 1 2 3 def word_detokenizer ( input : Iterable [ str ] ) -> str Simple regex detokenizer, ideally resulting in i = detokenizer(tokenizer(i)) . View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def word_detokenizer(input: Iterable[str]) -> str: \"\"\"Simple regex detokenizer, ideally resulting in `i = detokenizer(tokenizer(i))`.\"\"\" out = \" \".join(input).replace(\"`` \", '\"') \\ .replace(\" ''\", '\"') \\ .replace('. . .', '...') \\ .replace(\" ( \", \" (\") \\ .replace(\" ) \", \") \") out = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", out) out = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", out) out = re.sub(r'(\\s+[0-9]+):\\s+([0-9]+\\s+)', r\"\\1:\\2\", out) return out.replace(\" ` \", \" '\").strip() word_tokenizer 1 2 3 4 def word_tokenizer ( input : str , exclude_curly_brackets : bool = False ) -> Sequence [ str ] Simple regex tokenizer. View Source 1 2 3 4 5 6 7 def word_tokenizer(input: str, exclude_curly_brackets: bool = False) -> Sequence[str]: \"\"\"Simple regex tokenizer.\"\"\" pattern = r\"\\{.+?\\}|\\w+|[^\\w\\s]+\" if exclude_curly_brackets else r\"\\w+|[^\\w\\s]+\" return re.findall(pattern, input)","title":"Utils"},{"location":"reference/text_explainability/utils/#module-text_explainabilityutils","text":"Utility functions. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 \"\"\"Utility functions.\"\"\" import re import string from typing import Iterable, Sequence import numpy as np def word_tokenizer(input: str, exclude_curly_brackets: bool = False) -> Sequence[str]: \"\"\"Simple regex tokenizer.\"\"\" pattern = r\"\\{.+?\\}|\\w+|[^\\w\\s]+\" if exclude_curly_brackets else r\"\\w+|[^\\w\\s]+\" return re.findall(pattern, input) def word_detokenizer(input: Iterable[str]) -> str: \"\"\"Simple regex detokenizer, ideally resulting in `i = detokenizer(tokenizer(i))`.\"\"\" out = \" \".join(input).replace(\"`` \", '\"') \\ .replace(\" ''\", '\"') \\ .replace('. . .', '...') \\ .replace(\" ( \", \" (\") \\ .replace(\" ) \", \") \") out = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", out) out = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", out) out = re.sub(r'(\\s+[0-9]+):\\s+([0-9]+\\s+)', r\"\\1:\\2\", out) return out.replace(\" ` \", \" '\").strip() def character_tokenizer(input: str) -> Sequence[str]: \"\"\"Convert a string into a list of characters.\"\"\" return list(input) def character_detokenizer(input: Iterable[str]) -> str: \"\"\"Convert a list of characters into a string.\"\"\" return ''.join(input) def binarize(X: np.ndarray): return (X > 0).astype(int) default_tokenizer = word_tokenizer default_detokenizer = word_detokenizer PUNCTUATION = list(string.punctuation) + ['...']","title":"Module text_explainability.utils"},{"location":"reference/text_explainability/utils/#variables","text":"1 PUNCTUATION","title":"Variables"},{"location":"reference/text_explainability/utils/#functions","text":"","title":"Functions"},{"location":"reference/text_explainability/utils/#binarize","text":"1 2 3 def binarize ( X : numpy . ndarray ) View Source 1 2 3 def binarize(X: np.ndarray): return (X > 0).astype(int)","title":"binarize"},{"location":"reference/text_explainability/utils/#character_detokenizer","text":"1 2 3 def character_detokenizer ( input : Iterable [ str ] ) -> str Convert a list of characters into a string. View Source 1 2 3 4 5 def character_detokenizer(input: Iterable[str]) -> str: \"\"\"Convert a list of characters into a string.\"\"\" return ''.join(input)","title":"character_detokenizer"},{"location":"reference/text_explainability/utils/#character_tokenizer","text":"1 2 3 def character_tokenizer ( input : str ) -> Sequence [ str ] Convert a string into a list of characters. View Source 1 2 3 4 5 def character_tokenizer(input: str) -> Sequence[str]: \"\"\"Convert a string into a list of characters.\"\"\" return list(input)","title":"character_tokenizer"},{"location":"reference/text_explainability/utils/#default_detokenizer","text":"1 2 3 def default_detokenizer ( input : Iterable [ str ] ) -> str Simple regex detokenizer, ideally resulting in i = detokenizer(tokenizer(i)) . View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def word_detokenizer(input: Iterable[str]) -> str: \"\"\"Simple regex detokenizer, ideally resulting in `i = detokenizer(tokenizer(i))`.\"\"\" out = \" \".join(input).replace(\"`` \", '\"') \\ .replace(\" ''\", '\"') \\ .replace('. . .', '...') \\ .replace(\" ( \", \" (\") \\ .replace(\" ) \", \") \") out = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", out) out = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", out) out = re.sub(r'(\\s+[0-9]+):\\s+([0-9]+\\s+)', r\"\\1:\\2\", out) return out.replace(\" ` \", \" '\").strip()","title":"default_detokenizer"},{"location":"reference/text_explainability/utils/#default_tokenizer","text":"1 2 3 4 def default_tokenizer ( input : str , exclude_curly_brackets : bool = False ) -> Sequence [ str ] Simple regex tokenizer. View Source 1 2 3 4 5 6 7 def word_tokenizer(input: str, exclude_curly_brackets: bool = False) -> Sequence[str]: \"\"\"Simple regex tokenizer.\"\"\" pattern = r\"\\{.+?\\}|\\w+|[^\\w\\s]+\" if exclude_curly_brackets else r\"\\w+|[^\\w\\s]+\" return re.findall(pattern, input)","title":"default_tokenizer"},{"location":"reference/text_explainability/utils/#word_detokenizer","text":"1 2 3 def word_detokenizer ( input : Iterable [ str ] ) -> str Simple regex detokenizer, ideally resulting in i = detokenizer(tokenizer(i)) . View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def word_detokenizer(input: Iterable[str]) -> str: \"\"\"Simple regex detokenizer, ideally resulting in `i = detokenizer(tokenizer(i))`.\"\"\" out = \" \".join(input).replace(\"`` \", '\"') \\ .replace(\" ''\", '\"') \\ .replace('. . .', '...') \\ .replace(\" ( \", \" (\") \\ .replace(\" ) \", \") \") out = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", out) out = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", out) out = re.sub(r'(\\s+[0-9]+):\\s+([0-9]+\\s+)', r\"\\1:\\2\", out) return out.replace(\" ` \", \" '\").strip()","title":"word_detokenizer"},{"location":"reference/text_explainability/utils/#word_tokenizer","text":"1 2 3 4 def word_tokenizer ( input : str , exclude_curly_brackets : bool = False ) -> Sequence [ str ] Simple regex tokenizer. View Source 1 2 3 4 5 6 7 def word_tokenizer(input: str, exclude_curly_brackets: bool = False) -> Sequence[str]: \"\"\"Simple regex tokenizer.\"\"\" pattern = r\"\\{.+?\\}|\\w+|[^\\w\\s]+\" if exclude_curly_brackets else r\"\\w+|[^\\w\\s]+\" return re.findall(pattern, input)","title":"word_tokenizer"},{"location":"reference/text_explainability/data/","text":"Module text_explainability.data Data imports, sampling and generation. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 \"\"\"Data imports, sampling and generation.\"\"\" import os from typing import Callable, Sequence from uuid import uuid4 from genbase.data import import_data, train_test_split from instancelib.environment.text import TextEnvironment from instancelib.instances.text import MemoryTextInstance from instancelib.typehints import LT from ..utils import default_tokenizer def from_list(instances: Sequence[str], labels: Sequence[LT]) -> TextEnvironment: \"\"\"Create a TextEnvironment from a list of instances, and list of labels Example: >>> from_list(instances=['A positive test.', 'A negative test.', 'Another positive test'], >>> labels=['pos', 'neg', 'pos']) Args: instances (Sequence[str]): List of instances. labels (Sequence[LT]): List of corresponding labels. Returns: TextEnvironment: Environment holding data (`.dataset`) and labelprovider (`.labels`). \"\"\" instances, labels = list(instances), list(labels) return TextEnvironment.from_data(indices=list(range(len(instances))), data=instances, target_labels=list(set(labels)), ground_truth=[[label] for label in labels], vectors=[]) def from_string(string: str, tokenizer: Callable[[str], Sequence[str]] = default_tokenizer) -> MemoryTextInstance: \"\"\"Create a MemoryTextInstance from a string. Example: >>> from_string('This is a test example.') Args: string (str): Input string. tokenizer (Callable[[str], Sequence[str]], optional): Tokenizer that converts string into list of tokens (e.g. words or characters). Defaults to default_tokenizer. Returns: MemoryTextInstance: Holds information on the string, and its tokenized representation. \"\"\" return MemoryTextInstance(str(uuid4()), data=string, vector=None, tokenized=tokenizer(string)) __all__ = ['import_data', 'from_list', 'from_string', 'train_test_split'] Sub-modules text_explainability.data.augmentation text_explainability.data.embedding text_explainability.data.sampling text_explainability.data.weights Functions from_list 1 2 3 4 def from_list ( instances : Sequence [ str ], labels : Sequence [ ~ LT ] ) -> instancelib . environment . text . TextEnvironment Create a TextEnvironment from a list of instances, and list of labels Parameters: Name Type Description Default instances Sequence[str] List of instances. None labels Sequence[LT] List of corresponding labels. None Returns: Type Description TextEnvironment Environment holding data ( .dataset ) and labelprovider ( .labels ). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def from_list(instances: Sequence[str], labels: Sequence[LT]) -> TextEnvironment: \"\"\"Create a TextEnvironment from a list of instances, and list of labels Example: >>> from_list(instances=['A positive test.', 'A negative test.', 'Another positive test'], >>> labels=['pos', 'neg', 'pos']) Args: instances (Sequence[str]): List of instances. labels (Sequence[LT]): List of corresponding labels. Returns: TextEnvironment: Environment holding data (`.dataset`) and labelprovider (`.labels`). \"\"\" instances, labels = list(instances), list(labels) return TextEnvironment.from_data(indices=list(range(len(instances))), data=instances, target_labels=list(set(labels)), ground_truth=[[label] for label in labels], vectors=[]) from_string 1 2 3 4 def from_string ( string : str , tokenizer : Callable [[ str ], Sequence [ str ]] = < function word_tokenizer at 0x16c90d4c0 > ) -> instancelib . instances . text . MemoryTextInstance Create a MemoryTextInstance from a string. Parameters: Name Type Description Default string str Input string. None tokenizer Callable[[str], Sequence[str]] Tokenizer that converts string into list of tokens (e.g. words or characters). Defaults to default_tokenizer. None Returns: Type Description MemoryTextInstance Holds information on the string, and its tokenized representation. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def from_string(string: str, tokenizer: Callable[[str], Sequence[str]] = default_tokenizer) -> MemoryTextInstance: \"\"\"Create a MemoryTextInstance from a string. Example: >>> from_string('This is a test example.') Args: string (str): Input string. tokenizer (Callable[[str], Sequence[str]], optional): Tokenizer that converts string into list of tokens (e.g. words or characters). Defaults to default_tokenizer. Returns: MemoryTextInstance: Holds information on the string, and its tokenized representation. \"\"\" return MemoryTextInstance(str(uuid4()), data=string, vector=None, tokenized=tokenizer(string)) import_data 1 2 3 4 5 6 7 8 9 def import_data ( dataset , data_cols : Union [ ~ KT , List [ ~ KT ]], label_cols : Union [ ~ KT , List [ ~ KT ]], label_map : Union [ Callable , dict , NoneType ] = None , method : Literal [ 'infer' , 'glob' , 'pandas' ] = 'infer' , _to_instancelib : bool = True , ** read_kwargs ) -> Union [ instancelib . environment . base . Environment , pandas . core . frame . DataFrame ] Import data in an instancelib Environment. Parameters: Name Type Description Default dataset type Dataset to import. None data_cols Union[KT, List[KT]] Name of column(s) containing data. None label_cols Union[KT, List[KT]] Name of column(s) containing labels. None label_map Optional[Union[Callable, dict]] Label renaming dictionary/function. Defaults to None. None method Method Method used to import data. Choose from 'infer', 'glob', 'pandas'. Defaults to 'infer'. None _to_instancelib bool Whether to convert the final result to instancelib. Defaults to True. True **read_kwargs None Optional arguments passed to reading call. None Returns: Type Description Union[il.Environment, pd.DataFrame] Environment for each file or dataset provided. Raises: Type Description ImportError Unable to import file. ValueError Invalid type of method. NotImplementedError Import not yet implemented. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def import_data(dataset, data_cols: Union[KT, List[KT]], label_cols: Union[KT, List[KT]], label_map: Optional[Union[Callable, dict]] = None, method: Method = 'infer', _to_instancelib: bool = True, **read_kwargs) -> Union[il.Environment, pd.DataFrame]: \"\"\"Import data in an instancelib Environment. Examples: Import from an online .csv file with data in the 'text' column and labels in 'category': >>> from genbase import import_data >>> ds = import_data('https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv', data_cols='text', label_cols='category') Convert a pandas DataFrame to instancelib Environment: >>> from genbase import import_data >>> import pandas as pd >>> df = pd.read_csv('https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv') >>> ds = import_data(df, data_cols='text', label_cols='category') Download a .zip file and convert each file in the zip to an instancelib Environment: >>> from genbase import import_data >>> ds = import_data('https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip', data_cols='review', label_cols='rating') Convert a huggingface dataset (sst2) to an instancelib Environment: >>> from genbase import import_data >>> from datasets import load_dataset >>> ds = import_data(load_dataset('glue', 'sst2'), data_cols='sentence', label_cols='label') Args: dataset (_type_): Dataset to import. data_cols (Union[KT, List[KT]]): Name of column(s) containing data. label_cols (Union[KT, List[KT]]): Name of column(s) containing labels. label_map (Optional[Union[Callable, dict]], optional): Label renaming dictionary/function. Defaults to None. method (Method, optional): Method used to import data. Choose from 'infer', 'glob', 'pandas'. Defaults to 'infer'. _to_instancelib (bool, optional): Whether to convert the final result to instancelib. Defaults to True. **read_kwargs: Optional arguments passed to reading call. Raises: ImportError: Unable to import file. ValueError: Invalid type of method. NotImplementedError: Import not yet implemented. Returns: Union[il.Environment, pd.DataFrame]: Environment for each file or dataset provided. \"\"\" if method not in METHODS: raise ValueError(f'Unknown method \"{method}\", choose from {METHODS}.') if isinstance(data_cols, (int, str)): data_cols = [data_cols] if isinstance(label_cols, (int, str)): label_cols = [label_cols] file_type = get_file_type(dataset) path_like = isinstance(dataset, str) # Unpack archived file if file_type in pd.io.common._compression_to_extension.values(): ioargs = pd.io.common._get_filepath_or_buffer(dataset, compression=file_type.replace('.', '')) info(f'Unpacking file \"{dataset}\".') return import_from_key_values([(file.name, file) for file in get_compressed_files(ioargs)], data_cols=data_cols, label_cols=label_cols, label_map=label_map, method=method, **read_kwargs) # Infer method if method == 'infer': if path_like and '*' in dataset: method = 'glob' elif file_type in PANDAS_FILE_TYPES: method = 'pandas' # Multiple files if method == 'glob': import glob return import_from_key_values([(file, file) for file in glob.glob(dataset)], data_cols=data_cols, label_cols=label_cols, label_map=label_map, method=method, **read_kwargs) # Read one file with Pandas if method == 'pandas': if file_type is not None: info(f'Reading file \"{dataset}\".') if file_type in ['.csv', '.tsv', '.txt']: if 'sep' not in read_kwargs: if file_type == '.csv': read_kwargs['sep']= ',' elif file_type == '.tsv': read_kwargs['sep']= '\\t' dataset = pd.read_csv(dataset, **read_kwargs) elif file_type == '.json': dataset = pd.read_json(dataset, **read_kwargs) elif file_type == '.pkl': dataset = pd.read_pickle(dataset, **read_kwargs) elif file_type in ['.xls', '.xlsx']: dataset = pd.read_excel(dataset, **read_kwargs) else: raise ImportError(f'Unable to process file type \"{file_type}\" with method \"pandas\"!') if hasattr(dataset, 'to_pandas') and callable(dataset.to_pandas): info(f'Preparing \"{dataset}\" for import with Pandas.'.replace('\\n', ' ').replace('\\t', '')) dataset = dataset.to_pandas() elif isinstance(dataset, dict): return import_from_key_values(dataset.items(), data_cols=data_cols, label_cols=label_cols, label_map=label_map, **read_kwargs) if _to_instancelib: return pandas_to_instancelib(dataset, data_cols=data_cols, label_cols=label_cols, label_map=label_map) return dataset train_test_split 1 2 3 4 5 6 def train_test_split ( environment : instancelib . environment . base . Environment , train_size : Union [ int , float ], train_name : str = 'train' , test_name : str = 'test' ) -> instancelib . environment . base . Environment Split an environment into training and test data, and save it to the original environment. Parameters: Name Type Description Default environment instancelib.Environment Environment containing all data ( environment.dataset ), including labels ( environment.labels ). None train_size Union[int, float] Size of training data, as a proportion [0, 1] or number of instances > 1. None train_name str Name of train split. Defaults to 'train'. 'train' test_name str Name of train split. Defaults to 'test'. 'test' Returns: Type Description instancelib.Environment Environment with named splits train_name (containing training data) and test_name (containing test data) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def train_test_split(environment: il.Environment, train_size: Union[int, float], train_name: str = 'train', test_name: str = 'test') -> il.Environment: \"\"\"Split an environment into training and test data, and save it to the original environment. Args: environment (instancelib.Environment): Environment containing all data (`environment.dataset`), including labels (`environment.labels`). train_size (Union[int, float]): Size of training data, as a proportion [0, 1] or number of instances > 1. train_name (str, optional): Name of train split. Defaults to 'train'. test_name (str, optional): Name of train split. Defaults to 'test'. Returns: instancelib.Environment: Environment with named splits `train_name` (containing training data) and `test_name` (containing test data) \"\"\" environment[train_name], environment[test_name] = environment.train_test_split(environment.dataset, train_size=train_size) return environment","title":"Index"},{"location":"reference/text_explainability/data/#module-text_explainabilitydata","text":"Data imports, sampling and generation. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 \"\"\"Data imports, sampling and generation.\"\"\" import os from typing import Callable, Sequence from uuid import uuid4 from genbase.data import import_data, train_test_split from instancelib.environment.text import TextEnvironment from instancelib.instances.text import MemoryTextInstance from instancelib.typehints import LT from ..utils import default_tokenizer def from_list(instances: Sequence[str], labels: Sequence[LT]) -> TextEnvironment: \"\"\"Create a TextEnvironment from a list of instances, and list of labels Example: >>> from_list(instances=['A positive test.', 'A negative test.', 'Another positive test'], >>> labels=['pos', 'neg', 'pos']) Args: instances (Sequence[str]): List of instances. labels (Sequence[LT]): List of corresponding labels. Returns: TextEnvironment: Environment holding data (`.dataset`) and labelprovider (`.labels`). \"\"\" instances, labels = list(instances), list(labels) return TextEnvironment.from_data(indices=list(range(len(instances))), data=instances, target_labels=list(set(labels)), ground_truth=[[label] for label in labels], vectors=[]) def from_string(string: str, tokenizer: Callable[[str], Sequence[str]] = default_tokenizer) -> MemoryTextInstance: \"\"\"Create a MemoryTextInstance from a string. Example: >>> from_string('This is a test example.') Args: string (str): Input string. tokenizer (Callable[[str], Sequence[str]], optional): Tokenizer that converts string into list of tokens (e.g. words or characters). Defaults to default_tokenizer. Returns: MemoryTextInstance: Holds information on the string, and its tokenized representation. \"\"\" return MemoryTextInstance(str(uuid4()), data=string, vector=None, tokenized=tokenizer(string)) __all__ = ['import_data', 'from_list', 'from_string', 'train_test_split']","title":"Module text_explainability.data"},{"location":"reference/text_explainability/data/#sub-modules","text":"text_explainability.data.augmentation text_explainability.data.embedding text_explainability.data.sampling text_explainability.data.weights","title":"Sub-modules"},{"location":"reference/text_explainability/data/#functions","text":"","title":"Functions"},{"location":"reference/text_explainability/data/#from_list","text":"1 2 3 4 def from_list ( instances : Sequence [ str ], labels : Sequence [ ~ LT ] ) -> instancelib . environment . text . TextEnvironment Create a TextEnvironment from a list of instances, and list of labels Parameters: Name Type Description Default instances Sequence[str] List of instances. None labels Sequence[LT] List of corresponding labels. None Returns: Type Description TextEnvironment Environment holding data ( .dataset ) and labelprovider ( .labels ). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def from_list(instances: Sequence[str], labels: Sequence[LT]) -> TextEnvironment: \"\"\"Create a TextEnvironment from a list of instances, and list of labels Example: >>> from_list(instances=['A positive test.', 'A negative test.', 'Another positive test'], >>> labels=['pos', 'neg', 'pos']) Args: instances (Sequence[str]): List of instances. labels (Sequence[LT]): List of corresponding labels. Returns: TextEnvironment: Environment holding data (`.dataset`) and labelprovider (`.labels`). \"\"\" instances, labels = list(instances), list(labels) return TextEnvironment.from_data(indices=list(range(len(instances))), data=instances, target_labels=list(set(labels)), ground_truth=[[label] for label in labels], vectors=[])","title":"from_list"},{"location":"reference/text_explainability/data/#from_string","text":"1 2 3 4 def from_string ( string : str , tokenizer : Callable [[ str ], Sequence [ str ]] = < function word_tokenizer at 0x16c90d4c0 > ) -> instancelib . instances . text . MemoryTextInstance Create a MemoryTextInstance from a string. Parameters: Name Type Description Default string str Input string. None tokenizer Callable[[str], Sequence[str]] Tokenizer that converts string into list of tokens (e.g. words or characters). Defaults to default_tokenizer. None Returns: Type Description MemoryTextInstance Holds information on the string, and its tokenized representation. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def from_string(string: str, tokenizer: Callable[[str], Sequence[str]] = default_tokenizer) -> MemoryTextInstance: \"\"\"Create a MemoryTextInstance from a string. Example: >>> from_string('This is a test example.') Args: string (str): Input string. tokenizer (Callable[[str], Sequence[str]], optional): Tokenizer that converts string into list of tokens (e.g. words or characters). Defaults to default_tokenizer. Returns: MemoryTextInstance: Holds information on the string, and its tokenized representation. \"\"\" return MemoryTextInstance(str(uuid4()), data=string, vector=None, tokenized=tokenizer(string))","title":"from_string"},{"location":"reference/text_explainability/data/#import_data","text":"1 2 3 4 5 6 7 8 9 def import_data ( dataset , data_cols : Union [ ~ KT , List [ ~ KT ]], label_cols : Union [ ~ KT , List [ ~ KT ]], label_map : Union [ Callable , dict , NoneType ] = None , method : Literal [ 'infer' , 'glob' , 'pandas' ] = 'infer' , _to_instancelib : bool = True , ** read_kwargs ) -> Union [ instancelib . environment . base . Environment , pandas . core . frame . DataFrame ] Import data in an instancelib Environment. Parameters: Name Type Description Default dataset type Dataset to import. None data_cols Union[KT, List[KT]] Name of column(s) containing data. None label_cols Union[KT, List[KT]] Name of column(s) containing labels. None label_map Optional[Union[Callable, dict]] Label renaming dictionary/function. Defaults to None. None method Method Method used to import data. Choose from 'infer', 'glob', 'pandas'. Defaults to 'infer'. None _to_instancelib bool Whether to convert the final result to instancelib. Defaults to True. True **read_kwargs None Optional arguments passed to reading call. None Returns: Type Description Union[il.Environment, pd.DataFrame] Environment for each file or dataset provided. Raises: Type Description ImportError Unable to import file. ValueError Invalid type of method. NotImplementedError Import not yet implemented. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def import_data(dataset, data_cols: Union[KT, List[KT]], label_cols: Union[KT, List[KT]], label_map: Optional[Union[Callable, dict]] = None, method: Method = 'infer', _to_instancelib: bool = True, **read_kwargs) -> Union[il.Environment, pd.DataFrame]: \"\"\"Import data in an instancelib Environment. Examples: Import from an online .csv file with data in the 'text' column and labels in 'category': >>> from genbase import import_data >>> ds = import_data('https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv', data_cols='text', label_cols='category') Convert a pandas DataFrame to instancelib Environment: >>> from genbase import import_data >>> import pandas as pd >>> df = pd.read_csv('https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv') >>> ds = import_data(df, data_cols='text', label_cols='category') Download a .zip file and convert each file in the zip to an instancelib Environment: >>> from genbase import import_data >>> ds = import_data('https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip', data_cols='review', label_cols='rating') Convert a huggingface dataset (sst2) to an instancelib Environment: >>> from genbase import import_data >>> from datasets import load_dataset >>> ds = import_data(load_dataset('glue', 'sst2'), data_cols='sentence', label_cols='label') Args: dataset (_type_): Dataset to import. data_cols (Union[KT, List[KT]]): Name of column(s) containing data. label_cols (Union[KT, List[KT]]): Name of column(s) containing labels. label_map (Optional[Union[Callable, dict]], optional): Label renaming dictionary/function. Defaults to None. method (Method, optional): Method used to import data. Choose from 'infer', 'glob', 'pandas'. Defaults to 'infer'. _to_instancelib (bool, optional): Whether to convert the final result to instancelib. Defaults to True. **read_kwargs: Optional arguments passed to reading call. Raises: ImportError: Unable to import file. ValueError: Invalid type of method. NotImplementedError: Import not yet implemented. Returns: Union[il.Environment, pd.DataFrame]: Environment for each file or dataset provided. \"\"\" if method not in METHODS: raise ValueError(f'Unknown method \"{method}\", choose from {METHODS}.') if isinstance(data_cols, (int, str)): data_cols = [data_cols] if isinstance(label_cols, (int, str)): label_cols = [label_cols] file_type = get_file_type(dataset) path_like = isinstance(dataset, str) # Unpack archived file if file_type in pd.io.common._compression_to_extension.values(): ioargs = pd.io.common._get_filepath_or_buffer(dataset, compression=file_type.replace('.', '')) info(f'Unpacking file \"{dataset}\".') return import_from_key_values([(file.name, file) for file in get_compressed_files(ioargs)], data_cols=data_cols, label_cols=label_cols, label_map=label_map, method=method, **read_kwargs) # Infer method if method == 'infer': if path_like and '*' in dataset: method = 'glob' elif file_type in PANDAS_FILE_TYPES: method = 'pandas' # Multiple files if method == 'glob': import glob return import_from_key_values([(file, file) for file in glob.glob(dataset)], data_cols=data_cols, label_cols=label_cols, label_map=label_map, method=method, **read_kwargs) # Read one file with Pandas if method == 'pandas': if file_type is not None: info(f'Reading file \"{dataset}\".') if file_type in ['.csv', '.tsv', '.txt']: if 'sep' not in read_kwargs: if file_type == '.csv': read_kwargs['sep']= ',' elif file_type == '.tsv': read_kwargs['sep']= '\\t' dataset = pd.read_csv(dataset, **read_kwargs) elif file_type == '.json': dataset = pd.read_json(dataset, **read_kwargs) elif file_type == '.pkl': dataset = pd.read_pickle(dataset, **read_kwargs) elif file_type in ['.xls', '.xlsx']: dataset = pd.read_excel(dataset, **read_kwargs) else: raise ImportError(f'Unable to process file type \"{file_type}\" with method \"pandas\"!') if hasattr(dataset, 'to_pandas') and callable(dataset.to_pandas): info(f'Preparing \"{dataset}\" for import with Pandas.'.replace('\\n', ' ').replace('\\t', '')) dataset = dataset.to_pandas() elif isinstance(dataset, dict): return import_from_key_values(dataset.items(), data_cols=data_cols, label_cols=label_cols, label_map=label_map, **read_kwargs) if _to_instancelib: return pandas_to_instancelib(dataset, data_cols=data_cols, label_cols=label_cols, label_map=label_map) return dataset","title":"import_data"},{"location":"reference/text_explainability/data/#train_test_split","text":"1 2 3 4 5 6 def train_test_split ( environment : instancelib . environment . base . Environment , train_size : Union [ int , float ], train_name : str = 'train' , test_name : str = 'test' ) -> instancelib . environment . base . Environment Split an environment into training and test data, and save it to the original environment. Parameters: Name Type Description Default environment instancelib.Environment Environment containing all data ( environment.dataset ), including labels ( environment.labels ). None train_size Union[int, float] Size of training data, as a proportion [0, 1] or number of instances > 1. None train_name str Name of train split. Defaults to 'train'. 'train' test_name str Name of train split. Defaults to 'test'. 'test' Returns: Type Description instancelib.Environment Environment with named splits train_name (containing training data) and test_name (containing test data) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def train_test_split(environment: il.Environment, train_size: Union[int, float], train_name: str = 'train', test_name: str = 'test') -> il.Environment: \"\"\"Split an environment into training and test data, and save it to the original environment. Args: environment (instancelib.Environment): Environment containing all data (`environment.dataset`), including labels (`environment.labels`). train_size (Union[int, float]): Size of training data, as a proportion [0, 1] or number of instances > 1. train_name (str, optional): Name of train split. Defaults to 'train'. test_name (str, optional): Name of train split. Defaults to 'test'. Returns: instancelib.Environment: Environment with named splits `train_name` (containing training data) and `test_name` (containing test data) \"\"\" environment[train_name], environment[test_name] = environment.train_test_split(environment.dataset, train_size=train_size) return environment","title":"train_test_split"},{"location":"reference/text_explainability/data/augmentation/","text":"Module text_explainability.data.augmentation Augment a single instance to generate neighborhood data. Todo: 1 2 3 4 * Add more complex sampling methods (e.g. top-k replacement by contextual language model, WordNet, ...) * Replacement with k tokens at each index * Ensure inactive[i] is set to 0 if the replacement token is the same as the original token[i] * Replace seed with SeedMixin View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 \"\"\"Augment a single instance to generate neighborhood data. Todo: * Add more complex sampling methods (e.g. top-k replacement by contextual language model, WordNet, ...) * Replacement with k tokens at each index * Ensure inactive[i] is set to 0 if the replacement token is the same as the original token[i] * Replace seed with SeedMixin \"\"\" import itertools import math from typing import (Any, Callable, Iterable, Iterator, List, Optional, Tuple, Union) import numpy as np from genbase import Readable, SeedMixin from instancelib.environment.base import AbstractEnvironment from instancelib.environment.text import TextEnvironment from instancelib.instances.base import InstanceProvider from instancelib.instances.text import TextInstance from instancelib.pertubations.base import ChildGenerator, MultiplePertubator from ..decorators import text_instance from ..utils import default_detokenizer class LocalTokenPertubator(MultiplePertubator[TextInstance], ChildGenerator[TextInstance], Readable): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer): \"\"\"Perturb a single instance into neighborhood samples. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. \"\"\" super().__init__() if env is None: env = TextEnvironment.from_data([], [], [], [], []) self.env = env self.detokenizer = detokenizer @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res def perturb(self, tokenized_instance: Iterable[str], *args: Any, **kwargs: Any) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: raise NotImplementedError('Implemented in subclasses') def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent) def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent) @text_instance(tokenize=True) def __call__(self, instance: TextInstance, discard_children: bool = True, *args, **kwargs) -> Iterator[TextInstance]: \"\"\"Apply perturbations to an instance to generate neighborhood data. Args: instance (TextInstance): Tokenized instance to perturb. discard_children (bool, optional): Remove children from previous passes. Defaults to True. *args: Arguments to be passed on to `perturb()` function. **kwargs: Keyword arguments to be passed on to `perturb()` function. Yields: Iterator[Sequence[TextInstance]]: Neighborhood data instances. \"\"\" if instance.data not in self.env.all_instances.all_data(): provider = self.env.create_empty_provider() provider.add(instance) if discard_children: self.discard_children(instance) for new_tokenized, map_to_original in self.perturb(instance.tokenized, *args, **kwargs): new_data = self.detokenizer(new_tokenized) new_instance = self.env.create( data=new_data, vector=None, map_to_original=map_to_original, representation=new_data, tokenized=new_tokenized ) self.register_child(instance, new_instance) return self.get_children(instance) class TokenReplacement(LocalTokenPertubator, SeedMixin): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer, replacement: Optional[Union[str, List[str]]] = 'UNKWRDZ', seed: int = 0): \"\"\"Perturb a tokenized instance by replacing with a set token (e.g. 'UNKWRDZ') or deleting it. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. replacement (Optional[Union[str, List[str]]], optional): Replacement string, or set to None if you want to delete the word entirely. Defaults to 'UNKWRDZ'. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__(env=env, detokenizer=detokenizer) self.replacement = replacement self._seed = self._original_seed = seed def _replace(self, tokenized_instance: Iterable[str], keep: Iterable[int]) -> Iterable[str]: \"\"\"Apply replacement/deletion to tokenized instance. Args: tokenized_instance (Iterable[str]): Tokenized instance. keep (Iterable[int]): Binary indicator whether to keep (1) or replace (0) a token. Raises: ValueError: Too few replacements in self.replacement. Returns: Iterable[str]: Tokenized instance with perturbation applied. \"\"\" if not self.replacement or self.replacement is None: return [token for token, i in zip(tokenized_instance, keep) if i == 1] if isinstance(self.replacement, list): instance_len = sum(1 for _ in tokenized_instance) replacement_len = len(self.replacement) if not (replacement_len >= instance_len): raise ValueError(f'Too few replacements in `self.replacement`, got {replacement_len} ', f'and expected {instance_len}') return [self.replacement[i] if j == 0 else token for i, (token, j) in enumerate(zip(tokenized_instance, keep))] return [self.replacement if i == 0 else token for token, i in zip(tokenized_instance, keep)] def perturb(self, tokenized_instance: Iterable[str], n_samples: int = 50, sequential: bool = True, contiguous: bool = False, min_changes: int = 1, max_changes: int = 10000, add_background_instance: bool = False) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: \"\"\"Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Example: Randomly replace at least two tokens with the replacement word 'UNK': >>> from text_explainability.augmentation import TokenReplacement >>> TokenReplacement(replacement='UNK').perturb(['perturb', 'this', 'into', 'multiple'], >>> n_samples=3, >>> min_changes=2) Args: tokenized_instance (Iterable[str]): Tokenized instance to apply perturbations to. n_samples (int, optional): Number of samples to return. Defaults to 50. sequential (bool, optional): Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. contiguous (bool, optional): Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. min_changes (int, optional): Minimum number of tokens changes (1+). Defaults to 1. max_changes (int, optional): Maximum number of tokens changed. Defaults to 10000. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. Raises: ValueError: min_changes cannot be greater than max_changes. Yields: Iterator[Sequence[Iterable[str], Iterable[int]]]: Pertubed text instances and indices where perturbation were applied. \"\"\" instance_len = sum(1 for _ in tokenized_instance) min_changes = min(max(min_changes, 1), instance_len) max_changes = min(instance_len, max_changes) rand = np.random.RandomState(self.seed) def get_inactive(inactive_range): inactive = TokenReplacement.binary_inactive(inactive_range, instance_len) return self._replace(tokenized_instance, inactive), inactive if sequential: if contiguous: # n-grams of length size, up to n_samples for size in range(min_changes, max_changes + 1): n_contiguous = instance_len - size if n_contiguous <= n_samples: n_samples -= n_contiguous for start in range(instance_len - size + 1): yield get_inactive(range(start, start + size)) else: for start in rand.choice(instance_len - size + 1, size=n_samples, replace=False): yield get_inactive(range(start, start + size)) break else: # used by SHAP for size in range(min_changes, max_changes + 1): n_choose_k = math.comb(instance_len, size) if n_choose_k <= n_samples: # make all combinations of length size n_samples -= n_choose_k for disable in itertools.combinations(range(instance_len), size): yield get_inactive(disable) else: # fill up remainder with random samples of length size for _ in range(n_samples): yield get_inactive(rand.choice(instance_len, size, replace=False)) break else: sample = rand.randint(min_changes, max_changes + 1, n_samples) for size in sample: if contiguous: # use n-grams start = rand.choice(max_changes - size + 1, replace=False) inactive = TokenReplacement.binary_inactive(range(start, start + size), instance_len) # used by LIME, # https://github.com/marcotcr/lime/blob/a2c7a6fb70bce2e089cb146a31f483bf218875eb/lime/lime_text.py#L436 else: inactive = TokenReplacement.binary_inactive(rand.choice(instance_len, size, replace=False), instance_len) yield self._replace(tokenized_instance, inactive), inactive if add_background_instance: inactive = np.zeros(instance_len) yield self._replace(tokenized_instance, inactive), inactive class LeaveOut(TokenReplacement): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer, seed: int = 0): \"\"\"Leave tokens out of the tokenized sequence. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__(env=env, detokenizer=detokenizer, replacement=None, seed=seed) Classes LeaveOut 1 2 3 4 5 class LeaveOut ( env : Optional [ instancelib . environment . base . AbstractEnvironment [ instancelib . instances . text . TextInstance , Any , Any , Any , Any , Any ]] = None , detokenizer : Optional [ Callable [[ Iterable [ str ]], str ]] = < function word_detokenizer at 0x16eaef670 > , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class LeaveOut(TokenReplacement): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer, seed: int = 0): \"\"\"Leave tokens out of the tokenized sequence. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__(env=env, detokenizer=detokenizer, replacement=None, seed=seed) Ancestors (in MRO) text_explainability.data.augmentation.TokenReplacement text_explainability.data.augmentation.LocalTokenPertubator instancelib.pertubations.base.MultiplePertubator instancelib.pertubations.base.ChildGenerator abc.ABC typing.Generic genbase.Readable genbase.mixin.SeedMixin Static methods binary_inactive 1 2 3 4 def binary_inactive ( inactive , length ) -> numpy . ndarray View Source 1 2 3 4 5 6 7 8 9 10 11 @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res Instance variables 1 seed Methods discard_children 1 2 3 4 def discard_children ( self , parent : instancelib . instances . text . TextInstance ) -> None Discard the generated children for a given parent. View Source 1 2 3 4 5 def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent) get_children 1 2 3 4 def get_children ( self , parent : instancelib . instances . text . TextInstance ) -> instancelib . instances . base . InstanceProvider [ instancelib . instances . text . TextInstance , typing . Any , typing . Any , typing . Any , typing . Any ] Get the children of a given parent. View Source 1 2 3 4 5 def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent) perturb 1 2 3 4 5 6 7 8 9 10 def perturb ( self , tokenized_instance : Iterable [ str ], n_samples : int = 50 , sequential : bool = True , contiguous : bool = False , min_changes : int = 1 , max_changes : int = 10000 , add_background_instance : bool = False ) -> Iterator [ Tuple [ Iterable [ str ], Iterable [ int ]]] Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Parameters: Name Type Description Default tokenized_instance Iterable[str] Tokenized instance to apply perturbations to. None n_samples int Number of samples to return. Defaults to 50. 50 sequential bool Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. None contiguous bool Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. False min_changes int Minimum number of tokens changes (1+). Defaults to 1. 1 max_changes int Maximum number of tokens changed. Defaults to 10000. 10000 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. None Yields: Type Description Iterator[Sequence[Iterable[str], Iterable[int]]] Pertubed text instances and indices where perturbation were applied. Raises: Type Description ValueError min_changes cannot be greater than max_changes. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def perturb(self, tokenized_instance: Iterable[str], n_samples: int = 50, sequential: bool = True, contiguous: bool = False, min_changes: int = 1, max_changes: int = 10000, add_background_instance: bool = False) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: \"\"\"Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Example: Randomly replace at least two tokens with the replacement word 'UNK': >>> from text_explainability.augmentation import TokenReplacement >>> TokenReplacement(replacement='UNK').perturb(['perturb', 'this', 'into', 'multiple'], >>> n_samples=3, >>> min_changes=2) Args: tokenized_instance (Iterable[str]): Tokenized instance to apply perturbations to. n_samples (int, optional): Number of samples to return. Defaults to 50. sequential (bool, optional): Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. contiguous (bool, optional): Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. min_changes (int, optional): Minimum number of tokens changes (1+). Defaults to 1. max_changes (int, optional): Maximum number of tokens changed. Defaults to 10000. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. Raises: ValueError: min_changes cannot be greater than max_changes. Yields: Iterator[Sequence[Iterable[str], Iterable[int]]]: Pertubed text instances and indices where perturbation were applied. \"\"\" instance_len = sum(1 for _ in tokenized_instance) min_changes = min(max(min_changes, 1), instance_len) max_changes = min(instance_len, max_changes) rand = np.random.RandomState(self.seed) def get_inactive(inactive_range): inactive = TokenReplacement.binary_inactive(inactive_range, instance_len) return self._replace(tokenized_instance, inactive), inactive if sequential: if contiguous: # n-grams of length size, up to n_samples for size in range(min_changes, max_changes + 1): n_contiguous = instance_len - size if n_contiguous <= n_samples: n_samples -= n_contiguous for start in range(instance_len - size + 1): yield get_inactive(range(start, start + size)) else: for start in rand.choice(instance_len - size + 1, size=n_samples, replace=False): yield get_inactive(range(start, start + size)) break else: # used by SHAP for size in range(min_changes, max_changes + 1): n_choose_k = math.comb(instance_len, size) if n_choose_k <= n_samples: # make all combinations of length size n_samples -= n_choose_k for disable in itertools.combinations(range(instance_len), size): yield get_inactive(disable) else: # fill up remainder with random samples of length size for _ in range(n_samples): yield get_inactive(rand.choice(instance_len, size, replace=False)) break else: sample = rand.randint(min_changes, max_changes + 1, n_samples) for size in sample: if contiguous: # use n-grams start = rand.choice(max_changes - size + 1, replace=False) inactive = TokenReplacement.binary_inactive(range(start, start + size), instance_len) # used by LIME, # https://github.com/marcotcr/lime/blob/a2c7a6fb70bce2e089cb146a31f483bf218875eb/lime/lime_text.py#L436 else: inactive = TokenReplacement.binary_inactive(rand.choice(instance_len, size, replace=False), instance_len) yield self._replace(tokenized_instance, inactive), inactive if add_background_instance: inactive = np.zeros(instance_len) yield self._replace(tokenized_instance, inactive), inactive register_child 1 2 3 4 5 def register_child ( self , parent : ~ IT , child : ~ IT ) -> None View Source 1 2 3 def register_child(self, parent: IT, child: IT) -> None: self.env.all_instances.add_child(parent, child) reset_seed 1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self set_seed 1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed() LocalTokenPertubator 1 2 3 4 class LocalTokenPertubator ( env : Optional [ instancelib . environment . base . AbstractEnvironment [ instancelib . instances . text . TextInstance , Any , Any , Any , Any , Any ]] = None , detokenizer : Optional [ Callable [[ Iterable [ str ]], str ]] = < function word_detokenizer at 0x16eaef670 > ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 class LocalTokenPertubator(MultiplePertubator[TextInstance], ChildGenerator[TextInstance], Readable): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer): \"\"\"Perturb a single instance into neighborhood samples. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. \"\"\" super().__init__() if env is None: env = TextEnvironment.from_data([], [], [], [], []) self.env = env self.detokenizer = detokenizer @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res def perturb(self, tokenized_instance: Iterable[str], *args: Any, **kwargs: Any) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: raise NotImplementedError('Implemented in subclasses') def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent) def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent) @text_instance(tokenize=True) def __call__(self, instance: TextInstance, discard_children: bool = True, *args, **kwargs) -> Iterator[TextInstance]: \"\"\"Apply perturbations to an instance to generate neighborhood data. Args: instance (TextInstance): Tokenized instance to perturb. discard_children (bool, optional): Remove children from previous passes. Defaults to True. *args: Arguments to be passed on to `perturb()` function. **kwargs: Keyword arguments to be passed on to `perturb()` function. Yields: Iterator[Sequence[TextInstance]]: Neighborhood data instances. \"\"\" if instance.data not in self.env.all_instances.all_data(): provider = self.env.create_empty_provider() provider.add(instance) if discard_children: self.discard_children(instance) for new_tokenized, map_to_original in self.perturb(instance.tokenized, *args, **kwargs): new_data = self.detokenizer(new_tokenized) new_instance = self.env.create( data=new_data, vector=None, map_to_original=map_to_original, representation=new_data, tokenized=new_tokenized ) self.register_child(instance, new_instance) return self.get_children(instance) Ancestors (in MRO) instancelib.pertubations.base.MultiplePertubator instancelib.pertubations.base.ChildGenerator abc.ABC typing.Generic genbase.Readable Descendants text_explainability.data.augmentation.TokenReplacement Static methods binary_inactive 1 2 3 4 def binary_inactive ( inactive , length ) -> numpy . ndarray View Source 1 2 3 4 5 6 7 8 9 10 11 @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res Methods discard_children 1 2 3 4 def discard_children ( self , parent : instancelib . instances . text . TextInstance ) -> None Discard the generated children for a given parent. View Source 1 2 3 4 5 def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent) get_children 1 2 3 4 def get_children ( self , parent : instancelib . instances . text . TextInstance ) -> instancelib . instances . base . InstanceProvider [ instancelib . instances . text . TextInstance , typing . Any , typing . Any , typing . Any , typing . Any ] Get the children of a given parent. View Source 1 2 3 4 5 def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent) perturb 1 2 3 4 5 6 def perturb ( self , tokenized_instance : Iterable [ str ], * args : Any , ** kwargs : Any ) -> Iterator [ Tuple [ Iterable [ str ], Iterable [ int ]]] View Source 1 2 3 4 5 def perturb(self, tokenized_instance: Iterable[str], *args: Any, **kwargs: Any) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: raise NotImplementedError('Implemented in subclasses') register_child 1 2 3 4 5 def register_child ( self , parent : ~ IT , child : ~ IT ) -> None View Source 1 2 3 def register_child(self, parent: IT, child: IT) -> None: self.env.all_instances.add_child(parent, child) TokenReplacement 1 2 3 4 5 6 class TokenReplacement ( env : Optional [ instancelib . environment . base . AbstractEnvironment [ instancelib . instances . text . TextInstance , Any , Any , Any , Any , Any ]] = None , detokenizer : Optional [ Callable [[ Iterable [ str ]], str ]] = < function word_detokenizer at 0x16eaef670 > , replacement : Union [ str , List [ str ], NoneType ] = 'UNKWRDZ' , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 class TokenReplacement(LocalTokenPertubator, SeedMixin): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer, replacement: Optional[Union[str, List[str]]] = 'UNKWRDZ', seed: int = 0): \"\"\"Perturb a tokenized instance by replacing with a set token (e.g. 'UNKWRDZ') or deleting it. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. replacement (Optional[Union[str, List[str]]], optional): Replacement string, or set to None if you want to delete the word entirely. Defaults to 'UNKWRDZ'. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__(env=env, detokenizer=detokenizer) self.replacement = replacement self._seed = self._original_seed = seed def _replace(self, tokenized_instance: Iterable[str], keep: Iterable[int]) -> Iterable[str]: \"\"\"Apply replacement/deletion to tokenized instance. Args: tokenized_instance (Iterable[str]): Tokenized instance. keep (Iterable[int]): Binary indicator whether to keep (1) or replace (0) a token. Raises: ValueError: Too few replacements in self.replacement. Returns: Iterable[str]: Tokenized instance with perturbation applied. \"\"\" if not self.replacement or self.replacement is None: return [token for token, i in zip(tokenized_instance, keep) if i == 1] if isinstance(self.replacement, list): instance_len = sum(1 for _ in tokenized_instance) replacement_len = len(self.replacement) if not (replacement_len >= instance_len): raise ValueError(f'Too few replacements in `self.replacement`, got {replacement_len} ', f'and expected {instance_len}') return [self.replacement[i] if j == 0 else token for i, (token, j) in enumerate(zip(tokenized_instance, keep))] return [self.replacement if i == 0 else token for token, i in zip(tokenized_instance, keep)] def perturb(self, tokenized_instance: Iterable[str], n_samples: int = 50, sequential: bool = True, contiguous: bool = False, min_changes: int = 1, max_changes: int = 10000, add_background_instance: bool = False) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: \"\"\"Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Example: Randomly replace at least two tokens with the replacement word 'UNK': >>> from text_explainability.augmentation import TokenReplacement >>> TokenReplacement(replacement='UNK').perturb(['perturb', 'this', 'into', 'multiple'], >>> n_samples=3, >>> min_changes=2) Args: tokenized_instance (Iterable[str]): Tokenized instance to apply perturbations to. n_samples (int, optional): Number of samples to return. Defaults to 50. sequential (bool, optional): Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. contiguous (bool, optional): Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. min_changes (int, optional): Minimum number of tokens changes (1+). Defaults to 1. max_changes (int, optional): Maximum number of tokens changed. Defaults to 10000. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. Raises: ValueError: min_changes cannot be greater than max_changes. Yields: Iterator[Sequence[Iterable[str], Iterable[int]]]: Pertubed text instances and indices where perturbation were applied. \"\"\" instance_len = sum(1 for _ in tokenized_instance) min_changes = min(max(min_changes, 1), instance_len) max_changes = min(instance_len, max_changes) rand = np.random.RandomState(self.seed) def get_inactive(inactive_range): inactive = TokenReplacement.binary_inactive(inactive_range, instance_len) return self._replace(tokenized_instance, inactive), inactive if sequential: if contiguous: # n-grams of length size, up to n_samples for size in range(min_changes, max_changes + 1): n_contiguous = instance_len - size if n_contiguous <= n_samples: n_samples -= n_contiguous for start in range(instance_len - size + 1): yield get_inactive(range(start, start + size)) else: for start in rand.choice(instance_len - size + 1, size=n_samples, replace=False): yield get_inactive(range(start, start + size)) break else: # used by SHAP for size in range(min_changes, max_changes + 1): n_choose_k = math.comb(instance_len, size) if n_choose_k <= n_samples: # make all combinations of length size n_samples -= n_choose_k for disable in itertools.combinations(range(instance_len), size): yield get_inactive(disable) else: # fill up remainder with random samples of length size for _ in range(n_samples): yield get_inactive(rand.choice(instance_len, size, replace=False)) break else: sample = rand.randint(min_changes, max_changes + 1, n_samples) for size in sample: if contiguous: # use n-grams start = rand.choice(max_changes - size + 1, replace=False) inactive = TokenReplacement.binary_inactive(range(start, start + size), instance_len) # used by LIME, # https://github.com/marcotcr/lime/blob/a2c7a6fb70bce2e089cb146a31f483bf218875eb/lime/lime_text.py#L436 else: inactive = TokenReplacement.binary_inactive(rand.choice(instance_len, size, replace=False), instance_len) yield self._replace(tokenized_instance, inactive), inactive if add_background_instance: inactive = np.zeros(instance_len) yield self._replace(tokenized_instance, inactive), inactive Ancestors (in MRO) text_explainability.data.augmentation.LocalTokenPertubator instancelib.pertubations.base.MultiplePertubator instancelib.pertubations.base.ChildGenerator abc.ABC typing.Generic genbase.Readable genbase.mixin.SeedMixin Descendants text_explainability.data.augmentation.LeaveOut Static methods binary_inactive 1 2 3 4 def binary_inactive ( inactive , length ) -> numpy . ndarray View Source 1 2 3 4 5 6 7 8 9 10 11 @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res Instance variables 1 seed Methods discard_children 1 2 3 4 def discard_children ( self , parent : instancelib . instances . text . TextInstance ) -> None Discard the generated children for a given parent. View Source 1 2 3 4 5 def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent) get_children 1 2 3 4 def get_children ( self , parent : instancelib . instances . text . TextInstance ) -> instancelib . instances . base . InstanceProvider [ instancelib . instances . text . TextInstance , typing . Any , typing . Any , typing . Any , typing . Any ] Get the children of a given parent. View Source 1 2 3 4 5 def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent) perturb 1 2 3 4 5 6 7 8 9 10 def perturb ( self , tokenized_instance : Iterable [ str ], n_samples : int = 50 , sequential : bool = True , contiguous : bool = False , min_changes : int = 1 , max_changes : int = 10000 , add_background_instance : bool = False ) -> Iterator [ Tuple [ Iterable [ str ], Iterable [ int ]]] Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Parameters: Name Type Description Default tokenized_instance Iterable[str] Tokenized instance to apply perturbations to. None n_samples int Number of samples to return. Defaults to 50. 50 sequential bool Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. None contiguous bool Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. False min_changes int Minimum number of tokens changes (1+). Defaults to 1. 1 max_changes int Maximum number of tokens changed. Defaults to 10000. 10000 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. None Yields: Type Description Iterator[Sequence[Iterable[str], Iterable[int]]] Pertubed text instances and indices where perturbation were applied. Raises: Type Description ValueError min_changes cannot be greater than max_changes. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def perturb(self, tokenized_instance: Iterable[str], n_samples: int = 50, sequential: bool = True, contiguous: bool = False, min_changes: int = 1, max_changes: int = 10000, add_background_instance: bool = False) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: \"\"\"Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Example: Randomly replace at least two tokens with the replacement word 'UNK': >>> from text_explainability.augmentation import TokenReplacement >>> TokenReplacement(replacement='UNK').perturb(['perturb', 'this', 'into', 'multiple'], >>> n_samples=3, >>> min_changes=2) Args: tokenized_instance (Iterable[str]): Tokenized instance to apply perturbations to. n_samples (int, optional): Number of samples to return. Defaults to 50. sequential (bool, optional): Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. contiguous (bool, optional): Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. min_changes (int, optional): Minimum number of tokens changes (1+). Defaults to 1. max_changes (int, optional): Maximum number of tokens changed. Defaults to 10000. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. Raises: ValueError: min_changes cannot be greater than max_changes. Yields: Iterator[Sequence[Iterable[str], Iterable[int]]]: Pertubed text instances and indices where perturbation were applied. \"\"\" instance_len = sum(1 for _ in tokenized_instance) min_changes = min(max(min_changes, 1), instance_len) max_changes = min(instance_len, max_changes) rand = np.random.RandomState(self.seed) def get_inactive(inactive_range): inactive = TokenReplacement.binary_inactive(inactive_range, instance_len) return self._replace(tokenized_instance, inactive), inactive if sequential: if contiguous: # n-grams of length size, up to n_samples for size in range(min_changes, max_changes + 1): n_contiguous = instance_len - size if n_contiguous <= n_samples: n_samples -= n_contiguous for start in range(instance_len - size + 1): yield get_inactive(range(start, start + size)) else: for start in rand.choice(instance_len - size + 1, size=n_samples, replace=False): yield get_inactive(range(start, start + size)) break else: # used by SHAP for size in range(min_changes, max_changes + 1): n_choose_k = math.comb(instance_len, size) if n_choose_k <= n_samples: # make all combinations of length size n_samples -= n_choose_k for disable in itertools.combinations(range(instance_len), size): yield get_inactive(disable) else: # fill up remainder with random samples of length size for _ in range(n_samples): yield get_inactive(rand.choice(instance_len, size, replace=False)) break else: sample = rand.randint(min_changes, max_changes + 1, n_samples) for size in sample: if contiguous: # use n-grams start = rand.choice(max_changes - size + 1, replace=False) inactive = TokenReplacement.binary_inactive(range(start, start + size), instance_len) # used by LIME, # https://github.com/marcotcr/lime/blob/a2c7a6fb70bce2e089cb146a31f483bf218875eb/lime/lime_text.py#L436 else: inactive = TokenReplacement.binary_inactive(rand.choice(instance_len, size, replace=False), instance_len) yield self._replace(tokenized_instance, inactive), inactive if add_background_instance: inactive = np.zeros(instance_len) yield self._replace(tokenized_instance, inactive), inactive register_child 1 2 3 4 5 def register_child ( self , parent : ~ IT , child : ~ IT ) -> None View Source 1 2 3 def register_child(self, parent: IT, child: IT) -> None: self.env.all_instances.add_child(parent, child) reset_seed 1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self set_seed 1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed()","title":"Augmentation"},{"location":"reference/text_explainability/data/augmentation/#module-text_explainabilitydataaugmentation","text":"Augment a single instance to generate neighborhood data. Todo: 1 2 3 4 * Add more complex sampling methods (e.g. top-k replacement by contextual language model, WordNet, ...) * Replacement with k tokens at each index * Ensure inactive[i] is set to 0 if the replacement token is the same as the original token[i] * Replace seed with SeedMixin View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 \"\"\"Augment a single instance to generate neighborhood data. Todo: * Add more complex sampling methods (e.g. top-k replacement by contextual language model, WordNet, ...) * Replacement with k tokens at each index * Ensure inactive[i] is set to 0 if the replacement token is the same as the original token[i] * Replace seed with SeedMixin \"\"\" import itertools import math from typing import (Any, Callable, Iterable, Iterator, List, Optional, Tuple, Union) import numpy as np from genbase import Readable, SeedMixin from instancelib.environment.base import AbstractEnvironment from instancelib.environment.text import TextEnvironment from instancelib.instances.base import InstanceProvider from instancelib.instances.text import TextInstance from instancelib.pertubations.base import ChildGenerator, MultiplePertubator from ..decorators import text_instance from ..utils import default_detokenizer class LocalTokenPertubator(MultiplePertubator[TextInstance], ChildGenerator[TextInstance], Readable): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer): \"\"\"Perturb a single instance into neighborhood samples. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. \"\"\" super().__init__() if env is None: env = TextEnvironment.from_data([], [], [], [], []) self.env = env self.detokenizer = detokenizer @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res def perturb(self, tokenized_instance: Iterable[str], *args: Any, **kwargs: Any) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: raise NotImplementedError('Implemented in subclasses') def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent) def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent) @text_instance(tokenize=True) def __call__(self, instance: TextInstance, discard_children: bool = True, *args, **kwargs) -> Iterator[TextInstance]: \"\"\"Apply perturbations to an instance to generate neighborhood data. Args: instance (TextInstance): Tokenized instance to perturb. discard_children (bool, optional): Remove children from previous passes. Defaults to True. *args: Arguments to be passed on to `perturb()` function. **kwargs: Keyword arguments to be passed on to `perturb()` function. Yields: Iterator[Sequence[TextInstance]]: Neighborhood data instances. \"\"\" if instance.data not in self.env.all_instances.all_data(): provider = self.env.create_empty_provider() provider.add(instance) if discard_children: self.discard_children(instance) for new_tokenized, map_to_original in self.perturb(instance.tokenized, *args, **kwargs): new_data = self.detokenizer(new_tokenized) new_instance = self.env.create( data=new_data, vector=None, map_to_original=map_to_original, representation=new_data, tokenized=new_tokenized ) self.register_child(instance, new_instance) return self.get_children(instance) class TokenReplacement(LocalTokenPertubator, SeedMixin): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer, replacement: Optional[Union[str, List[str]]] = 'UNKWRDZ', seed: int = 0): \"\"\"Perturb a tokenized instance by replacing with a set token (e.g. 'UNKWRDZ') or deleting it. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. replacement (Optional[Union[str, List[str]]], optional): Replacement string, or set to None if you want to delete the word entirely. Defaults to 'UNKWRDZ'. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__(env=env, detokenizer=detokenizer) self.replacement = replacement self._seed = self._original_seed = seed def _replace(self, tokenized_instance: Iterable[str], keep: Iterable[int]) -> Iterable[str]: \"\"\"Apply replacement/deletion to tokenized instance. Args: tokenized_instance (Iterable[str]): Tokenized instance. keep (Iterable[int]): Binary indicator whether to keep (1) or replace (0) a token. Raises: ValueError: Too few replacements in self.replacement. Returns: Iterable[str]: Tokenized instance with perturbation applied. \"\"\" if not self.replacement or self.replacement is None: return [token for token, i in zip(tokenized_instance, keep) if i == 1] if isinstance(self.replacement, list): instance_len = sum(1 for _ in tokenized_instance) replacement_len = len(self.replacement) if not (replacement_len >= instance_len): raise ValueError(f'Too few replacements in `self.replacement`, got {replacement_len} ', f'and expected {instance_len}') return [self.replacement[i] if j == 0 else token for i, (token, j) in enumerate(zip(tokenized_instance, keep))] return [self.replacement if i == 0 else token for token, i in zip(tokenized_instance, keep)] def perturb(self, tokenized_instance: Iterable[str], n_samples: int = 50, sequential: bool = True, contiguous: bool = False, min_changes: int = 1, max_changes: int = 10000, add_background_instance: bool = False) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: \"\"\"Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Example: Randomly replace at least two tokens with the replacement word 'UNK': >>> from text_explainability.augmentation import TokenReplacement >>> TokenReplacement(replacement='UNK').perturb(['perturb', 'this', 'into', 'multiple'], >>> n_samples=3, >>> min_changes=2) Args: tokenized_instance (Iterable[str]): Tokenized instance to apply perturbations to. n_samples (int, optional): Number of samples to return. Defaults to 50. sequential (bool, optional): Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. contiguous (bool, optional): Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. min_changes (int, optional): Minimum number of tokens changes (1+). Defaults to 1. max_changes (int, optional): Maximum number of tokens changed. Defaults to 10000. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. Raises: ValueError: min_changes cannot be greater than max_changes. Yields: Iterator[Sequence[Iterable[str], Iterable[int]]]: Pertubed text instances and indices where perturbation were applied. \"\"\" instance_len = sum(1 for _ in tokenized_instance) min_changes = min(max(min_changes, 1), instance_len) max_changes = min(instance_len, max_changes) rand = np.random.RandomState(self.seed) def get_inactive(inactive_range): inactive = TokenReplacement.binary_inactive(inactive_range, instance_len) return self._replace(tokenized_instance, inactive), inactive if sequential: if contiguous: # n-grams of length size, up to n_samples for size in range(min_changes, max_changes + 1): n_contiguous = instance_len - size if n_contiguous <= n_samples: n_samples -= n_contiguous for start in range(instance_len - size + 1): yield get_inactive(range(start, start + size)) else: for start in rand.choice(instance_len - size + 1, size=n_samples, replace=False): yield get_inactive(range(start, start + size)) break else: # used by SHAP for size in range(min_changes, max_changes + 1): n_choose_k = math.comb(instance_len, size) if n_choose_k <= n_samples: # make all combinations of length size n_samples -= n_choose_k for disable in itertools.combinations(range(instance_len), size): yield get_inactive(disable) else: # fill up remainder with random samples of length size for _ in range(n_samples): yield get_inactive(rand.choice(instance_len, size, replace=False)) break else: sample = rand.randint(min_changes, max_changes + 1, n_samples) for size in sample: if contiguous: # use n-grams start = rand.choice(max_changes - size + 1, replace=False) inactive = TokenReplacement.binary_inactive(range(start, start + size), instance_len) # used by LIME, # https://github.com/marcotcr/lime/blob/a2c7a6fb70bce2e089cb146a31f483bf218875eb/lime/lime_text.py#L436 else: inactive = TokenReplacement.binary_inactive(rand.choice(instance_len, size, replace=False), instance_len) yield self._replace(tokenized_instance, inactive), inactive if add_background_instance: inactive = np.zeros(instance_len) yield self._replace(tokenized_instance, inactive), inactive class LeaveOut(TokenReplacement): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer, seed: int = 0): \"\"\"Leave tokens out of the tokenized sequence. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__(env=env, detokenizer=detokenizer, replacement=None, seed=seed)","title":"Module text_explainability.data.augmentation"},{"location":"reference/text_explainability/data/augmentation/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/data/augmentation/#leaveout","text":"1 2 3 4 5 class LeaveOut ( env : Optional [ instancelib . environment . base . AbstractEnvironment [ instancelib . instances . text . TextInstance , Any , Any , Any , Any , Any ]] = None , detokenizer : Optional [ Callable [[ Iterable [ str ]], str ]] = < function word_detokenizer at 0x16eaef670 > , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class LeaveOut(TokenReplacement): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer, seed: int = 0): \"\"\"Leave tokens out of the tokenized sequence. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__(env=env, detokenizer=detokenizer, replacement=None, seed=seed)","title":"LeaveOut"},{"location":"reference/text_explainability/data/augmentation/#ancestors-in-mro","text":"text_explainability.data.augmentation.TokenReplacement text_explainability.data.augmentation.LocalTokenPertubator instancelib.pertubations.base.MultiplePertubator instancelib.pertubations.base.ChildGenerator abc.ABC typing.Generic genbase.Readable genbase.mixin.SeedMixin","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/augmentation/#static-methods","text":"","title":"Static methods"},{"location":"reference/text_explainability/data/augmentation/#binary_inactive","text":"1 2 3 4 def binary_inactive ( inactive , length ) -> numpy . ndarray View Source 1 2 3 4 5 6 7 8 9 10 11 @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res","title":"binary_inactive"},{"location":"reference/text_explainability/data/augmentation/#instance-variables","text":"1 seed","title":"Instance variables"},{"location":"reference/text_explainability/data/augmentation/#methods","text":"","title":"Methods"},{"location":"reference/text_explainability/data/augmentation/#discard_children","text":"1 2 3 4 def discard_children ( self , parent : instancelib . instances . text . TextInstance ) -> None Discard the generated children for a given parent. View Source 1 2 3 4 5 def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent)","title":"discard_children"},{"location":"reference/text_explainability/data/augmentation/#get_children","text":"1 2 3 4 def get_children ( self , parent : instancelib . instances . text . TextInstance ) -> instancelib . instances . base . InstanceProvider [ instancelib . instances . text . TextInstance , typing . Any , typing . Any , typing . Any , typing . Any ] Get the children of a given parent. View Source 1 2 3 4 5 def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent)","title":"get_children"},{"location":"reference/text_explainability/data/augmentation/#perturb","text":"1 2 3 4 5 6 7 8 9 10 def perturb ( self , tokenized_instance : Iterable [ str ], n_samples : int = 50 , sequential : bool = True , contiguous : bool = False , min_changes : int = 1 , max_changes : int = 10000 , add_background_instance : bool = False ) -> Iterator [ Tuple [ Iterable [ str ], Iterable [ int ]]] Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Parameters: Name Type Description Default tokenized_instance Iterable[str] Tokenized instance to apply perturbations to. None n_samples int Number of samples to return. Defaults to 50. 50 sequential bool Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. None contiguous bool Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. False min_changes int Minimum number of tokens changes (1+). Defaults to 1. 1 max_changes int Maximum number of tokens changed. Defaults to 10000. 10000 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. None Yields: Type Description Iterator[Sequence[Iterable[str], Iterable[int]]] Pertubed text instances and indices where perturbation were applied. Raises: Type Description ValueError min_changes cannot be greater than max_changes. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def perturb(self, tokenized_instance: Iterable[str], n_samples: int = 50, sequential: bool = True, contiguous: bool = False, min_changes: int = 1, max_changes: int = 10000, add_background_instance: bool = False) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: \"\"\"Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Example: Randomly replace at least two tokens with the replacement word 'UNK': >>> from text_explainability.augmentation import TokenReplacement >>> TokenReplacement(replacement='UNK').perturb(['perturb', 'this', 'into', 'multiple'], >>> n_samples=3, >>> min_changes=2) Args: tokenized_instance (Iterable[str]): Tokenized instance to apply perturbations to. n_samples (int, optional): Number of samples to return. Defaults to 50. sequential (bool, optional): Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. contiguous (bool, optional): Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. min_changes (int, optional): Minimum number of tokens changes (1+). Defaults to 1. max_changes (int, optional): Maximum number of tokens changed. Defaults to 10000. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. Raises: ValueError: min_changes cannot be greater than max_changes. Yields: Iterator[Sequence[Iterable[str], Iterable[int]]]: Pertubed text instances and indices where perturbation were applied. \"\"\" instance_len = sum(1 for _ in tokenized_instance) min_changes = min(max(min_changes, 1), instance_len) max_changes = min(instance_len, max_changes) rand = np.random.RandomState(self.seed) def get_inactive(inactive_range): inactive = TokenReplacement.binary_inactive(inactive_range, instance_len) return self._replace(tokenized_instance, inactive), inactive if sequential: if contiguous: # n-grams of length size, up to n_samples for size in range(min_changes, max_changes + 1): n_contiguous = instance_len - size if n_contiguous <= n_samples: n_samples -= n_contiguous for start in range(instance_len - size + 1): yield get_inactive(range(start, start + size)) else: for start in rand.choice(instance_len - size + 1, size=n_samples, replace=False): yield get_inactive(range(start, start + size)) break else: # used by SHAP for size in range(min_changes, max_changes + 1): n_choose_k = math.comb(instance_len, size) if n_choose_k <= n_samples: # make all combinations of length size n_samples -= n_choose_k for disable in itertools.combinations(range(instance_len), size): yield get_inactive(disable) else: # fill up remainder with random samples of length size for _ in range(n_samples): yield get_inactive(rand.choice(instance_len, size, replace=False)) break else: sample = rand.randint(min_changes, max_changes + 1, n_samples) for size in sample: if contiguous: # use n-grams start = rand.choice(max_changes - size + 1, replace=False) inactive = TokenReplacement.binary_inactive(range(start, start + size), instance_len) # used by LIME, # https://github.com/marcotcr/lime/blob/a2c7a6fb70bce2e089cb146a31f483bf218875eb/lime/lime_text.py#L436 else: inactive = TokenReplacement.binary_inactive(rand.choice(instance_len, size, replace=False), instance_len) yield self._replace(tokenized_instance, inactive), inactive if add_background_instance: inactive = np.zeros(instance_len) yield self._replace(tokenized_instance, inactive), inactive","title":"perturb"},{"location":"reference/text_explainability/data/augmentation/#register_child","text":"1 2 3 4 5 def register_child ( self , parent : ~ IT , child : ~ IT ) -> None View Source 1 2 3 def register_child(self, parent: IT, child: IT) -> None: self.env.all_instances.add_child(parent, child)","title":"register_child"},{"location":"reference/text_explainability/data/augmentation/#reset_seed","text":"1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self","title":"reset_seed"},{"location":"reference/text_explainability/data/augmentation/#set_seed","text":"1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed()","title":"set_seed"},{"location":"reference/text_explainability/data/augmentation/#localtokenpertubator","text":"1 2 3 4 class LocalTokenPertubator ( env : Optional [ instancelib . environment . base . AbstractEnvironment [ instancelib . instances . text . TextInstance , Any , Any , Any , Any , Any ]] = None , detokenizer : Optional [ Callable [[ Iterable [ str ]], str ]] = < function word_detokenizer at 0x16eaef670 > ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 class LocalTokenPertubator(MultiplePertubator[TextInstance], ChildGenerator[TextInstance], Readable): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer): \"\"\"Perturb a single instance into neighborhood samples. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. \"\"\" super().__init__() if env is None: env = TextEnvironment.from_data([], [], [], [], []) self.env = env self.detokenizer = detokenizer @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res def perturb(self, tokenized_instance: Iterable[str], *args: Any, **kwargs: Any) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: raise NotImplementedError('Implemented in subclasses') def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent) def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent) @text_instance(tokenize=True) def __call__(self, instance: TextInstance, discard_children: bool = True, *args, **kwargs) -> Iterator[TextInstance]: \"\"\"Apply perturbations to an instance to generate neighborhood data. Args: instance (TextInstance): Tokenized instance to perturb. discard_children (bool, optional): Remove children from previous passes. Defaults to True. *args: Arguments to be passed on to `perturb()` function. **kwargs: Keyword arguments to be passed on to `perturb()` function. Yields: Iterator[Sequence[TextInstance]]: Neighborhood data instances. \"\"\" if instance.data not in self.env.all_instances.all_data(): provider = self.env.create_empty_provider() provider.add(instance) if discard_children: self.discard_children(instance) for new_tokenized, map_to_original in self.perturb(instance.tokenized, *args, **kwargs): new_data = self.detokenizer(new_tokenized) new_instance = self.env.create( data=new_data, vector=None, map_to_original=map_to_original, representation=new_data, tokenized=new_tokenized ) self.register_child(instance, new_instance) return self.get_children(instance)","title":"LocalTokenPertubator"},{"location":"reference/text_explainability/data/augmentation/#ancestors-in-mro_1","text":"instancelib.pertubations.base.MultiplePertubator instancelib.pertubations.base.ChildGenerator abc.ABC typing.Generic genbase.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/augmentation/#descendants","text":"text_explainability.data.augmentation.TokenReplacement","title":"Descendants"},{"location":"reference/text_explainability/data/augmentation/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/text_explainability/data/augmentation/#binary_inactive_1","text":"1 2 3 4 def binary_inactive ( inactive , length ) -> numpy . ndarray View Source 1 2 3 4 5 6 7 8 9 10 11 @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res","title":"binary_inactive"},{"location":"reference/text_explainability/data/augmentation/#methods_1","text":"","title":"Methods"},{"location":"reference/text_explainability/data/augmentation/#discard_children_1","text":"1 2 3 4 def discard_children ( self , parent : instancelib . instances . text . TextInstance ) -> None Discard the generated children for a given parent. View Source 1 2 3 4 5 def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent)","title":"discard_children"},{"location":"reference/text_explainability/data/augmentation/#get_children_1","text":"1 2 3 4 def get_children ( self , parent : instancelib . instances . text . TextInstance ) -> instancelib . instances . base . InstanceProvider [ instancelib . instances . text . TextInstance , typing . Any , typing . Any , typing . Any , typing . Any ] Get the children of a given parent. View Source 1 2 3 4 5 def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent)","title":"get_children"},{"location":"reference/text_explainability/data/augmentation/#perturb_1","text":"1 2 3 4 5 6 def perturb ( self , tokenized_instance : Iterable [ str ], * args : Any , ** kwargs : Any ) -> Iterator [ Tuple [ Iterable [ str ], Iterable [ int ]]] View Source 1 2 3 4 5 def perturb(self, tokenized_instance: Iterable[str], *args: Any, **kwargs: Any) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: raise NotImplementedError('Implemented in subclasses')","title":"perturb"},{"location":"reference/text_explainability/data/augmentation/#register_child_1","text":"1 2 3 4 5 def register_child ( self , parent : ~ IT , child : ~ IT ) -> None View Source 1 2 3 def register_child(self, parent: IT, child: IT) -> None: self.env.all_instances.add_child(parent, child)","title":"register_child"},{"location":"reference/text_explainability/data/augmentation/#tokenreplacement","text":"1 2 3 4 5 6 class TokenReplacement ( env : Optional [ instancelib . environment . base . AbstractEnvironment [ instancelib . instances . text . TextInstance , Any , Any , Any , Any , Any ]] = None , detokenizer : Optional [ Callable [[ Iterable [ str ]], str ]] = < function word_detokenizer at 0x16eaef670 > , replacement : Union [ str , List [ str ], NoneType ] = 'UNKWRDZ' , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 class TokenReplacement(LocalTokenPertubator, SeedMixin): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer, replacement: Optional[Union[str, List[str]]] = 'UNKWRDZ', seed: int = 0): \"\"\"Perturb a tokenized instance by replacing with a set token (e.g. 'UNKWRDZ') or deleting it. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. replacement (Optional[Union[str, List[str]]], optional): Replacement string, or set to None if you want to delete the word entirely. Defaults to 'UNKWRDZ'. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__(env=env, detokenizer=detokenizer) self.replacement = replacement self._seed = self._original_seed = seed def _replace(self, tokenized_instance: Iterable[str], keep: Iterable[int]) -> Iterable[str]: \"\"\"Apply replacement/deletion to tokenized instance. Args: tokenized_instance (Iterable[str]): Tokenized instance. keep (Iterable[int]): Binary indicator whether to keep (1) or replace (0) a token. Raises: ValueError: Too few replacements in self.replacement. Returns: Iterable[str]: Tokenized instance with perturbation applied. \"\"\" if not self.replacement or self.replacement is None: return [token for token, i in zip(tokenized_instance, keep) if i == 1] if isinstance(self.replacement, list): instance_len = sum(1 for _ in tokenized_instance) replacement_len = len(self.replacement) if not (replacement_len >= instance_len): raise ValueError(f'Too few replacements in `self.replacement`, got {replacement_len} ', f'and expected {instance_len}') return [self.replacement[i] if j == 0 else token for i, (token, j) in enumerate(zip(tokenized_instance, keep))] return [self.replacement if i == 0 else token for token, i in zip(tokenized_instance, keep)] def perturb(self, tokenized_instance: Iterable[str], n_samples: int = 50, sequential: bool = True, contiguous: bool = False, min_changes: int = 1, max_changes: int = 10000, add_background_instance: bool = False) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: \"\"\"Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Example: Randomly replace at least two tokens with the replacement word 'UNK': >>> from text_explainability.augmentation import TokenReplacement >>> TokenReplacement(replacement='UNK').perturb(['perturb', 'this', 'into', 'multiple'], >>> n_samples=3, >>> min_changes=2) Args: tokenized_instance (Iterable[str]): Tokenized instance to apply perturbations to. n_samples (int, optional): Number of samples to return. Defaults to 50. sequential (bool, optional): Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. contiguous (bool, optional): Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. min_changes (int, optional): Minimum number of tokens changes (1+). Defaults to 1. max_changes (int, optional): Maximum number of tokens changed. Defaults to 10000. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. Raises: ValueError: min_changes cannot be greater than max_changes. Yields: Iterator[Sequence[Iterable[str], Iterable[int]]]: Pertubed text instances and indices where perturbation were applied. \"\"\" instance_len = sum(1 for _ in tokenized_instance) min_changes = min(max(min_changes, 1), instance_len) max_changes = min(instance_len, max_changes) rand = np.random.RandomState(self.seed) def get_inactive(inactive_range): inactive = TokenReplacement.binary_inactive(inactive_range, instance_len) return self._replace(tokenized_instance, inactive), inactive if sequential: if contiguous: # n-grams of length size, up to n_samples for size in range(min_changes, max_changes + 1): n_contiguous = instance_len - size if n_contiguous <= n_samples: n_samples -= n_contiguous for start in range(instance_len - size + 1): yield get_inactive(range(start, start + size)) else: for start in rand.choice(instance_len - size + 1, size=n_samples, replace=False): yield get_inactive(range(start, start + size)) break else: # used by SHAP for size in range(min_changes, max_changes + 1): n_choose_k = math.comb(instance_len, size) if n_choose_k <= n_samples: # make all combinations of length size n_samples -= n_choose_k for disable in itertools.combinations(range(instance_len), size): yield get_inactive(disable) else: # fill up remainder with random samples of length size for _ in range(n_samples): yield get_inactive(rand.choice(instance_len, size, replace=False)) break else: sample = rand.randint(min_changes, max_changes + 1, n_samples) for size in sample: if contiguous: # use n-grams start = rand.choice(max_changes - size + 1, replace=False) inactive = TokenReplacement.binary_inactive(range(start, start + size), instance_len) # used by LIME, # https://github.com/marcotcr/lime/blob/a2c7a6fb70bce2e089cb146a31f483bf218875eb/lime/lime_text.py#L436 else: inactive = TokenReplacement.binary_inactive(rand.choice(instance_len, size, replace=False), instance_len) yield self._replace(tokenized_instance, inactive), inactive if add_background_instance: inactive = np.zeros(instance_len) yield self._replace(tokenized_instance, inactive), inactive","title":"TokenReplacement"},{"location":"reference/text_explainability/data/augmentation/#ancestors-in-mro_2","text":"text_explainability.data.augmentation.LocalTokenPertubator instancelib.pertubations.base.MultiplePertubator instancelib.pertubations.base.ChildGenerator abc.ABC typing.Generic genbase.Readable genbase.mixin.SeedMixin","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/augmentation/#descendants_1","text":"text_explainability.data.augmentation.LeaveOut","title":"Descendants"},{"location":"reference/text_explainability/data/augmentation/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/text_explainability/data/augmentation/#binary_inactive_2","text":"1 2 3 4 def binary_inactive ( inactive , length ) -> numpy . ndarray View Source 1 2 3 4 5 6 7 8 9 10 11 @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res","title":"binary_inactive"},{"location":"reference/text_explainability/data/augmentation/#instance-variables_1","text":"1 seed","title":"Instance variables"},{"location":"reference/text_explainability/data/augmentation/#methods_2","text":"","title":"Methods"},{"location":"reference/text_explainability/data/augmentation/#discard_children_2","text":"1 2 3 4 def discard_children ( self , parent : instancelib . instances . text . TextInstance ) -> None Discard the generated children for a given parent. View Source 1 2 3 4 5 def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent)","title":"discard_children"},{"location":"reference/text_explainability/data/augmentation/#get_children_2","text":"1 2 3 4 def get_children ( self , parent : instancelib . instances . text . TextInstance ) -> instancelib . instances . base . InstanceProvider [ instancelib . instances . text . TextInstance , typing . Any , typing . Any , typing . Any , typing . Any ] Get the children of a given parent. View Source 1 2 3 4 5 def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent)","title":"get_children"},{"location":"reference/text_explainability/data/augmentation/#perturb_2","text":"1 2 3 4 5 6 7 8 9 10 def perturb ( self , tokenized_instance : Iterable [ str ], n_samples : int = 50 , sequential : bool = True , contiguous : bool = False , min_changes : int = 1 , max_changes : int = 10000 , add_background_instance : bool = False ) -> Iterator [ Tuple [ Iterable [ str ], Iterable [ int ]]] Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Parameters: Name Type Description Default tokenized_instance Iterable[str] Tokenized instance to apply perturbations to. None n_samples int Number of samples to return. Defaults to 50. 50 sequential bool Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. None contiguous bool Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. False min_changes int Minimum number of tokens changes (1+). Defaults to 1. 1 max_changes int Maximum number of tokens changed. Defaults to 10000. 10000 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. None Yields: Type Description Iterator[Sequence[Iterable[str], Iterable[int]]] Pertubed text instances and indices where perturbation were applied. Raises: Type Description ValueError min_changes cannot be greater than max_changes. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def perturb(self, tokenized_instance: Iterable[str], n_samples: int = 50, sequential: bool = True, contiguous: bool = False, min_changes: int = 1, max_changes: int = 10000, add_background_instance: bool = False) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: \"\"\"Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Example: Randomly replace at least two tokens with the replacement word 'UNK': >>> from text_explainability.augmentation import TokenReplacement >>> TokenReplacement(replacement='UNK').perturb(['perturb', 'this', 'into', 'multiple'], >>> n_samples=3, >>> min_changes=2) Args: tokenized_instance (Iterable[str]): Tokenized instance to apply perturbations to. n_samples (int, optional): Number of samples to return. Defaults to 50. sequential (bool, optional): Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. contiguous (bool, optional): Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. min_changes (int, optional): Minimum number of tokens changes (1+). Defaults to 1. max_changes (int, optional): Maximum number of tokens changed. Defaults to 10000. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. Raises: ValueError: min_changes cannot be greater than max_changes. Yields: Iterator[Sequence[Iterable[str], Iterable[int]]]: Pertubed text instances and indices where perturbation were applied. \"\"\" instance_len = sum(1 for _ in tokenized_instance) min_changes = min(max(min_changes, 1), instance_len) max_changes = min(instance_len, max_changes) rand = np.random.RandomState(self.seed) def get_inactive(inactive_range): inactive = TokenReplacement.binary_inactive(inactive_range, instance_len) return self._replace(tokenized_instance, inactive), inactive if sequential: if contiguous: # n-grams of length size, up to n_samples for size in range(min_changes, max_changes + 1): n_contiguous = instance_len - size if n_contiguous <= n_samples: n_samples -= n_contiguous for start in range(instance_len - size + 1): yield get_inactive(range(start, start + size)) else: for start in rand.choice(instance_len - size + 1, size=n_samples, replace=False): yield get_inactive(range(start, start + size)) break else: # used by SHAP for size in range(min_changes, max_changes + 1): n_choose_k = math.comb(instance_len, size) if n_choose_k <= n_samples: # make all combinations of length size n_samples -= n_choose_k for disable in itertools.combinations(range(instance_len), size): yield get_inactive(disable) else: # fill up remainder with random samples of length size for _ in range(n_samples): yield get_inactive(rand.choice(instance_len, size, replace=False)) break else: sample = rand.randint(min_changes, max_changes + 1, n_samples) for size in sample: if contiguous: # use n-grams start = rand.choice(max_changes - size + 1, replace=False) inactive = TokenReplacement.binary_inactive(range(start, start + size), instance_len) # used by LIME, # https://github.com/marcotcr/lime/blob/a2c7a6fb70bce2e089cb146a31f483bf218875eb/lime/lime_text.py#L436 else: inactive = TokenReplacement.binary_inactive(rand.choice(instance_len, size, replace=False), instance_len) yield self._replace(tokenized_instance, inactive), inactive if add_background_instance: inactive = np.zeros(instance_len) yield self._replace(tokenized_instance, inactive), inactive","title":"perturb"},{"location":"reference/text_explainability/data/augmentation/#register_child_2","text":"1 2 3 4 5 def register_child ( self , parent : ~ IT , child : ~ IT ) -> None View Source 1 2 3 def register_child(self, parent: IT, child: IT) -> None: self.env.all_instances.add_child(parent, child)","title":"register_child"},{"location":"reference/text_explainability/data/augmentation/#reset_seed_1","text":"1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self","title":"reset_seed"},{"location":"reference/text_explainability/data/augmentation/#set_seed_1","text":"1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed()","title":"set_seed"},{"location":"reference/text_explainability/data/embedding/","text":"Module text_explainability.data.embedding Embed text instances into numerical vectors. Todo: 1 * Add more sentence embedding methods View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 \"\"\"Embed text instances into numerical vectors. Todo: * Add more sentence embedding methods \"\"\" from typing import Callable, Union import numpy as np from genbase import Readable from instancelib.instances.memory import MemoryBucketProvider def as_n_dimensional(vectors: Union[np.ndarray, list, MemoryBucketProvider], n: int = 2, method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors into n dimensions. Args: vectors (Union[np.ndarray, list, MemoryBucketProvider]): Vectors or BucketProvider with vectorized instances. n (int, optional): Number of dimensions (should be low, e.g. 2 or 3). Defaults to 2. method (str, optional): Method used for dimensionality reduction. Choose from ['pca', 'kernel_pca', 'incremental_pca', 'nmf', 'tsne']. Defaults to 'pca'. **kwargs: Optional arguments passed to method constructor. Raises: ValueError: Unknown method selected. Returns: np.ndarray: Vectors summarized in n dimensions. \"\"\" from sklearn.decomposition import NMF, PCA, IncrementalPCA, KernelPCA from sklearn.manifold import TSNE methods = {'pca': PCA, 'kernel_pca': KernelPCA, 'incremental_pca': IncrementalPCA, 'nmf': NMF, 'tsne': TSNE} # Default to `init='pca'` for tsne to ensure stability if method == 'tsne' and 'init' not in kwargs: kwargs['init'] = 'pca' if method not in methods.keys(): raise ValueError(f'Unknown {method=}. Choose from {list(methods.keys())}.') if isinstance(vectors, MemoryBucketProvider): vectors = vectors.bulk_get_vectors(list(vectors))[-1] return methods[method](n_components=n, **kwargs).fit_transform(vectors) def as_2d(vectors: Union[np.ndarray, list, MemoryBucketProvider], method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors in 2 dimensions.\"\"\" return as_n_dimensional(vectors=vectors, n=2, method=method, **kwargs) def as_3d(vectors: Union[np.ndarray, list, MemoryBucketProvider], method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors in 3 dimensions.\"\"\" return as_n_dimensional(vectors=vectors, n=3, method=method, **kwargs) class Embedder(Readable): def __init__(self, model_fn: Callable): \"\"\"Embedding model base class to transform instances into vectors. Args: model_fn (Callable): Model that embeds instances (transforms into vectors). \"\"\" self.model_fn = model_fn def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings def __call__(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Calls the `self.embed()` function.\"\"\" return self.embed(instances) class SentenceTransformer(Embedder): def __init__(self, model_name: str = 'distiluse-base-multilingual-cased-v1', **kwargs): \"\"\"Embed sentences using the `Sentence Transformers`_ package. By default requires and active internet connection, or provide the name of a local `model_name`. Args: model_name (str, optional): Name of Sentence Transformer model. See https://www.sbert.net/docs/pretrained_models.html for model names. Defaults to 'distiluse-base-multilingual-cased-v1'. **kwargs: Optional arguments to be passed to `SentenceTransformer.encode()` function. See https://www.sbert.net/examples/applications/computing-embeddings/README.html .. _Sentence Transformers: https://github.com/UKPLab/sentence-transformers \"\"\" from sentence_transformers import \\ SentenceTransformer as SentTransformer self.model = SentTransformer(model_name) super().__init__(lambda x: self.model.encode(x, **kwargs)) class CountVectorizer(Embedder): def __init__(self, **kwargs): \"\"\"Embed sentences using `sklearn.CountVectorizer`_. Args: **kwargs: Optional arguments passed for `sklearn.CountVectorizer()` construction. .. _sklearn.TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html \"\"\" from sklearn.feature_extraction.text import CountVectorizer as Count self.model = Count(**kwargs) super().__init__(lambda x: self.model.fit_transform(x).toarray()) class TfidfVectorizer(Embedder): def __init__(self, **kwargs): \"\"\"Embed sentences using `sklearn.TfidfVectorizer`_. Args: **kwargs: Optional arguments passed for `sklearn.TfidfVectorizer()` construction. .. _sklearn.TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html \"\"\" from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf self.model = Tfidf(**kwargs) super().__init__(lambda x: self.model.fit_transform(x).toarray()) Functions as_2d 1 2 3 4 5 def as_2d ( vectors : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ], method : str = 'pca' , ** kwargs ) -> numpy . ndarray Summarize vectors in 2 dimensions. View Source 1 2 3 4 5 def as_2d(vectors: Union[np.ndarray, list, MemoryBucketProvider], method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors in 2 dimensions.\"\"\" return as_n_dimensional(vectors=vectors, n=2, method=method, **kwargs) as_3d 1 2 3 4 5 def as_3d ( vectors : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ], method : str = 'pca' , ** kwargs ) -> numpy . ndarray Summarize vectors in 3 dimensions. View Source 1 2 3 4 5 def as_3d(vectors: Union[np.ndarray, list, MemoryBucketProvider], method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors in 3 dimensions.\"\"\" return as_n_dimensional(vectors=vectors, n=3, method=method, **kwargs) as_n_dimensional 1 2 3 4 5 6 def as_n_dimensional ( vectors : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ], n : int = 2 , method : str = 'pca' , ** kwargs ) -> numpy . ndarray Summarize vectors into n dimensions. Parameters: Name Type Description Default vectors Union[np.ndarray, list, MemoryBucketProvider] Vectors or BucketProvider with vectorized instances. None n int Number of dimensions (should be low, e.g. 2 or 3). Defaults to 2. 2 method str Method used for dimensionality reduction. Choose from ['pca', 'kernel_pca', 'incremental_pca', 'nmf', 'tsne']. Defaults to 'pca'. None **kwargs None Optional arguments passed to method constructor. None Returns: Type Description np.ndarray Vectors summarized in n dimensions. Raises: Type Description ValueError Unknown method selected. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def as_n_dimensional(vectors: Union[np.ndarray, list, MemoryBucketProvider], n: int = 2, method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors into n dimensions. Args: vectors (Union[np.ndarray, list, MemoryBucketProvider]): Vectors or BucketProvider with vectorized instances. n (int, optional): Number of dimensions (should be low, e.g. 2 or 3). Defaults to 2. method (str, optional): Method used for dimensionality reduction. Choose from ['pca', 'kernel_pca', 'incremental_pca', 'nmf', 'tsne']. Defaults to 'pca'. **kwargs: Optional arguments passed to method constructor. Raises: ValueError: Unknown method selected. Returns: np.ndarray: Vectors summarized in n dimensions. \"\"\" from sklearn.decomposition import NMF, PCA, IncrementalPCA, KernelPCA from sklearn.manifold import TSNE methods = {'pca': PCA, 'kernel_pca': KernelPCA, 'incremental_pca': IncrementalPCA, 'nmf': NMF, 'tsne': TSNE} # Default to `init='pca'` for tsne to ensure stability if method == 'tsne' and 'init' not in kwargs: kwargs['init'] = 'pca' if method not in methods.keys(): raise ValueError(f'Unknown {method=}. Choose from {list(methods.keys())}.') if isinstance(vectors, MemoryBucketProvider): vectors = vectors.bulk_get_vectors(list(vectors))[-1] return methods[method](n_components=n, **kwargs).fit_transform(vectors) Classes CountVectorizer 1 2 3 class CountVectorizer ( ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class CountVectorizer(Embedder): def __init__(self, **kwargs): \"\"\"Embed sentences using `sklearn.CountVectorizer`_. Args: **kwargs: Optional arguments passed for `sklearn.CountVectorizer()` construction. .. _sklearn.TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html \"\"\" from sklearn.feature_extraction.text import CountVectorizer as Count self.model = Count(**kwargs) super().__init__(lambda x: self.model.fit_transform(x).toarray()) Ancestors (in MRO) text_explainability.data.embedding.Embedder genbase.Readable Methods embed 1 2 3 4 def embed ( self , instances : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ] ) -> Union [ numpy . ndarray , instancelib . instances . memory . MemoryBucketProvider ] Embed instances (transform into numerical vectors). Parameters: Name Type Description Default instances Union[np.ndarray, list, MemoryBucketProvider] Sequence of instances. None Returns: Type Description Union[np.ndarray, MemoryBucketProvider] Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings Embedder 1 2 3 class Embedder ( model_fn : Callable ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class Embedder(Readable): def __init__(self, model_fn: Callable): \"\"\"Embedding model base class to transform instances into vectors. Args: model_fn (Callable): Model that embeds instances (transforms into vectors). \"\"\" self.model_fn = model_fn def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings def __call__(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Calls the `self.embed()` function.\"\"\" return self.embed(instances) Ancestors (in MRO) genbase.Readable Descendants text_explainability.data.embedding.SentenceTransformer text_explainability.data.embedding.CountVectorizer text_explainability.data.embedding.TfidfVectorizer Methods embed 1 2 3 4 def embed ( self , instances : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ] ) -> Union [ numpy . ndarray , instancelib . instances . memory . MemoryBucketProvider ] Embed instances (transform into numerical vectors). Parameters: Name Type Description Default instances Union[np.ndarray, list, MemoryBucketProvider] Sequence of instances. None Returns: Type Description Union[np.ndarray, MemoryBucketProvider] Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings SentenceTransformer 1 2 3 4 class SentenceTransformer ( model_name : str = 'distiluse-base-multilingual-cased-v1' , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class SentenceTransformer(Embedder): def __init__(self, model_name: str = 'distiluse-base-multilingual-cased-v1', **kwargs): \"\"\"Embed sentences using the `Sentence Transformers`_ package. By default requires and active internet connection, or provide the name of a local `model_name`. Args: model_name (str, optional): Name of Sentence Transformer model. See https://www.sbert.net/docs/pretrained_models.html for model names. Defaults to 'distiluse-base-multilingual-cased-v1'. **kwargs: Optional arguments to be passed to `SentenceTransformer.encode()` function. See https://www.sbert.net/examples/applications/computing-embeddings/README.html .. _Sentence Transformers: https://github.com/UKPLab/sentence-transformers \"\"\" from sentence_transformers import \\ SentenceTransformer as SentTransformer self.model = SentTransformer(model_name) super().__init__(lambda x: self.model.encode(x, **kwargs)) Ancestors (in MRO) text_explainability.data.embedding.Embedder genbase.Readable Methods embed 1 2 3 4 def embed ( self , instances : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ] ) -> Union [ numpy . ndarray , instancelib . instances . memory . MemoryBucketProvider ] Embed instances (transform into numerical vectors). Parameters: Name Type Description Default instances Union[np.ndarray, list, MemoryBucketProvider] Sequence of instances. None Returns: Type Description Union[np.ndarray, MemoryBucketProvider] Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings TfidfVectorizer 1 2 3 class TfidfVectorizer ( ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class TfidfVectorizer(Embedder): def __init__(self, **kwargs): \"\"\"Embed sentences using `sklearn.TfidfVectorizer`_. Args: **kwargs: Optional arguments passed for `sklearn.TfidfVectorizer()` construction. .. _sklearn.TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html \"\"\" from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf self.model = Tfidf(**kwargs) super().__init__(lambda x: self.model.fit_transform(x).toarray()) Ancestors (in MRO) text_explainability.data.embedding.Embedder genbase.Readable Methods embed 1 2 3 4 def embed ( self , instances : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ] ) -> Union [ numpy . ndarray , instancelib . instances . memory . MemoryBucketProvider ] Embed instances (transform into numerical vectors). Parameters: Name Type Description Default instances Union[np.ndarray, list, MemoryBucketProvider] Sequence of instances. None Returns: Type Description Union[np.ndarray, MemoryBucketProvider] Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings","title":"Embedding"},{"location":"reference/text_explainability/data/embedding/#module-text_explainabilitydataembedding","text":"Embed text instances into numerical vectors. Todo: 1 * Add more sentence embedding methods View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 \"\"\"Embed text instances into numerical vectors. Todo: * Add more sentence embedding methods \"\"\" from typing import Callable, Union import numpy as np from genbase import Readable from instancelib.instances.memory import MemoryBucketProvider def as_n_dimensional(vectors: Union[np.ndarray, list, MemoryBucketProvider], n: int = 2, method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors into n dimensions. Args: vectors (Union[np.ndarray, list, MemoryBucketProvider]): Vectors or BucketProvider with vectorized instances. n (int, optional): Number of dimensions (should be low, e.g. 2 or 3). Defaults to 2. method (str, optional): Method used for dimensionality reduction. Choose from ['pca', 'kernel_pca', 'incremental_pca', 'nmf', 'tsne']. Defaults to 'pca'. **kwargs: Optional arguments passed to method constructor. Raises: ValueError: Unknown method selected. Returns: np.ndarray: Vectors summarized in n dimensions. \"\"\" from sklearn.decomposition import NMF, PCA, IncrementalPCA, KernelPCA from sklearn.manifold import TSNE methods = {'pca': PCA, 'kernel_pca': KernelPCA, 'incremental_pca': IncrementalPCA, 'nmf': NMF, 'tsne': TSNE} # Default to `init='pca'` for tsne to ensure stability if method == 'tsne' and 'init' not in kwargs: kwargs['init'] = 'pca' if method not in methods.keys(): raise ValueError(f'Unknown {method=}. Choose from {list(methods.keys())}.') if isinstance(vectors, MemoryBucketProvider): vectors = vectors.bulk_get_vectors(list(vectors))[-1] return methods[method](n_components=n, **kwargs).fit_transform(vectors) def as_2d(vectors: Union[np.ndarray, list, MemoryBucketProvider], method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors in 2 dimensions.\"\"\" return as_n_dimensional(vectors=vectors, n=2, method=method, **kwargs) def as_3d(vectors: Union[np.ndarray, list, MemoryBucketProvider], method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors in 3 dimensions.\"\"\" return as_n_dimensional(vectors=vectors, n=3, method=method, **kwargs) class Embedder(Readable): def __init__(self, model_fn: Callable): \"\"\"Embedding model base class to transform instances into vectors. Args: model_fn (Callable): Model that embeds instances (transforms into vectors). \"\"\" self.model_fn = model_fn def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings def __call__(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Calls the `self.embed()` function.\"\"\" return self.embed(instances) class SentenceTransformer(Embedder): def __init__(self, model_name: str = 'distiluse-base-multilingual-cased-v1', **kwargs): \"\"\"Embed sentences using the `Sentence Transformers`_ package. By default requires and active internet connection, or provide the name of a local `model_name`. Args: model_name (str, optional): Name of Sentence Transformer model. See https://www.sbert.net/docs/pretrained_models.html for model names. Defaults to 'distiluse-base-multilingual-cased-v1'. **kwargs: Optional arguments to be passed to `SentenceTransformer.encode()` function. See https://www.sbert.net/examples/applications/computing-embeddings/README.html .. _Sentence Transformers: https://github.com/UKPLab/sentence-transformers \"\"\" from sentence_transformers import \\ SentenceTransformer as SentTransformer self.model = SentTransformer(model_name) super().__init__(lambda x: self.model.encode(x, **kwargs)) class CountVectorizer(Embedder): def __init__(self, **kwargs): \"\"\"Embed sentences using `sklearn.CountVectorizer`_. Args: **kwargs: Optional arguments passed for `sklearn.CountVectorizer()` construction. .. _sklearn.TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html \"\"\" from sklearn.feature_extraction.text import CountVectorizer as Count self.model = Count(**kwargs) super().__init__(lambda x: self.model.fit_transform(x).toarray()) class TfidfVectorizer(Embedder): def __init__(self, **kwargs): \"\"\"Embed sentences using `sklearn.TfidfVectorizer`_. Args: **kwargs: Optional arguments passed for `sklearn.TfidfVectorizer()` construction. .. _sklearn.TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html \"\"\" from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf self.model = Tfidf(**kwargs) super().__init__(lambda x: self.model.fit_transform(x).toarray())","title":"Module text_explainability.data.embedding"},{"location":"reference/text_explainability/data/embedding/#functions","text":"","title":"Functions"},{"location":"reference/text_explainability/data/embedding/#as_2d","text":"1 2 3 4 5 def as_2d ( vectors : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ], method : str = 'pca' , ** kwargs ) -> numpy . ndarray Summarize vectors in 2 dimensions. View Source 1 2 3 4 5 def as_2d(vectors: Union[np.ndarray, list, MemoryBucketProvider], method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors in 2 dimensions.\"\"\" return as_n_dimensional(vectors=vectors, n=2, method=method, **kwargs)","title":"as_2d"},{"location":"reference/text_explainability/data/embedding/#as_3d","text":"1 2 3 4 5 def as_3d ( vectors : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ], method : str = 'pca' , ** kwargs ) -> numpy . ndarray Summarize vectors in 3 dimensions. View Source 1 2 3 4 5 def as_3d(vectors: Union[np.ndarray, list, MemoryBucketProvider], method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors in 3 dimensions.\"\"\" return as_n_dimensional(vectors=vectors, n=3, method=method, **kwargs)","title":"as_3d"},{"location":"reference/text_explainability/data/embedding/#as_n_dimensional","text":"1 2 3 4 5 6 def as_n_dimensional ( vectors : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ], n : int = 2 , method : str = 'pca' , ** kwargs ) -> numpy . ndarray Summarize vectors into n dimensions. Parameters: Name Type Description Default vectors Union[np.ndarray, list, MemoryBucketProvider] Vectors or BucketProvider with vectorized instances. None n int Number of dimensions (should be low, e.g. 2 or 3). Defaults to 2. 2 method str Method used for dimensionality reduction. Choose from ['pca', 'kernel_pca', 'incremental_pca', 'nmf', 'tsne']. Defaults to 'pca'. None **kwargs None Optional arguments passed to method constructor. None Returns: Type Description np.ndarray Vectors summarized in n dimensions. Raises: Type Description ValueError Unknown method selected. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def as_n_dimensional(vectors: Union[np.ndarray, list, MemoryBucketProvider], n: int = 2, method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors into n dimensions. Args: vectors (Union[np.ndarray, list, MemoryBucketProvider]): Vectors or BucketProvider with vectorized instances. n (int, optional): Number of dimensions (should be low, e.g. 2 or 3). Defaults to 2. method (str, optional): Method used for dimensionality reduction. Choose from ['pca', 'kernel_pca', 'incremental_pca', 'nmf', 'tsne']. Defaults to 'pca'. **kwargs: Optional arguments passed to method constructor. Raises: ValueError: Unknown method selected. Returns: np.ndarray: Vectors summarized in n dimensions. \"\"\" from sklearn.decomposition import NMF, PCA, IncrementalPCA, KernelPCA from sklearn.manifold import TSNE methods = {'pca': PCA, 'kernel_pca': KernelPCA, 'incremental_pca': IncrementalPCA, 'nmf': NMF, 'tsne': TSNE} # Default to `init='pca'` for tsne to ensure stability if method == 'tsne' and 'init' not in kwargs: kwargs['init'] = 'pca' if method not in methods.keys(): raise ValueError(f'Unknown {method=}. Choose from {list(methods.keys())}.') if isinstance(vectors, MemoryBucketProvider): vectors = vectors.bulk_get_vectors(list(vectors))[-1] return methods[method](n_components=n, **kwargs).fit_transform(vectors)","title":"as_n_dimensional"},{"location":"reference/text_explainability/data/embedding/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/data/embedding/#countvectorizer","text":"1 2 3 class CountVectorizer ( ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class CountVectorizer(Embedder): def __init__(self, **kwargs): \"\"\"Embed sentences using `sklearn.CountVectorizer`_. Args: **kwargs: Optional arguments passed for `sklearn.CountVectorizer()` construction. .. _sklearn.TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html \"\"\" from sklearn.feature_extraction.text import CountVectorizer as Count self.model = Count(**kwargs) super().__init__(lambda x: self.model.fit_transform(x).toarray())","title":"CountVectorizer"},{"location":"reference/text_explainability/data/embedding/#ancestors-in-mro","text":"text_explainability.data.embedding.Embedder genbase.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/embedding/#methods","text":"","title":"Methods"},{"location":"reference/text_explainability/data/embedding/#embed","text":"1 2 3 4 def embed ( self , instances : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ] ) -> Union [ numpy . ndarray , instancelib . instances . memory . MemoryBucketProvider ] Embed instances (transform into numerical vectors). Parameters: Name Type Description Default instances Union[np.ndarray, list, MemoryBucketProvider] Sequence of instances. None Returns: Type Description Union[np.ndarray, MemoryBucketProvider] Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings","title":"embed"},{"location":"reference/text_explainability/data/embedding/#embedder","text":"1 2 3 class Embedder ( model_fn : Callable ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class Embedder(Readable): def __init__(self, model_fn: Callable): \"\"\"Embedding model base class to transform instances into vectors. Args: model_fn (Callable): Model that embeds instances (transforms into vectors). \"\"\" self.model_fn = model_fn def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings def __call__(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Calls the `self.embed()` function.\"\"\" return self.embed(instances)","title":"Embedder"},{"location":"reference/text_explainability/data/embedding/#ancestors-in-mro_1","text":"genbase.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/embedding/#descendants","text":"text_explainability.data.embedding.SentenceTransformer text_explainability.data.embedding.CountVectorizer text_explainability.data.embedding.TfidfVectorizer","title":"Descendants"},{"location":"reference/text_explainability/data/embedding/#methods_1","text":"","title":"Methods"},{"location":"reference/text_explainability/data/embedding/#embed_1","text":"1 2 3 4 def embed ( self , instances : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ] ) -> Union [ numpy . ndarray , instancelib . instances . memory . MemoryBucketProvider ] Embed instances (transform into numerical vectors). Parameters: Name Type Description Default instances Union[np.ndarray, list, MemoryBucketProvider] Sequence of instances. None Returns: Type Description Union[np.ndarray, MemoryBucketProvider] Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings","title":"embed"},{"location":"reference/text_explainability/data/embedding/#sentencetransformer","text":"1 2 3 4 class SentenceTransformer ( model_name : str = 'distiluse-base-multilingual-cased-v1' , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class SentenceTransformer(Embedder): def __init__(self, model_name: str = 'distiluse-base-multilingual-cased-v1', **kwargs): \"\"\"Embed sentences using the `Sentence Transformers`_ package. By default requires and active internet connection, or provide the name of a local `model_name`. Args: model_name (str, optional): Name of Sentence Transformer model. See https://www.sbert.net/docs/pretrained_models.html for model names. Defaults to 'distiluse-base-multilingual-cased-v1'. **kwargs: Optional arguments to be passed to `SentenceTransformer.encode()` function. See https://www.sbert.net/examples/applications/computing-embeddings/README.html .. _Sentence Transformers: https://github.com/UKPLab/sentence-transformers \"\"\" from sentence_transformers import \\ SentenceTransformer as SentTransformer self.model = SentTransformer(model_name) super().__init__(lambda x: self.model.encode(x, **kwargs))","title":"SentenceTransformer"},{"location":"reference/text_explainability/data/embedding/#ancestors-in-mro_2","text":"text_explainability.data.embedding.Embedder genbase.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/embedding/#methods_2","text":"","title":"Methods"},{"location":"reference/text_explainability/data/embedding/#embed_2","text":"1 2 3 4 def embed ( self , instances : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ] ) -> Union [ numpy . ndarray , instancelib . instances . memory . MemoryBucketProvider ] Embed instances (transform into numerical vectors). Parameters: Name Type Description Default instances Union[np.ndarray, list, MemoryBucketProvider] Sequence of instances. None Returns: Type Description Union[np.ndarray, MemoryBucketProvider] Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings","title":"embed"},{"location":"reference/text_explainability/data/embedding/#tfidfvectorizer","text":"1 2 3 class TfidfVectorizer ( ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class TfidfVectorizer(Embedder): def __init__(self, **kwargs): \"\"\"Embed sentences using `sklearn.TfidfVectorizer`_. Args: **kwargs: Optional arguments passed for `sklearn.TfidfVectorizer()` construction. .. _sklearn.TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html \"\"\" from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf self.model = Tfidf(**kwargs) super().__init__(lambda x: self.model.fit_transform(x).toarray())","title":"TfidfVectorizer"},{"location":"reference/text_explainability/data/embedding/#ancestors-in-mro_3","text":"text_explainability.data.embedding.Embedder genbase.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/embedding/#methods_3","text":"","title":"Methods"},{"location":"reference/text_explainability/data/embedding/#embed_3","text":"1 2 3 4 def embed ( self , instances : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ] ) -> Union [ numpy . ndarray , instancelib . instances . memory . MemoryBucketProvider ] Embed instances (transform into numerical vectors). Parameters: Name Type Description Default instances Union[np.ndarray, list, MemoryBucketProvider] Sequence of instances. None Returns: Type Description Union[np.ndarray, MemoryBucketProvider] Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings","title":"embed"},{"location":"reference/text_explainability/data/sampling/","text":"Module text_explainability.data.sampling Sample an (informative) subset from the data. Todo: 1 2 3 * Sample (informative?) subset from data * Refactor to make sampling base class * Add ability to perform MMD critic on a subset (e.g. single class) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 \"\"\"Sample an (informative) subset from the data. Todo: * Sample (informative?) subset from data * Refactor to make sampling base class * Add ability to perform MMD critic on a subset (e.g. single class) \"\"\" from typing import Callable, Dict, Optional, Sequence, Union import numpy as np from genbase import Readable, SeedMixin from instancelib.instances.memory import DataPoint, MemoryBucketProvider from instancelib.labels.base import LabelProvider from instancelib.labels.memory import MemoryLabelProvider from instancelib.machinelearning.base import AbstractClassifier from .embedding import Embedder, TfidfVectorizer from .weights import exponential_kernel class PrototypeSampler(Readable): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer): \"\"\"Generic class for sampling prototypes (representative samples) based on embedding distances. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. \"\"\" self.embedder = embedder() if isinstance(embedder, type) else embedder self.instances = self.embedder(instances) if any(instances[i].vector is None for i in instances) \\ else instances @property def embedded(self) -> np.ndarray: return np.stack(self.instances.bulk_get_vectors(list(self.instances))[-1]) def _select_from_provider(self, keys: Sequence[int]) -> Sequence[DataPoint]: \"\"\"Select instances from provider by keys.\"\"\" id_map = np.array(self.instances) return [self.instances[id_map[i]] for i in keys] def prototypes(self, n: int = 5) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[DataPoint]: List of prototype instances. \"\"\" raise NotImplementedError('Implemented in subclasses') def __call__(self, *args, **kwargs): return self.prototypes(*args, **kwargs) class KMedoids(PrototypeSampler, SeedMixin): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer, seed: int = 0): \"\"\"Sampling prototypes (representative samples) based on embedding distances using `k-Medoids`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html \"\"\" super().__init__(instances, embedder) self._seed = self._original_seed = seed def prototypes(self, n: int = 5, metric: Union[str, Callable] = 'cosine', **kwargs) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes (most representative samples) using `k-Medoids`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. metrics (Union[str, Callable], optional): Distance metric used to calculate medoids (e.g. 'cosine', 'euclidean' or your own function). See `pairwise distances` for a full list. Defaults to 'cosine'. **kwargs: Optional arguments passed to `k-Medoids`_ constructor. Returns: Sequence[DataPoint]: List of prototype instances. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html .. _pairwise distances: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html \"\"\" from sklearn_extra.cluster import KMedoids kmedoids = KMedoids(n_clusters=n, metric=metric, random_state=self.seed, **kwargs).fit(self.embedded) return self._select_from_provider(kmedoids.medoid_indices_) class MMDCritic(PrototypeSampler): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer, kernel: Callable = exponential_kernel): \"\"\"Select prototypes and criticisms based on embedding distances using `MMD-Critic`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. kernel (Callable, optional): Kernel to calculate distances. Defaults to exponential_kernel. .. _MMD-critic: https://christophm.github.io/interpretable-ml-book/proto.html \"\"\" super().__init__(instances, embedder) self.kernel = kernel self._calculate_kernel() self._prototypes = None self._criticisms = None def _calculate_kernel(self): \"\"\"Calculate kernel `K` and column totals `colsum`.\"\"\" self.K = self.kernel(self.embedded, 1.0 / self.embedded.shape[1]) self.colsum = np.sum(self.K, axis=0) / self.embedded.shape[1] def prototypes(self, n: int = 5) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes (most representatitve instances), using `MMD-critic implementation`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Raises: ValueError: Cannot select more instances than the total number of instances. Returns: Sequence[DataPoint]: List of prototype instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if n > len(self.instances): raise ValueError(f'Cannot select more than all instances ({len(self.instances)}.') K = self.K colsum = self.colsum.copy() * 2 sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] diag = np.diagonal(K)[candidate_indices] if selected.shape[0] == 0: s1 -= np.abs(diag) else: temp = K[selected, :][:, candidate_indices] s2 = np.sum(temp, axis=0) * 2 + diag s2 /= (selected.shape[0] + 1) s1 -= s2 best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[is_selected > 0] selected_in_order = selected[is_selected[is_selected > 0].argsort()] self._prototypes = self._select_from_provider(selected_in_order) return self._prototypes def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Sequence[DataPoint]: \"\"\"Select `n` criticisms (instances not well represented by prototypes), using `MMD-critic implementation`_. Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. ValueError: Unknown regularizer or requested more criticisms than there are samples left. Returns: Sequence[DataPoint]: List of criticism instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if self._prototypes is None: raise Exception('Calculating criticisms requires prototypes. Run `MMDCritic.prototypes()` first.') regularizers = {None, 'logdet', 'iterative'} if regularizer not in regularizers: raise ValueError(f'Unknown {regularizer=}. Choose from {regularizers}.') if n > (len(self.instances) - len(self._prototypes)): raise ValueError('Cannot select more than instances excluding prototypes ', f'({len(self.instances) - len(self._prototypes)})') id_map = {instance: id for id, instance in enumerate(self.instances)} prototypes = np.array([id_map[p.identifier] for p in self._prototypes]) K = self.K colsum = self.colsum sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] is_selected[prototypes] = n + 1 inverse_of_prev_selected = None for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] temp = K[prototypes, :][:, candidate_indices] s2 = np.sum(temp, axis=0) s2 /= prototypes.shape[0] s1 -= s2 s1 = np.abs(s1) if regularizer == 'logdet': diag = np.diagonal(K + 1)[candidate_indices] if inverse_of_prev_selected is not None: temp = K[selected, :][:, candidate_indices] temp2 = np.dot(inverse_of_prev_selected, temp) reg = temp2 * temp regcolsum = np.sum(reg, axis=0) with np.errstate(divide='ignore'): reg = np.log(np.abs(diag - regcolsum)) s1 += reg else: with np.errstate(divide='ignore'): s1 -= np.log(np.abs(diag)) best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[(is_selected > 0) & (is_selected != (n + 1))] if regularizer == 'iterative': prototypes = np.concatenate([prototypes, np.expand_dims(best_sample_index, 0)]) if regularizer == 'logdet': inverse_of_prev_selected = np.linalg.pinv(K[selected, :][:, selected]) selected_in_order = selected[is_selected[(is_selected > 0) & (is_selected != (n + 1))].argsort()] self._criticisms = self._select_from_provider(selected_in_order) return self._criticisms def __call__(self, n_prototypes: int = 5, n_criticisms: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[DataPoint]]: \"\"\"Calculate prototypes and criticisms for the provided instances. Args: n_prototypes (int, optional): Number of prototypes. Defaults to 5. n_criticisms (int, optional): Number of criticisms. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Returns: Dict[str, Sequence[DataPoint]]: Dictionary containing prototypes and criticisms. \"\"\" return {'prototypes': self.prototypes(n=n_prototypes), 'criticisms': self.criticisms(n=n_criticisms, regularizer=regularizer)} class LabelwisePrototypeSampler(Readable): def __init__(self, sampler: PrototypeSampler, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier], embedder: Embedder = TfidfVectorizer, **kwargs): \"\"\"Apply `PrototypeSampler()` for each label. Args: sampler (PrototypeSampler): Prototype sampler to construct (e.g. `KMedoids`, `MMDCritic`) instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. **kwargs: Additional arguments passed to `_setup_instances()` constructor. \"\"\" self.sampler = sampler if isinstance(sampler, type) else self.sampler.__class__ self.instances = instances self._get_labels(labels) self._setup_samplers(embedder, **kwargs) def _get_labels(self, labels: Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier]): \"\"\"Transform the labels into a `LabelProvider`.\"\"\" if not isinstance(labels, LabelProvider): if isinstance(labels, AbstractClassifier): labels_ = labels.predict(self.instances) else: labels_ = [(id, frozenset({label})) for id, label in zip(list(self.instances), labels)] labels = MemoryLabelProvider.from_tuples(labels_) self.labels = labels def _setup_samplers(self, embedder: Embedder, **kwargs): \"\"\"Setup a sampler for each label in `self.labels.labelset`. Args: embedder (Embedder): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. **kwargs: Additional arguments passed to sampler constructor. \"\"\" import copy def select_by_label(label): instances = copy.deepcopy(self.instances) keys_to_keep = self.labels.get_instances_by_label(label) instances._remove_from_bucket(frozenset(list(instances)).difference(keys_to_keep)) return instances self._samplers = {label: self.sampler(instances=select_by_label(label), embedder=embedder, **kwargs) for label in self.labels.labelset} self.samplers = self._samplers def prototypes(self, n: int = 5) -> Dict[str, Sequence[DataPoint]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[DataPoint]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()} def __call__(self, n: int = 5) -> Dict[str, Dict[str, Sequence[DataPoint]]]: \"\"\"Generate prototypes for each label. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Dict[str, Sequence[DataPoint]]]: Dictionary with labels and corresponding dictionary containing prototypes. \"\"\" return {label: {'prototypes': sampler.prototypes(n=n)} for label, sampler in self._samplers.items()} class LabelwiseKMedoids(LabelwisePrototypeSampler): def __init__(self, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider], embedder: Embedder = TfidfVectorizer, seed: int = 0): \"\"\"Select prototypes for each label based on embedding distances using `k-Medoids`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html \"\"\" super().__init__(KMedoids, instances=instances, labels=labels, embedder=embedder, seed=seed) class LabelwiseMMDCritic(LabelwisePrototypeSampler): def __init__(self, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider], embedder: Embedder = TfidfVectorizer, kernel: Callable = exponential_kernel): \"\"\"Select prototypes and criticisms for each label based on embedding distances using `MMD-Critic`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. kernel (Callable, optional): Kernel to calculate distances. Defaults to exponential_kernel. .. _MMD-critic: https://christophm.github.io/interpretable-ml-book/proto.html \"\"\" super().__init__(MMDCritic, instances=instances, labels=labels, embedder=embedder, kernel=kernel) def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[DataPoint]]: \"\"\"Select `n` criticisms (instances not well represented by prototypes). Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Dict[str, Sequence[DataPoint]]: Dictionary with labels and corresponding list of criticisms. \"\"\" return {label: sampler.criticisms(n=n, regularizer=regularizer) for label, sampler in self._samplers.items()} def __call__(self, n_prototypes: int = 5, n_criticisms: int = 5, regularizer: Optional[str] = None) -> Dict[str, Dict[str, Sequence[DataPoint]]]: \"\"\"Generate prototypes and criticisms for each label. Args: n_prototypes (int, optional): Number of prototypes to select. Defaults to 5. n_criticisms (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Returns: Dict[str, Dict[str, Sequence[DataPoint]]]: Dictionary with labels and corresponding dictionary containing prototypes and criticisms. \"\"\" return {label: sampler(n_prototypes=n_prototypes, n_criticisms=n_criticisms, regularizer=regularizer) for label, sampler in self._samplers.items()} Classes KMedoids 1 2 3 4 5 class KMedoids ( instances : instancelib . instances . memory . MemoryBucketProvider , embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class KMedoids(PrototypeSampler, SeedMixin): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer, seed: int = 0): \"\"\"Sampling prototypes (representative samples) based on embedding distances using `k-Medoids`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html \"\"\" super().__init__(instances, embedder) self._seed = self._original_seed = seed def prototypes(self, n: int = 5, metric: Union[str, Callable] = 'cosine', **kwargs) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes (most representative samples) using `k-Medoids`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. metrics (Union[str, Callable], optional): Distance metric used to calculate medoids (e.g. 'cosine', 'euclidean' or your own function). See `pairwise distances` for a full list. Defaults to 'cosine'. **kwargs: Optional arguments passed to `k-Medoids`_ constructor. Returns: Sequence[DataPoint]: List of prototype instances. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html .. _pairwise distances: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html \"\"\" from sklearn_extra.cluster import KMedoids kmedoids = KMedoids(n_clusters=n, metric=metric, random_state=self.seed, **kwargs).fit(self.embedded) return self._select_from_provider(kmedoids.medoid_indices_) Ancestors (in MRO) text_explainability.data.sampling.PrototypeSampler genbase.Readable genbase.mixin.SeedMixin Instance variables 1 embedded 1 seed Methods prototypes 1 2 3 4 5 6 def prototypes ( self , n : int = 5 , metric : Union [ str , Callable ] = 'cosine' , ** kwargs ) -> Sequence [ instancelib . instances . memory . DataPoint ] Select n prototypes (most representative samples) using k-Medoids _. Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 metrics Union[str, Callable] Distance metric used to calculate medoids (e.g. 'cosine', 'euclidean' or your own function). See pairwise distances for a full list. Defaults to 'cosine'. None **kwargs None Optional arguments passed to k-Medoids _ constructor. None Returns: Type Description Sequence[DataPoint] List of prototype instances. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html .. _pairwise distances: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def prototypes(self, n: int = 5, metric: Union[str, Callable] = 'cosine', **kwargs) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes (most representative samples) using `k-Medoids`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. metrics (Union[str, Callable], optional): Distance metric used to calculate medoids (e.g. 'cosine', 'euclidean' or your own function). See `pairwise distances` for a full list. Defaults to 'cosine'. **kwargs: Optional arguments passed to `k-Medoids`_ constructor. Returns: Sequence[DataPoint]: List of prototype instances. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html .. _pairwise distances: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html \"\"\" from sklearn_extra.cluster import KMedoids kmedoids = KMedoids(n_clusters=n, metric=metric, random_state=self.seed, **kwargs).fit(self.embedded) return self._select_from_provider(kmedoids.medoid_indices_) reset_seed 1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self set_seed 1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed() LabelwiseKMedoids 1 2 3 4 5 6 class LabelwiseKMedoids ( instances : instancelib . instances . memory . MemoryBucketProvider , labels : Union [ Sequence [ str ], Sequence [ int ], instancelib . labels . base . LabelProvider ], embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class LabelwiseKMedoids(LabelwisePrototypeSampler): def __init__(self, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider], embedder: Embedder = TfidfVectorizer, seed: int = 0): \"\"\"Select prototypes for each label based on embedding distances using `k-Medoids`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html \"\"\" super().__init__(KMedoids, instances=instances, labels=labels, embedder=embedder, seed=seed) Ancestors (in MRO) text_explainability.data.sampling.LabelwisePrototypeSampler genbase.Readable Methods prototypes 1 2 3 4 def prototypes ( self , n : int = 5 ) -> Dict [ str , Sequence [ instancelib . instances . memory . DataPoint ]] Select n prototypes (most representatitve instances). Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Dict[str, Sequence[DataPoint]] Dictionary with labels and corresponding list of prototypes. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def prototypes(self, n: int = 5) -> Dict[str, Sequence[DataPoint]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[DataPoint]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()} LabelwiseMMDCritic 1 2 3 4 5 6 class LabelwiseMMDCritic ( instances : instancelib . instances . memory . MemoryBucketProvider , labels : Union [ Sequence [ str ], Sequence [ int ], instancelib . labels . base . LabelProvider ], embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, kernel : Callable = < function exponential_kernel at 0x16eaf9310 > ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class LabelwiseMMDCritic(LabelwisePrototypeSampler): def __init__(self, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider], embedder: Embedder = TfidfVectorizer, kernel: Callable = exponential_kernel): \"\"\"Select prototypes and criticisms for each label based on embedding distances using `MMD-Critic`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. kernel (Callable, optional): Kernel to calculate distances. Defaults to exponential_kernel. .. _MMD-critic: https://christophm.github.io/interpretable-ml-book/proto.html \"\"\" super().__init__(MMDCritic, instances=instances, labels=labels, embedder=embedder, kernel=kernel) def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[DataPoint]]: \"\"\"Select `n` criticisms (instances not well represented by prototypes). Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Dict[str, Sequence[DataPoint]]: Dictionary with labels and corresponding list of criticisms. \"\"\" return {label: sampler.criticisms(n=n, regularizer=regularizer) for label, sampler in self._samplers.items()} def __call__(self, n_prototypes: int = 5, n_criticisms: int = 5, regularizer: Optional[str] = None) -> Dict[str, Dict[str, Sequence[DataPoint]]]: \"\"\"Generate prototypes and criticisms for each label. Args: n_prototypes (int, optional): Number of prototypes to select. Defaults to 5. n_criticisms (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Returns: Dict[str, Dict[str, Sequence[DataPoint]]]: Dictionary with labels and corresponding dictionary containing prototypes and criticisms. \"\"\" return {label: sampler(n_prototypes=n_prototypes, n_criticisms=n_criticisms, regularizer=regularizer) for label, sampler in self._samplers.items()} Ancestors (in MRO) text_explainability.data.sampling.LabelwisePrototypeSampler genbase.Readable Methods criticisms 1 2 3 4 5 def criticisms ( self , n : int = 5 , regularizer : Optional [ str ] = None ) -> Dict [ str , Sequence [ instancelib . instances . memory . DataPoint ]] Select n criticisms (instances not well represented by prototypes). Parameters: Name Type Description Default n int Number of criticisms to select. Defaults to 5. 5 regularizer Optional[str] Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. None Returns: Type Description Dict[str, Sequence[DataPoint]] Dictionary with labels and corresponding list of criticisms. Raises: Type Description Exception MMDCritic.prototypes() must first be run before being able to determine the criticisms. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[DataPoint]]: \"\"\"Select `n` criticisms (instances not well represented by prototypes). Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Dict[str, Sequence[DataPoint]]: Dictionary with labels and corresponding list of criticisms. \"\"\" return {label: sampler.criticisms(n=n, regularizer=regularizer) for label, sampler in self._samplers.items()} prototypes 1 2 3 4 def prototypes ( self , n : int = 5 ) -> Dict [ str , Sequence [ instancelib . instances . memory . DataPoint ]] Select n prototypes (most representatitve instances). Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Dict[str, Sequence[DataPoint]] Dictionary with labels and corresponding list of prototypes. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def prototypes(self, n: int = 5) -> Dict[str, Sequence[DataPoint]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[DataPoint]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()} LabelwisePrototypeSampler 1 2 3 4 5 6 7 class LabelwisePrototypeSampler ( sampler : text_explainability . data . sampling . PrototypeSampler , instances : instancelib . instances . memory . MemoryBucketProvider , labels : Union [ Sequence [ str ], Sequence [ int ], instancelib . labels . base . LabelProvider , instancelib . machinelearning . base . AbstractClassifier ], embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class LabelwisePrototypeSampler(Readable): def __init__(self, sampler: PrototypeSampler, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier], embedder: Embedder = TfidfVectorizer, **kwargs): \"\"\"Apply `PrototypeSampler()` for each label. Args: sampler (PrototypeSampler): Prototype sampler to construct (e.g. `KMedoids`, `MMDCritic`) instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. **kwargs: Additional arguments passed to `_setup_instances()` constructor. \"\"\" self.sampler = sampler if isinstance(sampler, type) else self.sampler.__class__ self.instances = instances self._get_labels(labels) self._setup_samplers(embedder, **kwargs) def _get_labels(self, labels: Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier]): \"\"\"Transform the labels into a `LabelProvider`.\"\"\" if not isinstance(labels, LabelProvider): if isinstance(labels, AbstractClassifier): labels_ = labels.predict(self.instances) else: labels_ = [(id, frozenset({label})) for id, label in zip(list(self.instances), labels)] labels = MemoryLabelProvider.from_tuples(labels_) self.labels = labels def _setup_samplers(self, embedder: Embedder, **kwargs): \"\"\"Setup a sampler for each label in `self.labels.labelset`. Args: embedder (Embedder): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. **kwargs: Additional arguments passed to sampler constructor. \"\"\" import copy def select_by_label(label): instances = copy.deepcopy(self.instances) keys_to_keep = self.labels.get_instances_by_label(label) instances._remove_from_bucket(frozenset(list(instances)).difference(keys_to_keep)) return instances self._samplers = {label: self.sampler(instances=select_by_label(label), embedder=embedder, **kwargs) for label in self.labels.labelset} self.samplers = self._samplers def prototypes(self, n: int = 5) -> Dict[str, Sequence[DataPoint]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[DataPoint]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()} def __call__(self, n: int = 5) -> Dict[str, Dict[str, Sequence[DataPoint]]]: \"\"\"Generate prototypes for each label. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Dict[str, Sequence[DataPoint]]]: Dictionary with labels and corresponding dictionary containing prototypes. \"\"\" return {label: {'prototypes': sampler.prototypes(n=n)} for label, sampler in self._samplers.items()} Ancestors (in MRO) genbase.Readable Descendants text_explainability.data.sampling.LabelwiseKMedoids text_explainability.data.sampling.LabelwiseMMDCritic Methods prototypes 1 2 3 4 def prototypes ( self , n : int = 5 ) -> Dict [ str , Sequence [ instancelib . instances . memory . DataPoint ]] Select n prototypes (most representatitve instances). Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Dict[str, Sequence[DataPoint]] Dictionary with labels and corresponding list of prototypes. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def prototypes(self, n: int = 5) -> Dict[str, Sequence[DataPoint]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[DataPoint]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()} MMDCritic 1 2 3 4 5 class MMDCritic ( instances : instancelib . instances . memory . MemoryBucketProvider , embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, kernel : Callable = < function exponential_kernel at 0x16eaf9310 > ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 class MMDCritic(PrototypeSampler): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer, kernel: Callable = exponential_kernel): \"\"\"Select prototypes and criticisms based on embedding distances using `MMD-Critic`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. kernel (Callable, optional): Kernel to calculate distances. Defaults to exponential_kernel. .. _MMD-critic: https://christophm.github.io/interpretable-ml-book/proto.html \"\"\" super().__init__(instances, embedder) self.kernel = kernel self._calculate_kernel() self._prototypes = None self._criticisms = None def _calculate_kernel(self): \"\"\"Calculate kernel `K` and column totals `colsum`.\"\"\" self.K = self.kernel(self.embedded, 1.0 / self.embedded.shape[1]) self.colsum = np.sum(self.K, axis=0) / self.embedded.shape[1] def prototypes(self, n: int = 5) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes (most representatitve instances), using `MMD-critic implementation`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Raises: ValueError: Cannot select more instances than the total number of instances. Returns: Sequence[DataPoint]: List of prototype instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if n > len(self.instances): raise ValueError(f'Cannot select more than all instances ({len(self.instances)}.') K = self.K colsum = self.colsum.copy() * 2 sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] diag = np.diagonal(K)[candidate_indices] if selected.shape[0] == 0: s1 -= np.abs(diag) else: temp = K[selected, :][:, candidate_indices] s2 = np.sum(temp, axis=0) * 2 + diag s2 /= (selected.shape[0] + 1) s1 -= s2 best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[is_selected > 0] selected_in_order = selected[is_selected[is_selected > 0].argsort()] self._prototypes = self._select_from_provider(selected_in_order) return self._prototypes def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Sequence[DataPoint]: \"\"\"Select `n` criticisms (instances not well represented by prototypes), using `MMD-critic implementation`_. Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. ValueError: Unknown regularizer or requested more criticisms than there are samples left. Returns: Sequence[DataPoint]: List of criticism instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if self._prototypes is None: raise Exception('Calculating criticisms requires prototypes. Run `MMDCritic.prototypes()` first.') regularizers = {None, 'logdet', 'iterative'} if regularizer not in regularizers: raise ValueError(f'Unknown {regularizer=}. Choose from {regularizers}.') if n > (len(self.instances) - len(self._prototypes)): raise ValueError('Cannot select more than instances excluding prototypes ', f'({len(self.instances) - len(self._prototypes)})') id_map = {instance: id for id, instance in enumerate(self.instances)} prototypes = np.array([id_map[p.identifier] for p in self._prototypes]) K = self.K colsum = self.colsum sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] is_selected[prototypes] = n + 1 inverse_of_prev_selected = None for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] temp = K[prototypes, :][:, candidate_indices] s2 = np.sum(temp, axis=0) s2 /= prototypes.shape[0] s1 -= s2 s1 = np.abs(s1) if regularizer == 'logdet': diag = np.diagonal(K + 1)[candidate_indices] if inverse_of_prev_selected is not None: temp = K[selected, :][:, candidate_indices] temp2 = np.dot(inverse_of_prev_selected, temp) reg = temp2 * temp regcolsum = np.sum(reg, axis=0) with np.errstate(divide='ignore'): reg = np.log(np.abs(diag - regcolsum)) s1 += reg else: with np.errstate(divide='ignore'): s1 -= np.log(np.abs(diag)) best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[(is_selected > 0) & (is_selected != (n + 1))] if regularizer == 'iterative': prototypes = np.concatenate([prototypes, np.expand_dims(best_sample_index, 0)]) if regularizer == 'logdet': inverse_of_prev_selected = np.linalg.pinv(K[selected, :][:, selected]) selected_in_order = selected[is_selected[(is_selected > 0) & (is_selected != (n + 1))].argsort()] self._criticisms = self._select_from_provider(selected_in_order) return self._criticisms def __call__(self, n_prototypes: int = 5, n_criticisms: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[DataPoint]]: \"\"\"Calculate prototypes and criticisms for the provided instances. Args: n_prototypes (int, optional): Number of prototypes. Defaults to 5. n_criticisms (int, optional): Number of criticisms. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Returns: Dict[str, Sequence[DataPoint]]: Dictionary containing prototypes and criticisms. \"\"\" return {'prototypes': self.prototypes(n=n_prototypes), 'criticisms': self.criticisms(n=n_criticisms, regularizer=regularizer)} Ancestors (in MRO) text_explainability.data.sampling.PrototypeSampler genbase.Readable Instance variables 1 embedded Methods criticisms 1 2 3 4 5 def criticisms ( self , n : int = 5 , regularizer : Optional [ str ] = None ) -> Sequence [ instancelib . instances . memory . DataPoint ] Select n criticisms (instances not well represented by prototypes), using MMD-critic implementation _. Parameters: Name Type Description Default n int Number of criticisms to select. Defaults to 5. 5 regularizer Optional[str] Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. None Returns: Type Description Sequence[DataPoint] List of criticism instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py Raises: Type Description Exception MMDCritic.prototypes() must first be run before being able to determine the criticisms. ValueError Unknown regularizer or requested more criticisms than there are samples left. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Sequence[DataPoint]: \"\"\"Select `n` criticisms (instances not well represented by prototypes), using `MMD-critic implementation`_. Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. ValueError: Unknown regularizer or requested more criticisms than there are samples left. Returns: Sequence[DataPoint]: List of criticism instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if self._prototypes is None: raise Exception('Calculating criticisms requires prototypes. Run `MMDCritic.prototypes()` first.') regularizers = {None, 'logdet', 'iterative'} if regularizer not in regularizers: raise ValueError(f'Unknown {regularizer=}. Choose from {regularizers}.') if n > (len(self.instances) - len(self._prototypes)): raise ValueError('Cannot select more than instances excluding prototypes ', f'({len(self.instances) - len(self._prototypes)})') id_map = {instance: id for id, instance in enumerate(self.instances)} prototypes = np.array([id_map[p.identifier] for p in self._prototypes]) K = self.K colsum = self.colsum sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] is_selected[prototypes] = n + 1 inverse_of_prev_selected = None for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] temp = K[prototypes, :][:, candidate_indices] s2 = np.sum(temp, axis=0) s2 /= prototypes.shape[0] s1 -= s2 s1 = np.abs(s1) if regularizer == 'logdet': diag = np.diagonal(K + 1)[candidate_indices] if inverse_of_prev_selected is not None: temp = K[selected, :][:, candidate_indices] temp2 = np.dot(inverse_of_prev_selected, temp) reg = temp2 * temp regcolsum = np.sum(reg, axis=0) with np.errstate(divide='ignore'): reg = np.log(np.abs(diag - regcolsum)) s1 += reg else: with np.errstate(divide='ignore'): s1 -= np.log(np.abs(diag)) best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[(is_selected > 0) & (is_selected != (n + 1))] if regularizer == 'iterative': prototypes = np.concatenate([prototypes, np.expand_dims(best_sample_index, 0)]) if regularizer == 'logdet': inverse_of_prev_selected = np.linalg.pinv(K[selected, :][:, selected]) selected_in_order = selected[is_selected[(is_selected > 0) & (is_selected != (n + 1))].argsort()] self._criticisms = self._select_from_provider(selected_in_order) return self._criticisms prototypes 1 2 3 4 def prototypes ( self , n : int = 5 ) -> Sequence [ instancelib . instances . memory . DataPoint ] Select n prototypes (most representatitve instances), using MMD-critic implementation _. Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Sequence[DataPoint] List of prototype instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py Raises: Type Description ValueError Cannot select more instances than the total number of instances. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def prototypes(self, n: int = 5) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes (most representatitve instances), using `MMD-critic implementation`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Raises: ValueError: Cannot select more instances than the total number of instances. Returns: Sequence[DataPoint]: List of prototype instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if n > len(self.instances): raise ValueError(f'Cannot select more than all instances ({len(self.instances)}.') K = self.K colsum = self.colsum.copy() * 2 sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] diag = np.diagonal(K)[candidate_indices] if selected.shape[0] == 0: s1 -= np.abs(diag) else: temp = K[selected, :][:, candidate_indices] s2 = np.sum(temp, axis=0) * 2 + diag s2 /= (selected.shape[0] + 1) s1 -= s2 best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[is_selected > 0] selected_in_order = selected[is_selected[is_selected > 0].argsort()] self._prototypes = self._select_from_provider(selected_in_order) return self._prototypes PrototypeSampler 1 2 3 4 class PrototypeSampler ( instances : instancelib . instances . memory . MemoryBucketProvider , embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '> ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class PrototypeSampler(Readable): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer): \"\"\"Generic class for sampling prototypes (representative samples) based on embedding distances. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. \"\"\" self.embedder = embedder() if isinstance(embedder, type) else embedder self.instances = self.embedder(instances) if any(instances[i].vector is None for i in instances) \\ else instances @property def embedded(self) -> np.ndarray: return np.stack(self.instances.bulk_get_vectors(list(self.instances))[-1]) def _select_from_provider(self, keys: Sequence[int]) -> Sequence[DataPoint]: \"\"\"Select instances from provider by keys.\"\"\" id_map = np.array(self.instances) return [self.instances[id_map[i]] for i in keys] def prototypes(self, n: int = 5) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[DataPoint]: List of prototype instances. \"\"\" raise NotImplementedError('Implemented in subclasses') def __call__(self, *args, **kwargs): return self.prototypes(*args, **kwargs) Ancestors (in MRO) genbase.Readable Descendants text_explainability.data.sampling.KMedoids text_explainability.data.sampling.MMDCritic Instance variables 1 embedded Methods prototypes 1 2 3 4 def prototypes ( self , n : int = 5 ) -> Sequence [ instancelib . instances . memory . DataPoint ] Select n prototypes. Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Sequence[DataPoint] List of prototype instances. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def prototypes(self, n: int = 5) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[DataPoint]: List of prototype instances. \"\"\" raise NotImplementedError('Implemented in subclasses')","title":"Sampling"},{"location":"reference/text_explainability/data/sampling/#module-text_explainabilitydatasampling","text":"Sample an (informative) subset from the data. Todo: 1 2 3 * Sample (informative?) subset from data * Refactor to make sampling base class * Add ability to perform MMD critic on a subset (e.g. single class) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 \"\"\"Sample an (informative) subset from the data. Todo: * Sample (informative?) subset from data * Refactor to make sampling base class * Add ability to perform MMD critic on a subset (e.g. single class) \"\"\" from typing import Callable, Dict, Optional, Sequence, Union import numpy as np from genbase import Readable, SeedMixin from instancelib.instances.memory import DataPoint, MemoryBucketProvider from instancelib.labels.base import LabelProvider from instancelib.labels.memory import MemoryLabelProvider from instancelib.machinelearning.base import AbstractClassifier from .embedding import Embedder, TfidfVectorizer from .weights import exponential_kernel class PrototypeSampler(Readable): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer): \"\"\"Generic class for sampling prototypes (representative samples) based on embedding distances. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. \"\"\" self.embedder = embedder() if isinstance(embedder, type) else embedder self.instances = self.embedder(instances) if any(instances[i].vector is None for i in instances) \\ else instances @property def embedded(self) -> np.ndarray: return np.stack(self.instances.bulk_get_vectors(list(self.instances))[-1]) def _select_from_provider(self, keys: Sequence[int]) -> Sequence[DataPoint]: \"\"\"Select instances from provider by keys.\"\"\" id_map = np.array(self.instances) return [self.instances[id_map[i]] for i in keys] def prototypes(self, n: int = 5) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[DataPoint]: List of prototype instances. \"\"\" raise NotImplementedError('Implemented in subclasses') def __call__(self, *args, **kwargs): return self.prototypes(*args, **kwargs) class KMedoids(PrototypeSampler, SeedMixin): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer, seed: int = 0): \"\"\"Sampling prototypes (representative samples) based on embedding distances using `k-Medoids`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html \"\"\" super().__init__(instances, embedder) self._seed = self._original_seed = seed def prototypes(self, n: int = 5, metric: Union[str, Callable] = 'cosine', **kwargs) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes (most representative samples) using `k-Medoids`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. metrics (Union[str, Callable], optional): Distance metric used to calculate medoids (e.g. 'cosine', 'euclidean' or your own function). See `pairwise distances` for a full list. Defaults to 'cosine'. **kwargs: Optional arguments passed to `k-Medoids`_ constructor. Returns: Sequence[DataPoint]: List of prototype instances. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html .. _pairwise distances: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html \"\"\" from sklearn_extra.cluster import KMedoids kmedoids = KMedoids(n_clusters=n, metric=metric, random_state=self.seed, **kwargs).fit(self.embedded) return self._select_from_provider(kmedoids.medoid_indices_) class MMDCritic(PrototypeSampler): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer, kernel: Callable = exponential_kernel): \"\"\"Select prototypes and criticisms based on embedding distances using `MMD-Critic`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. kernel (Callable, optional): Kernel to calculate distances. Defaults to exponential_kernel. .. _MMD-critic: https://christophm.github.io/interpretable-ml-book/proto.html \"\"\" super().__init__(instances, embedder) self.kernel = kernel self._calculate_kernel() self._prototypes = None self._criticisms = None def _calculate_kernel(self): \"\"\"Calculate kernel `K` and column totals `colsum`.\"\"\" self.K = self.kernel(self.embedded, 1.0 / self.embedded.shape[1]) self.colsum = np.sum(self.K, axis=0) / self.embedded.shape[1] def prototypes(self, n: int = 5) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes (most representatitve instances), using `MMD-critic implementation`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Raises: ValueError: Cannot select more instances than the total number of instances. Returns: Sequence[DataPoint]: List of prototype instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if n > len(self.instances): raise ValueError(f'Cannot select more than all instances ({len(self.instances)}.') K = self.K colsum = self.colsum.copy() * 2 sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] diag = np.diagonal(K)[candidate_indices] if selected.shape[0] == 0: s1 -= np.abs(diag) else: temp = K[selected, :][:, candidate_indices] s2 = np.sum(temp, axis=0) * 2 + diag s2 /= (selected.shape[0] + 1) s1 -= s2 best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[is_selected > 0] selected_in_order = selected[is_selected[is_selected > 0].argsort()] self._prototypes = self._select_from_provider(selected_in_order) return self._prototypes def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Sequence[DataPoint]: \"\"\"Select `n` criticisms (instances not well represented by prototypes), using `MMD-critic implementation`_. Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. ValueError: Unknown regularizer or requested more criticisms than there are samples left. Returns: Sequence[DataPoint]: List of criticism instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if self._prototypes is None: raise Exception('Calculating criticisms requires prototypes. Run `MMDCritic.prototypes()` first.') regularizers = {None, 'logdet', 'iterative'} if regularizer not in regularizers: raise ValueError(f'Unknown {regularizer=}. Choose from {regularizers}.') if n > (len(self.instances) - len(self._prototypes)): raise ValueError('Cannot select more than instances excluding prototypes ', f'({len(self.instances) - len(self._prototypes)})') id_map = {instance: id for id, instance in enumerate(self.instances)} prototypes = np.array([id_map[p.identifier] for p in self._prototypes]) K = self.K colsum = self.colsum sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] is_selected[prototypes] = n + 1 inverse_of_prev_selected = None for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] temp = K[prototypes, :][:, candidate_indices] s2 = np.sum(temp, axis=0) s2 /= prototypes.shape[0] s1 -= s2 s1 = np.abs(s1) if regularizer == 'logdet': diag = np.diagonal(K + 1)[candidate_indices] if inverse_of_prev_selected is not None: temp = K[selected, :][:, candidate_indices] temp2 = np.dot(inverse_of_prev_selected, temp) reg = temp2 * temp regcolsum = np.sum(reg, axis=0) with np.errstate(divide='ignore'): reg = np.log(np.abs(diag - regcolsum)) s1 += reg else: with np.errstate(divide='ignore'): s1 -= np.log(np.abs(diag)) best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[(is_selected > 0) & (is_selected != (n + 1))] if regularizer == 'iterative': prototypes = np.concatenate([prototypes, np.expand_dims(best_sample_index, 0)]) if regularizer == 'logdet': inverse_of_prev_selected = np.linalg.pinv(K[selected, :][:, selected]) selected_in_order = selected[is_selected[(is_selected > 0) & (is_selected != (n + 1))].argsort()] self._criticisms = self._select_from_provider(selected_in_order) return self._criticisms def __call__(self, n_prototypes: int = 5, n_criticisms: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[DataPoint]]: \"\"\"Calculate prototypes and criticisms for the provided instances. Args: n_prototypes (int, optional): Number of prototypes. Defaults to 5. n_criticisms (int, optional): Number of criticisms. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Returns: Dict[str, Sequence[DataPoint]]: Dictionary containing prototypes and criticisms. \"\"\" return {'prototypes': self.prototypes(n=n_prototypes), 'criticisms': self.criticisms(n=n_criticisms, regularizer=regularizer)} class LabelwisePrototypeSampler(Readable): def __init__(self, sampler: PrototypeSampler, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier], embedder: Embedder = TfidfVectorizer, **kwargs): \"\"\"Apply `PrototypeSampler()` for each label. Args: sampler (PrototypeSampler): Prototype sampler to construct (e.g. `KMedoids`, `MMDCritic`) instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. **kwargs: Additional arguments passed to `_setup_instances()` constructor. \"\"\" self.sampler = sampler if isinstance(sampler, type) else self.sampler.__class__ self.instances = instances self._get_labels(labels) self._setup_samplers(embedder, **kwargs) def _get_labels(self, labels: Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier]): \"\"\"Transform the labels into a `LabelProvider`.\"\"\" if not isinstance(labels, LabelProvider): if isinstance(labels, AbstractClassifier): labels_ = labels.predict(self.instances) else: labels_ = [(id, frozenset({label})) for id, label in zip(list(self.instances), labels)] labels = MemoryLabelProvider.from_tuples(labels_) self.labels = labels def _setup_samplers(self, embedder: Embedder, **kwargs): \"\"\"Setup a sampler for each label in `self.labels.labelset`. Args: embedder (Embedder): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. **kwargs: Additional arguments passed to sampler constructor. \"\"\" import copy def select_by_label(label): instances = copy.deepcopy(self.instances) keys_to_keep = self.labels.get_instances_by_label(label) instances._remove_from_bucket(frozenset(list(instances)).difference(keys_to_keep)) return instances self._samplers = {label: self.sampler(instances=select_by_label(label), embedder=embedder, **kwargs) for label in self.labels.labelset} self.samplers = self._samplers def prototypes(self, n: int = 5) -> Dict[str, Sequence[DataPoint]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[DataPoint]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()} def __call__(self, n: int = 5) -> Dict[str, Dict[str, Sequence[DataPoint]]]: \"\"\"Generate prototypes for each label. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Dict[str, Sequence[DataPoint]]]: Dictionary with labels and corresponding dictionary containing prototypes. \"\"\" return {label: {'prototypes': sampler.prototypes(n=n)} for label, sampler in self._samplers.items()} class LabelwiseKMedoids(LabelwisePrototypeSampler): def __init__(self, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider], embedder: Embedder = TfidfVectorizer, seed: int = 0): \"\"\"Select prototypes for each label based on embedding distances using `k-Medoids`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html \"\"\" super().__init__(KMedoids, instances=instances, labels=labels, embedder=embedder, seed=seed) class LabelwiseMMDCritic(LabelwisePrototypeSampler): def __init__(self, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider], embedder: Embedder = TfidfVectorizer, kernel: Callable = exponential_kernel): \"\"\"Select prototypes and criticisms for each label based on embedding distances using `MMD-Critic`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. kernel (Callable, optional): Kernel to calculate distances. Defaults to exponential_kernel. .. _MMD-critic: https://christophm.github.io/interpretable-ml-book/proto.html \"\"\" super().__init__(MMDCritic, instances=instances, labels=labels, embedder=embedder, kernel=kernel) def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[DataPoint]]: \"\"\"Select `n` criticisms (instances not well represented by prototypes). Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Dict[str, Sequence[DataPoint]]: Dictionary with labels and corresponding list of criticisms. \"\"\" return {label: sampler.criticisms(n=n, regularizer=regularizer) for label, sampler in self._samplers.items()} def __call__(self, n_prototypes: int = 5, n_criticisms: int = 5, regularizer: Optional[str] = None) -> Dict[str, Dict[str, Sequence[DataPoint]]]: \"\"\"Generate prototypes and criticisms for each label. Args: n_prototypes (int, optional): Number of prototypes to select. Defaults to 5. n_criticisms (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Returns: Dict[str, Dict[str, Sequence[DataPoint]]]: Dictionary with labels and corresponding dictionary containing prototypes and criticisms. \"\"\" return {label: sampler(n_prototypes=n_prototypes, n_criticisms=n_criticisms, regularizer=regularizer) for label, sampler in self._samplers.items()}","title":"Module text_explainability.data.sampling"},{"location":"reference/text_explainability/data/sampling/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/data/sampling/#kmedoids","text":"1 2 3 4 5 class KMedoids ( instances : instancelib . instances . memory . MemoryBucketProvider , embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class KMedoids(PrototypeSampler, SeedMixin): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer, seed: int = 0): \"\"\"Sampling prototypes (representative samples) based on embedding distances using `k-Medoids`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html \"\"\" super().__init__(instances, embedder) self._seed = self._original_seed = seed def prototypes(self, n: int = 5, metric: Union[str, Callable] = 'cosine', **kwargs) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes (most representative samples) using `k-Medoids`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. metrics (Union[str, Callable], optional): Distance metric used to calculate medoids (e.g. 'cosine', 'euclidean' or your own function). See `pairwise distances` for a full list. Defaults to 'cosine'. **kwargs: Optional arguments passed to `k-Medoids`_ constructor. Returns: Sequence[DataPoint]: List of prototype instances. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html .. _pairwise distances: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html \"\"\" from sklearn_extra.cluster import KMedoids kmedoids = KMedoids(n_clusters=n, metric=metric, random_state=self.seed, **kwargs).fit(self.embedded) return self._select_from_provider(kmedoids.medoid_indices_)","title":"KMedoids"},{"location":"reference/text_explainability/data/sampling/#ancestors-in-mro","text":"text_explainability.data.sampling.PrototypeSampler genbase.Readable genbase.mixin.SeedMixin","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/sampling/#instance-variables","text":"1 embedded 1 seed","title":"Instance variables"},{"location":"reference/text_explainability/data/sampling/#methods","text":"","title":"Methods"},{"location":"reference/text_explainability/data/sampling/#prototypes","text":"1 2 3 4 5 6 def prototypes ( self , n : int = 5 , metric : Union [ str , Callable ] = 'cosine' , ** kwargs ) -> Sequence [ instancelib . instances . memory . DataPoint ] Select n prototypes (most representative samples) using k-Medoids _. Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 metrics Union[str, Callable] Distance metric used to calculate medoids (e.g. 'cosine', 'euclidean' or your own function). See pairwise distances for a full list. Defaults to 'cosine'. None **kwargs None Optional arguments passed to k-Medoids _ constructor. None Returns: Type Description Sequence[DataPoint] List of prototype instances. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html .. _pairwise distances: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def prototypes(self, n: int = 5, metric: Union[str, Callable] = 'cosine', **kwargs) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes (most representative samples) using `k-Medoids`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. metrics (Union[str, Callable], optional): Distance metric used to calculate medoids (e.g. 'cosine', 'euclidean' or your own function). See `pairwise distances` for a full list. Defaults to 'cosine'. **kwargs: Optional arguments passed to `k-Medoids`_ constructor. Returns: Sequence[DataPoint]: List of prototype instances. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html .. _pairwise distances: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html \"\"\" from sklearn_extra.cluster import KMedoids kmedoids = KMedoids(n_clusters=n, metric=metric, random_state=self.seed, **kwargs).fit(self.embedded) return self._select_from_provider(kmedoids.medoid_indices_)","title":"prototypes"},{"location":"reference/text_explainability/data/sampling/#reset_seed","text":"1 2 3 def reset_seed ( self ) Reset the seed to the original seed value, and return self. View Source 1 2 3 4 5 6 7 def reset_seed(self): \"\"\"Reset the seed to the original seed value, and return self.\"\"\" self._seed = self._original_seed return self","title":"reset_seed"},{"location":"reference/text_explainability/data/sampling/#set_seed","text":"1 2 3 4 def set_seed ( self , seed : Optional [ int ] = None ) Set the current seed and original seed to a new value, and return self. Parameters: Name Type Description Default seed Optional[int] Seed value. If None, select a random seed. Defaults to None. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def set_seed(self, seed: Optional[int] = None): \"\"\"Set the current seed and original seed to a new value, and return self. Args: seed (Optional[int], optional): Seed value. If None, select a random seed. Defaults to None. \"\"\" if seed is None: seed = np.random.randint(100000) self._original_seed = seed return self.reset_seed()","title":"set_seed"},{"location":"reference/text_explainability/data/sampling/#labelwisekmedoids","text":"1 2 3 4 5 6 class LabelwiseKMedoids ( instances : instancelib . instances . memory . MemoryBucketProvider , labels : Union [ Sequence [ str ], Sequence [ int ], instancelib . labels . base . LabelProvider ], embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class LabelwiseKMedoids(LabelwisePrototypeSampler): def __init__(self, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider], embedder: Embedder = TfidfVectorizer, seed: int = 0): \"\"\"Select prototypes for each label based on embedding distances using `k-Medoids`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html \"\"\" super().__init__(KMedoids, instances=instances, labels=labels, embedder=embedder, seed=seed)","title":"LabelwiseKMedoids"},{"location":"reference/text_explainability/data/sampling/#ancestors-in-mro_1","text":"text_explainability.data.sampling.LabelwisePrototypeSampler genbase.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/sampling/#methods_1","text":"","title":"Methods"},{"location":"reference/text_explainability/data/sampling/#prototypes_1","text":"1 2 3 4 def prototypes ( self , n : int = 5 ) -> Dict [ str , Sequence [ instancelib . instances . memory . DataPoint ]] Select n prototypes (most representatitve instances). Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Dict[str, Sequence[DataPoint]] Dictionary with labels and corresponding list of prototypes. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def prototypes(self, n: int = 5) -> Dict[str, Sequence[DataPoint]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[DataPoint]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()}","title":"prototypes"},{"location":"reference/text_explainability/data/sampling/#labelwisemmdcritic","text":"1 2 3 4 5 6 class LabelwiseMMDCritic ( instances : instancelib . instances . memory . MemoryBucketProvider , labels : Union [ Sequence [ str ], Sequence [ int ], instancelib . labels . base . LabelProvider ], embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, kernel : Callable = < function exponential_kernel at 0x16eaf9310 > ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class LabelwiseMMDCritic(LabelwisePrototypeSampler): def __init__(self, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider], embedder: Embedder = TfidfVectorizer, kernel: Callable = exponential_kernel): \"\"\"Select prototypes and criticisms for each label based on embedding distances using `MMD-Critic`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. kernel (Callable, optional): Kernel to calculate distances. Defaults to exponential_kernel. .. _MMD-critic: https://christophm.github.io/interpretable-ml-book/proto.html \"\"\" super().__init__(MMDCritic, instances=instances, labels=labels, embedder=embedder, kernel=kernel) def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[DataPoint]]: \"\"\"Select `n` criticisms (instances not well represented by prototypes). Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Dict[str, Sequence[DataPoint]]: Dictionary with labels and corresponding list of criticisms. \"\"\" return {label: sampler.criticisms(n=n, regularizer=regularizer) for label, sampler in self._samplers.items()} def __call__(self, n_prototypes: int = 5, n_criticisms: int = 5, regularizer: Optional[str] = None) -> Dict[str, Dict[str, Sequence[DataPoint]]]: \"\"\"Generate prototypes and criticisms for each label. Args: n_prototypes (int, optional): Number of prototypes to select. Defaults to 5. n_criticisms (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Returns: Dict[str, Dict[str, Sequence[DataPoint]]]: Dictionary with labels and corresponding dictionary containing prototypes and criticisms. \"\"\" return {label: sampler(n_prototypes=n_prototypes, n_criticisms=n_criticisms, regularizer=regularizer) for label, sampler in self._samplers.items()}","title":"LabelwiseMMDCritic"},{"location":"reference/text_explainability/data/sampling/#ancestors-in-mro_2","text":"text_explainability.data.sampling.LabelwisePrototypeSampler genbase.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/sampling/#methods_2","text":"","title":"Methods"},{"location":"reference/text_explainability/data/sampling/#criticisms","text":"1 2 3 4 5 def criticisms ( self , n : int = 5 , regularizer : Optional [ str ] = None ) -> Dict [ str , Sequence [ instancelib . instances . memory . DataPoint ]] Select n criticisms (instances not well represented by prototypes). Parameters: Name Type Description Default n int Number of criticisms to select. Defaults to 5. 5 regularizer Optional[str] Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. None Returns: Type Description Dict[str, Sequence[DataPoint]] Dictionary with labels and corresponding list of criticisms. Raises: Type Description Exception MMDCritic.prototypes() must first be run before being able to determine the criticisms. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[DataPoint]]: \"\"\"Select `n` criticisms (instances not well represented by prototypes). Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Dict[str, Sequence[DataPoint]]: Dictionary with labels and corresponding list of criticisms. \"\"\" return {label: sampler.criticisms(n=n, regularizer=regularizer) for label, sampler in self._samplers.items()}","title":"criticisms"},{"location":"reference/text_explainability/data/sampling/#prototypes_2","text":"1 2 3 4 def prototypes ( self , n : int = 5 ) -> Dict [ str , Sequence [ instancelib . instances . memory . DataPoint ]] Select n prototypes (most representatitve instances). Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Dict[str, Sequence[DataPoint]] Dictionary with labels and corresponding list of prototypes. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def prototypes(self, n: int = 5) -> Dict[str, Sequence[DataPoint]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[DataPoint]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()}","title":"prototypes"},{"location":"reference/text_explainability/data/sampling/#labelwiseprototypesampler","text":"1 2 3 4 5 6 7 class LabelwisePrototypeSampler ( sampler : text_explainability . data . sampling . PrototypeSampler , instances : instancelib . instances . memory . MemoryBucketProvider , labels : Union [ Sequence [ str ], Sequence [ int ], instancelib . labels . base . LabelProvider , instancelib . machinelearning . base . AbstractClassifier ], embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class LabelwisePrototypeSampler(Readable): def __init__(self, sampler: PrototypeSampler, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier], embedder: Embedder = TfidfVectorizer, **kwargs): \"\"\"Apply `PrototypeSampler()` for each label. Args: sampler (PrototypeSampler): Prototype sampler to construct (e.g. `KMedoids`, `MMDCritic`) instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. **kwargs: Additional arguments passed to `_setup_instances()` constructor. \"\"\" self.sampler = sampler if isinstance(sampler, type) else self.sampler.__class__ self.instances = instances self._get_labels(labels) self._setup_samplers(embedder, **kwargs) def _get_labels(self, labels: Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier]): \"\"\"Transform the labels into a `LabelProvider`.\"\"\" if not isinstance(labels, LabelProvider): if isinstance(labels, AbstractClassifier): labels_ = labels.predict(self.instances) else: labels_ = [(id, frozenset({label})) for id, label in zip(list(self.instances), labels)] labels = MemoryLabelProvider.from_tuples(labels_) self.labels = labels def _setup_samplers(self, embedder: Embedder, **kwargs): \"\"\"Setup a sampler for each label in `self.labels.labelset`. Args: embedder (Embedder): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. **kwargs: Additional arguments passed to sampler constructor. \"\"\" import copy def select_by_label(label): instances = copy.deepcopy(self.instances) keys_to_keep = self.labels.get_instances_by_label(label) instances._remove_from_bucket(frozenset(list(instances)).difference(keys_to_keep)) return instances self._samplers = {label: self.sampler(instances=select_by_label(label), embedder=embedder, **kwargs) for label in self.labels.labelset} self.samplers = self._samplers def prototypes(self, n: int = 5) -> Dict[str, Sequence[DataPoint]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[DataPoint]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()} def __call__(self, n: int = 5) -> Dict[str, Dict[str, Sequence[DataPoint]]]: \"\"\"Generate prototypes for each label. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Dict[str, Sequence[DataPoint]]]: Dictionary with labels and corresponding dictionary containing prototypes. \"\"\" return {label: {'prototypes': sampler.prototypes(n=n)} for label, sampler in self._samplers.items()}","title":"LabelwisePrototypeSampler"},{"location":"reference/text_explainability/data/sampling/#ancestors-in-mro_3","text":"genbase.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/sampling/#descendants","text":"text_explainability.data.sampling.LabelwiseKMedoids text_explainability.data.sampling.LabelwiseMMDCritic","title":"Descendants"},{"location":"reference/text_explainability/data/sampling/#methods_3","text":"","title":"Methods"},{"location":"reference/text_explainability/data/sampling/#prototypes_3","text":"1 2 3 4 def prototypes ( self , n : int = 5 ) -> Dict [ str , Sequence [ instancelib . instances . memory . DataPoint ]] Select n prototypes (most representatitve instances). Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Dict[str, Sequence[DataPoint]] Dictionary with labels and corresponding list of prototypes. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def prototypes(self, n: int = 5) -> Dict[str, Sequence[DataPoint]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[DataPoint]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()}","title":"prototypes"},{"location":"reference/text_explainability/data/sampling/#mmdcritic","text":"1 2 3 4 5 class MMDCritic ( instances : instancelib . instances . memory . MemoryBucketProvider , embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, kernel : Callable = < function exponential_kernel at 0x16eaf9310 > ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 class MMDCritic(PrototypeSampler): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer, kernel: Callable = exponential_kernel): \"\"\"Select prototypes and criticisms based on embedding distances using `MMD-Critic`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. kernel (Callable, optional): Kernel to calculate distances. Defaults to exponential_kernel. .. _MMD-critic: https://christophm.github.io/interpretable-ml-book/proto.html \"\"\" super().__init__(instances, embedder) self.kernel = kernel self._calculate_kernel() self._prototypes = None self._criticisms = None def _calculate_kernel(self): \"\"\"Calculate kernel `K` and column totals `colsum`.\"\"\" self.K = self.kernel(self.embedded, 1.0 / self.embedded.shape[1]) self.colsum = np.sum(self.K, axis=0) / self.embedded.shape[1] def prototypes(self, n: int = 5) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes (most representatitve instances), using `MMD-critic implementation`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Raises: ValueError: Cannot select more instances than the total number of instances. Returns: Sequence[DataPoint]: List of prototype instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if n > len(self.instances): raise ValueError(f'Cannot select more than all instances ({len(self.instances)}.') K = self.K colsum = self.colsum.copy() * 2 sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] diag = np.diagonal(K)[candidate_indices] if selected.shape[0] == 0: s1 -= np.abs(diag) else: temp = K[selected, :][:, candidate_indices] s2 = np.sum(temp, axis=0) * 2 + diag s2 /= (selected.shape[0] + 1) s1 -= s2 best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[is_selected > 0] selected_in_order = selected[is_selected[is_selected > 0].argsort()] self._prototypes = self._select_from_provider(selected_in_order) return self._prototypes def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Sequence[DataPoint]: \"\"\"Select `n` criticisms (instances not well represented by prototypes), using `MMD-critic implementation`_. Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. ValueError: Unknown regularizer or requested more criticisms than there are samples left. Returns: Sequence[DataPoint]: List of criticism instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if self._prototypes is None: raise Exception('Calculating criticisms requires prototypes. Run `MMDCritic.prototypes()` first.') regularizers = {None, 'logdet', 'iterative'} if regularizer not in regularizers: raise ValueError(f'Unknown {regularizer=}. Choose from {regularizers}.') if n > (len(self.instances) - len(self._prototypes)): raise ValueError('Cannot select more than instances excluding prototypes ', f'({len(self.instances) - len(self._prototypes)})') id_map = {instance: id for id, instance in enumerate(self.instances)} prototypes = np.array([id_map[p.identifier] for p in self._prototypes]) K = self.K colsum = self.colsum sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] is_selected[prototypes] = n + 1 inverse_of_prev_selected = None for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] temp = K[prototypes, :][:, candidate_indices] s2 = np.sum(temp, axis=0) s2 /= prototypes.shape[0] s1 -= s2 s1 = np.abs(s1) if regularizer == 'logdet': diag = np.diagonal(K + 1)[candidate_indices] if inverse_of_prev_selected is not None: temp = K[selected, :][:, candidate_indices] temp2 = np.dot(inverse_of_prev_selected, temp) reg = temp2 * temp regcolsum = np.sum(reg, axis=0) with np.errstate(divide='ignore'): reg = np.log(np.abs(diag - regcolsum)) s1 += reg else: with np.errstate(divide='ignore'): s1 -= np.log(np.abs(diag)) best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[(is_selected > 0) & (is_selected != (n + 1))] if regularizer == 'iterative': prototypes = np.concatenate([prototypes, np.expand_dims(best_sample_index, 0)]) if regularizer == 'logdet': inverse_of_prev_selected = np.linalg.pinv(K[selected, :][:, selected]) selected_in_order = selected[is_selected[(is_selected > 0) & (is_selected != (n + 1))].argsort()] self._criticisms = self._select_from_provider(selected_in_order) return self._criticisms def __call__(self, n_prototypes: int = 5, n_criticisms: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[DataPoint]]: \"\"\"Calculate prototypes and criticisms for the provided instances. Args: n_prototypes (int, optional): Number of prototypes. Defaults to 5. n_criticisms (int, optional): Number of criticisms. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Returns: Dict[str, Sequence[DataPoint]]: Dictionary containing prototypes and criticisms. \"\"\" return {'prototypes': self.prototypes(n=n_prototypes), 'criticisms': self.criticisms(n=n_criticisms, regularizer=regularizer)}","title":"MMDCritic"},{"location":"reference/text_explainability/data/sampling/#ancestors-in-mro_4","text":"text_explainability.data.sampling.PrototypeSampler genbase.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/sampling/#instance-variables_1","text":"1 embedded","title":"Instance variables"},{"location":"reference/text_explainability/data/sampling/#methods_4","text":"","title":"Methods"},{"location":"reference/text_explainability/data/sampling/#criticisms_1","text":"1 2 3 4 5 def criticisms ( self , n : int = 5 , regularizer : Optional [ str ] = None ) -> Sequence [ instancelib . instances . memory . DataPoint ] Select n criticisms (instances not well represented by prototypes), using MMD-critic implementation _. Parameters: Name Type Description Default n int Number of criticisms to select. Defaults to 5. 5 regularizer Optional[str] Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. None Returns: Type Description Sequence[DataPoint] List of criticism instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py Raises: Type Description Exception MMDCritic.prototypes() must first be run before being able to determine the criticisms. ValueError Unknown regularizer or requested more criticisms than there are samples left. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Sequence[DataPoint]: \"\"\"Select `n` criticisms (instances not well represented by prototypes), using `MMD-critic implementation`_. Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. ValueError: Unknown regularizer or requested more criticisms than there are samples left. Returns: Sequence[DataPoint]: List of criticism instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if self._prototypes is None: raise Exception('Calculating criticisms requires prototypes. Run `MMDCritic.prototypes()` first.') regularizers = {None, 'logdet', 'iterative'} if regularizer not in regularizers: raise ValueError(f'Unknown {regularizer=}. Choose from {regularizers}.') if n > (len(self.instances) - len(self._prototypes)): raise ValueError('Cannot select more than instances excluding prototypes ', f'({len(self.instances) - len(self._prototypes)})') id_map = {instance: id for id, instance in enumerate(self.instances)} prototypes = np.array([id_map[p.identifier] for p in self._prototypes]) K = self.K colsum = self.colsum sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] is_selected[prototypes] = n + 1 inverse_of_prev_selected = None for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] temp = K[prototypes, :][:, candidate_indices] s2 = np.sum(temp, axis=0) s2 /= prototypes.shape[0] s1 -= s2 s1 = np.abs(s1) if regularizer == 'logdet': diag = np.diagonal(K + 1)[candidate_indices] if inverse_of_prev_selected is not None: temp = K[selected, :][:, candidate_indices] temp2 = np.dot(inverse_of_prev_selected, temp) reg = temp2 * temp regcolsum = np.sum(reg, axis=0) with np.errstate(divide='ignore'): reg = np.log(np.abs(diag - regcolsum)) s1 += reg else: with np.errstate(divide='ignore'): s1 -= np.log(np.abs(diag)) best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[(is_selected > 0) & (is_selected != (n + 1))] if regularizer == 'iterative': prototypes = np.concatenate([prototypes, np.expand_dims(best_sample_index, 0)]) if regularizer == 'logdet': inverse_of_prev_selected = np.linalg.pinv(K[selected, :][:, selected]) selected_in_order = selected[is_selected[(is_selected > 0) & (is_selected != (n + 1))].argsort()] self._criticisms = self._select_from_provider(selected_in_order) return self._criticisms","title":"criticisms"},{"location":"reference/text_explainability/data/sampling/#prototypes_4","text":"1 2 3 4 def prototypes ( self , n : int = 5 ) -> Sequence [ instancelib . instances . memory . DataPoint ] Select n prototypes (most representatitve instances), using MMD-critic implementation _. Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Sequence[DataPoint] List of prototype instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py Raises: Type Description ValueError Cannot select more instances than the total number of instances. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def prototypes(self, n: int = 5) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes (most representatitve instances), using `MMD-critic implementation`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Raises: ValueError: Cannot select more instances than the total number of instances. Returns: Sequence[DataPoint]: List of prototype instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if n > len(self.instances): raise ValueError(f'Cannot select more than all instances ({len(self.instances)}.') K = self.K colsum = self.colsum.copy() * 2 sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] diag = np.diagonal(K)[candidate_indices] if selected.shape[0] == 0: s1 -= np.abs(diag) else: temp = K[selected, :][:, candidate_indices] s2 = np.sum(temp, axis=0) * 2 + diag s2 /= (selected.shape[0] + 1) s1 -= s2 best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[is_selected > 0] selected_in_order = selected[is_selected[is_selected > 0].argsort()] self._prototypes = self._select_from_provider(selected_in_order) return self._prototypes","title":"prototypes"},{"location":"reference/text_explainability/data/sampling/#prototypesampler","text":"1 2 3 4 class PrototypeSampler ( instances : instancelib . instances . memory . MemoryBucketProvider , embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '> ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class PrototypeSampler(Readable): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer): \"\"\"Generic class for sampling prototypes (representative samples) based on embedding distances. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. \"\"\" self.embedder = embedder() if isinstance(embedder, type) else embedder self.instances = self.embedder(instances) if any(instances[i].vector is None for i in instances) \\ else instances @property def embedded(self) -> np.ndarray: return np.stack(self.instances.bulk_get_vectors(list(self.instances))[-1]) def _select_from_provider(self, keys: Sequence[int]) -> Sequence[DataPoint]: \"\"\"Select instances from provider by keys.\"\"\" id_map = np.array(self.instances) return [self.instances[id_map[i]] for i in keys] def prototypes(self, n: int = 5) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[DataPoint]: List of prototype instances. \"\"\" raise NotImplementedError('Implemented in subclasses') def __call__(self, *args, **kwargs): return self.prototypes(*args, **kwargs)","title":"PrototypeSampler"},{"location":"reference/text_explainability/data/sampling/#ancestors-in-mro_5","text":"genbase.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/sampling/#descendants_1","text":"text_explainability.data.sampling.KMedoids text_explainability.data.sampling.MMDCritic","title":"Descendants"},{"location":"reference/text_explainability/data/sampling/#instance-variables_2","text":"1 embedded","title":"Instance variables"},{"location":"reference/text_explainability/data/sampling/#methods_5","text":"","title":"Methods"},{"location":"reference/text_explainability/data/sampling/#prototypes_5","text":"1 2 3 4 def prototypes ( self , n : int = 5 ) -> Sequence [ instancelib . instances . memory . DataPoint ] Select n prototypes. Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Sequence[DataPoint] List of prototype instances. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def prototypes(self, n: int = 5) -> Sequence[DataPoint]: \"\"\"Select `n` prototypes. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[DataPoint]: List of prototype instances. \"\"\" raise NotImplementedError('Implemented in subclasses')","title":"prototypes"},{"location":"reference/text_explainability/data/weights/","text":"Module text_explainability.data.weights None None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import numpy as np from sklearn.metrics.pairwise import pairwise_distances as pwd def pairwise_distances(a, b, metric='cosine', multiply=100): if b.ndim == 1: b = b.reshape(1, -1) return pwd(a, b, metric=metric).ravel() * multiply def exponential_kernel(d, kw): return np.sqrt(np.exp(-(d ** 2) / kw ** 2)) Functions exponential_kernel 1 2 3 4 def exponential_kernel ( d , kw ) View Source 1 2 3 def exponential_kernel(d, kw): return np.sqrt(np.exp(-(d ** 2) / kw ** 2)) pairwise_distances 1 2 3 4 5 6 def pairwise_distances ( a , b , metric = 'cosine' , multiply = 100 ) View Source 1 2 3 4 5 6 7 def pairwise_distances(a, b, metric='cosine', multiply=100): if b.ndim == 1: b = b.reshape(1, -1) return pwd(a, b, metric=metric).ravel() * multiply","title":"Weights"},{"location":"reference/text_explainability/data/weights/#module-text_explainabilitydataweights","text":"None None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import numpy as np from sklearn.metrics.pairwise import pairwise_distances as pwd def pairwise_distances(a, b, metric='cosine', multiply=100): if b.ndim == 1: b = b.reshape(1, -1) return pwd(a, b, metric=metric).ravel() * multiply def exponential_kernel(d, kw): return np.sqrt(np.exp(-(d ** 2) / kw ** 2))","title":"Module text_explainability.data.weights"},{"location":"reference/text_explainability/data/weights/#functions","text":"","title":"Functions"},{"location":"reference/text_explainability/data/weights/#exponential_kernel","text":"1 2 3 4 def exponential_kernel ( d , kw ) View Source 1 2 3 def exponential_kernel(d, kw): return np.sqrt(np.exp(-(d ** 2) / kw ** 2))","title":"exponential_kernel"},{"location":"reference/text_explainability/data/weights/#pairwise_distances","text":"1 2 3 4 5 6 def pairwise_distances ( a , b , metric = 'cosine' , multiply = 100 ) View Source 1 2 3 4 5 6 7 def pairwise_distances(a, b, metric='cosine', multiply=100): if b.ndim == 1: b = b.reshape(1, -1) return pwd(a, b, metric=metric).ravel() * multiply","title":"pairwise_distances"},{"location":"reference/text_explainability/generation/","text":"Module text_explainability.generation Feature selection and local/global surrogate model generation. None View Source 1 \"\"\"Feature selection and local/global surrogate model generation.\"\"\" Sub-modules text_explainability.generation.feature_selection text_explainability.generation.return_types text_explainability.generation.surrogate text_explainability.generation.target_encoding","title":"Index"},{"location":"reference/text_explainability/generation/#module-text_explainabilitygeneration","text":"Feature selection and local/global surrogate model generation. None View Source 1 \"\"\"Feature selection and local/global surrogate model generation.\"\"\"","title":"Module text_explainability.generation"},{"location":"reference/text_explainability/generation/#sub-modules","text":"text_explainability.generation.feature_selection text_explainability.generation.return_types text_explainability.generation.surrogate text_explainability.generation.target_encoding","title":"Sub-modules"},{"location":"reference/text_explainability/generation/feature_selection/","text":"Module text_explainability.generation.feature_selection Feature selection methods for limiting explanation length. Todo: 1 * Convert to factory design pattern View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 \"\"\"Feature selection methods for limiting explanation length. Todo: * Convert to factory design pattern \"\"\" from typing import Optional import numpy as np from genbase import Readable from sklearn.linear_model import Lasso, LassoLarsIC, lars_path from .surrogate import LinearSurrogate class FeatureSelector(Readable): def __init__(self, model: Optional[LinearSurrogate] = None): \"\"\"[summary] Args: model (Optional[LinearSurrogate], optional): Linear surrogate used to calculate feature importance scores. Defaults to None. \"\"\" super().__init__() self.model = model if self.model is not None: self.model.alpha_zero() self.model.fit_intercept = True def _forward_selection(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection with forward selection, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for y. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): [description]. Defaults to 10. Raises: ValueError: The local linear model used to calculate forward_selection was not defined. Returns: np.ndarray: Indices of selected features. .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" if self.model is None: raise ValueError('forward_selection requires a local linear model') n_features = min(X.shape[1], n_features) used_features = [] for _ in range(n_features): max_ = -100000000 best = 0 for feature in range(X.shape[1]): if feature in used_features: continue self.model.fit(X[:, used_features + [feature]], y, weights=weights) score = self.model.score(X[:, used_features + [feature]], y, weights=weights) if score > max_: best = feature max_ = score used_features.append(best) return np.sort(np.array(used_features)) def _highest_weights(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection according to highest feature importance, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. Raises: ValueError: The local linear model used to calculate highest_weights was not defined. Returns: np.ndarray: Indices of selected features. .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" if self.model is None: raise ValueError('highest_weights requires a local linear model') self.model.fit(X, y, weights=weights) weighted_data = self.model.feature_importances * X[0] feature_weights = sorted( zip(range(X.shape[1]), weighted_data), key=lambda x: np.abs(x[1]), reverse=True) return np.sort(np.array([x[0] for x in feature_weights[:n_features]])) def _lasso_path(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection with `LASSO`_, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. Returns: np.ndarray: Indices of selected features. .. _LASSO: https://en.wikipedia.org/wiki/Lasso_(statistics) .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" if weights is None: weights = np.ones(X.shape[0]) weighted_data = ((X - np.average(X, axis=0, weights=weights)) * np.sqrt(weights[:, np.newaxis])) weighted_labels = ((y - np.average(y, weights=weights)) * np.sqrt(weights)) nonzero = range(weighted_data.shape[1]) _, _, coefs = lars_path(weighted_data, weighted_labels, method='lasso', verbose=False) for i in range(len(coefs.T) - 1, 0, -1): nonzero = coefs.T[i].nonzero()[0] if len(nonzero) <= n_features: break used_features = nonzero return np.sort(np.array(used_features)) def _information_criterion(self, X: np.ndarray, y: np.ndarray, criterion='aic') -> np.ndarray: \"\"\"AIC/BIC for feature selection, as used by `SHAP`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. criterion (str, optional): Whether to use `Akaike Information Criterion`_ (`aic`) or `Bayesian Information Criterion`_ (`bic`). Defaults to 'aic'. Raises: ValueError: Unknown criterion. Returns: np.ndarray: Indices of selected features. .. _SHAP: https://github.com/slundberg/shap .. _Akaike Information Criterion: https://en.wikipedia.org/wiki/Akaike_information_criterion .. _Bayesian Information Criterion: https://en.wikipedia.org/wiki/Bayesian_information_criterion \"\"\" if criterion not in ['aic', 'bic']: raise ValueError(f'Unknown criterion \"{criterion}\", choose from [aic, bic]') # use n_features if y.ndim > 1: # TODO: multiclass support? y = y[:, 0] return np.sort(np.nonzero(LassoLarsIC(criterion=criterion).fit(X, y).coef_)[0]) def _l1_reg(self, X: np.ndarray, y: np.ndarray, n_features: int = 10, alpha: Optional[float] = None) -> np.ndarray: \"\"\"L1-regularization for feature selection, as used by `SHAP`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. n_features (int, optional): Number of features to select. Defaults to 10. alpha (Optional[float], optional): Hyperparameter for L1 regularization. Defaults to None. Returns: np.ndarray: Indices of selected features. .. _SHAP: https://github.com/slundberg/shap \"\"\" if alpha is not None: return np.nonzero(Lasso(alpha=alpha).fit(X, y).coef_)[0] # use n_features if y.ndim > 1: # TODO: multiclass support? y = y[:, 0] return np.sort(lars_path(X, y, max_iter=n_features)[1]) def __call__(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10, method: Optional[str] = None, alpha: Optional[float] = None) -> np.ndarray: \"\"\"Apply feature selection for dataset X and targets y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. method (str, optional): Method to apply for feature selection, choose from `None`, `forward_selection`, `highest_weights`, `lasso_path`, `aic`, `bic`, `l1_reg`. Defaults to None. alpha (Optional[float], optional): Hyperparameter for L1 regularization. Defaults to None. Raises: ValueError: Unknown method, or the requirements of a method have not been satisfied. Returns: np.ndarray: Indices of selected features. \"\"\" if self.model is None and method in ['forward_selection', 'highest_weights']: raise ValueError(f'{self.__class__.__name__} requires a `model` to use methods forward_selection and ', 'highest_weights') if method not in [None, 'forward_selection', 'highest_weights', 'lasso_path', 'aic', 'bic', 'l1_reg']: raise ValueError(f'Unknown {method=}') n_features = min(X.shape[1], n_features) # Do not perform feature selection, but return all if n_features == X.shape[1] and method not in ['aic', 'bic', 'l1_reg'] or method is None: return np.arange(X.shape[1]) # Perform feature selection if method == 'forward_selection': return self._forward_selection(X, y, weights=weights, n_features=n_features) elif method == 'highest_weights': return self._highest_weights(X, y, weights=weights, n_features=n_features) elif method == 'lasso_path': return self._lasso_path(X, y, weights=weights, n_features=n_features) elif method in ['aic', 'bic']: return self._information_criterion(X, y, criterion=method) elif method == 'l1_reg': return self._l1_reg(X, y, n_features=n_features, alpha=alpha) def select(self, *args, **kwargs): \"\"\"Alias for `FeatureSelector().__call__()`\"\"\" return self(*args, **kwargs) Classes FeatureSelector 1 2 3 class FeatureSelector ( model : Optional [ text_explainability . generation . surrogate . LinearSurrogate ] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 class FeatureSelector(Readable): def __init__(self, model: Optional[LinearSurrogate] = None): \"\"\"[summary] Args: model (Optional[LinearSurrogate], optional): Linear surrogate used to calculate feature importance scores. Defaults to None. \"\"\" super().__init__() self.model = model if self.model is not None: self.model.alpha_zero() self.model.fit_intercept = True def _forward_selection(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection with forward selection, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for y. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): [description]. Defaults to 10. Raises: ValueError: The local linear model used to calculate forward_selection was not defined. Returns: np.ndarray: Indices of selected features. .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" if self.model is None: raise ValueError('forward_selection requires a local linear model') n_features = min(X.shape[1], n_features) used_features = [] for _ in range(n_features): max_ = -100000000 best = 0 for feature in range(X.shape[1]): if feature in used_features: continue self.model.fit(X[:, used_features + [feature]], y, weights=weights) score = self.model.score(X[:, used_features + [feature]], y, weights=weights) if score > max_: best = feature max_ = score used_features.append(best) return np.sort(np.array(used_features)) def _highest_weights(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection according to highest feature importance, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. Raises: ValueError: The local linear model used to calculate highest_weights was not defined. Returns: np.ndarray: Indices of selected features. .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" if self.model is None: raise ValueError('highest_weights requires a local linear model') self.model.fit(X, y, weights=weights) weighted_data = self.model.feature_importances * X[0] feature_weights = sorted( zip(range(X.shape[1]), weighted_data), key=lambda x: np.abs(x[1]), reverse=True) return np.sort(np.array([x[0] for x in feature_weights[:n_features]])) def _lasso_path(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection with `LASSO`_, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. Returns: np.ndarray: Indices of selected features. .. _LASSO: https://en.wikipedia.org/wiki/Lasso_(statistics) .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" if weights is None: weights = np.ones(X.shape[0]) weighted_data = ((X - np.average(X, axis=0, weights=weights)) * np.sqrt(weights[:, np.newaxis])) weighted_labels = ((y - np.average(y, weights=weights)) * np.sqrt(weights)) nonzero = range(weighted_data.shape[1]) _, _, coefs = lars_path(weighted_data, weighted_labels, method='lasso', verbose=False) for i in range(len(coefs.T) - 1, 0, -1): nonzero = coefs.T[i].nonzero()[0] if len(nonzero) <= n_features: break used_features = nonzero return np.sort(np.array(used_features)) def _information_criterion(self, X: np.ndarray, y: np.ndarray, criterion='aic') -> np.ndarray: \"\"\"AIC/BIC for feature selection, as used by `SHAP`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. criterion (str, optional): Whether to use `Akaike Information Criterion`_ (`aic`) or `Bayesian Information Criterion`_ (`bic`). Defaults to 'aic'. Raises: ValueError: Unknown criterion. Returns: np.ndarray: Indices of selected features. .. _SHAP: https://github.com/slundberg/shap .. _Akaike Information Criterion: https://en.wikipedia.org/wiki/Akaike_information_criterion .. _Bayesian Information Criterion: https://en.wikipedia.org/wiki/Bayesian_information_criterion \"\"\" if criterion not in ['aic', 'bic']: raise ValueError(f'Unknown criterion \"{criterion}\", choose from [aic, bic]') # use n_features if y.ndim > 1: # TODO: multiclass support? y = y[:, 0] return np.sort(np.nonzero(LassoLarsIC(criterion=criterion).fit(X, y).coef_)[0]) def _l1_reg(self, X: np.ndarray, y: np.ndarray, n_features: int = 10, alpha: Optional[float] = None) -> np.ndarray: \"\"\"L1-regularization for feature selection, as used by `SHAP`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. n_features (int, optional): Number of features to select. Defaults to 10. alpha (Optional[float], optional): Hyperparameter for L1 regularization. Defaults to None. Returns: np.ndarray: Indices of selected features. .. _SHAP: https://github.com/slundberg/shap \"\"\" if alpha is not None: return np.nonzero(Lasso(alpha=alpha).fit(X, y).coef_)[0] # use n_features if y.ndim > 1: # TODO: multiclass support? y = y[:, 0] return np.sort(lars_path(X, y, max_iter=n_features)[1]) def __call__(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10, method: Optional[str] = None, alpha: Optional[float] = None) -> np.ndarray: \"\"\"Apply feature selection for dataset X and targets y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. method (str, optional): Method to apply for feature selection, choose from `None`, `forward_selection`, `highest_weights`, `lasso_path`, `aic`, `bic`, `l1_reg`. Defaults to None. alpha (Optional[float], optional): Hyperparameter for L1 regularization. Defaults to None. Raises: ValueError: Unknown method, or the requirements of a method have not been satisfied. Returns: np.ndarray: Indices of selected features. \"\"\" if self.model is None and method in ['forward_selection', 'highest_weights']: raise ValueError(f'{self.__class__.__name__} requires a `model` to use methods forward_selection and ', 'highest_weights') if method not in [None, 'forward_selection', 'highest_weights', 'lasso_path', 'aic', 'bic', 'l1_reg']: raise ValueError(f'Unknown {method=}') n_features = min(X.shape[1], n_features) # Do not perform feature selection, but return all if n_features == X.shape[1] and method not in ['aic', 'bic', 'l1_reg'] or method is None: return np.arange(X.shape[1]) # Perform feature selection if method == 'forward_selection': return self._forward_selection(X, y, weights=weights, n_features=n_features) elif method == 'highest_weights': return self._highest_weights(X, y, weights=weights, n_features=n_features) elif method == 'lasso_path': return self._lasso_path(X, y, weights=weights, n_features=n_features) elif method in ['aic', 'bic']: return self._information_criterion(X, y, criterion=method) elif method == 'l1_reg': return self._l1_reg(X, y, n_features=n_features, alpha=alpha) def select(self, *args, **kwargs): \"\"\"Alias for `FeatureSelector().__call__()`\"\"\" return self(*args, **kwargs) Ancestors (in MRO) genbase.Readable Methods select 1 2 3 4 5 def select ( self , * args , ** kwargs ) Alias for FeatureSelector().__call__() View Source 1 2 3 4 5 def select(self, *args, **kwargs): \"\"\"Alias for `FeatureSelector().__call__()`\"\"\" return self(*args, **kwargs)","title":"Feature Selection"},{"location":"reference/text_explainability/generation/feature_selection/#module-text_explainabilitygenerationfeature_selection","text":"Feature selection methods for limiting explanation length. Todo: 1 * Convert to factory design pattern View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 \"\"\"Feature selection methods for limiting explanation length. Todo: * Convert to factory design pattern \"\"\" from typing import Optional import numpy as np from genbase import Readable from sklearn.linear_model import Lasso, LassoLarsIC, lars_path from .surrogate import LinearSurrogate class FeatureSelector(Readable): def __init__(self, model: Optional[LinearSurrogate] = None): \"\"\"[summary] Args: model (Optional[LinearSurrogate], optional): Linear surrogate used to calculate feature importance scores. Defaults to None. \"\"\" super().__init__() self.model = model if self.model is not None: self.model.alpha_zero() self.model.fit_intercept = True def _forward_selection(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection with forward selection, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for y. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): [description]. Defaults to 10. Raises: ValueError: The local linear model used to calculate forward_selection was not defined. Returns: np.ndarray: Indices of selected features. .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" if self.model is None: raise ValueError('forward_selection requires a local linear model') n_features = min(X.shape[1], n_features) used_features = [] for _ in range(n_features): max_ = -100000000 best = 0 for feature in range(X.shape[1]): if feature in used_features: continue self.model.fit(X[:, used_features + [feature]], y, weights=weights) score = self.model.score(X[:, used_features + [feature]], y, weights=weights) if score > max_: best = feature max_ = score used_features.append(best) return np.sort(np.array(used_features)) def _highest_weights(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection according to highest feature importance, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. Raises: ValueError: The local linear model used to calculate highest_weights was not defined. Returns: np.ndarray: Indices of selected features. .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" if self.model is None: raise ValueError('highest_weights requires a local linear model') self.model.fit(X, y, weights=weights) weighted_data = self.model.feature_importances * X[0] feature_weights = sorted( zip(range(X.shape[1]), weighted_data), key=lambda x: np.abs(x[1]), reverse=True) return np.sort(np.array([x[0] for x in feature_weights[:n_features]])) def _lasso_path(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection with `LASSO`_, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. Returns: np.ndarray: Indices of selected features. .. _LASSO: https://en.wikipedia.org/wiki/Lasso_(statistics) .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" if weights is None: weights = np.ones(X.shape[0]) weighted_data = ((X - np.average(X, axis=0, weights=weights)) * np.sqrt(weights[:, np.newaxis])) weighted_labels = ((y - np.average(y, weights=weights)) * np.sqrt(weights)) nonzero = range(weighted_data.shape[1]) _, _, coefs = lars_path(weighted_data, weighted_labels, method='lasso', verbose=False) for i in range(len(coefs.T) - 1, 0, -1): nonzero = coefs.T[i].nonzero()[0] if len(nonzero) <= n_features: break used_features = nonzero return np.sort(np.array(used_features)) def _information_criterion(self, X: np.ndarray, y: np.ndarray, criterion='aic') -> np.ndarray: \"\"\"AIC/BIC for feature selection, as used by `SHAP`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. criterion (str, optional): Whether to use `Akaike Information Criterion`_ (`aic`) or `Bayesian Information Criterion`_ (`bic`). Defaults to 'aic'. Raises: ValueError: Unknown criterion. Returns: np.ndarray: Indices of selected features. .. _SHAP: https://github.com/slundberg/shap .. _Akaike Information Criterion: https://en.wikipedia.org/wiki/Akaike_information_criterion .. _Bayesian Information Criterion: https://en.wikipedia.org/wiki/Bayesian_information_criterion \"\"\" if criterion not in ['aic', 'bic']: raise ValueError(f'Unknown criterion \"{criterion}\", choose from [aic, bic]') # use n_features if y.ndim > 1: # TODO: multiclass support? y = y[:, 0] return np.sort(np.nonzero(LassoLarsIC(criterion=criterion).fit(X, y).coef_)[0]) def _l1_reg(self, X: np.ndarray, y: np.ndarray, n_features: int = 10, alpha: Optional[float] = None) -> np.ndarray: \"\"\"L1-regularization for feature selection, as used by `SHAP`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. n_features (int, optional): Number of features to select. Defaults to 10. alpha (Optional[float], optional): Hyperparameter for L1 regularization. Defaults to None. Returns: np.ndarray: Indices of selected features. .. _SHAP: https://github.com/slundberg/shap \"\"\" if alpha is not None: return np.nonzero(Lasso(alpha=alpha).fit(X, y).coef_)[0] # use n_features if y.ndim > 1: # TODO: multiclass support? y = y[:, 0] return np.sort(lars_path(X, y, max_iter=n_features)[1]) def __call__(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10, method: Optional[str] = None, alpha: Optional[float] = None) -> np.ndarray: \"\"\"Apply feature selection for dataset X and targets y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. method (str, optional): Method to apply for feature selection, choose from `None`, `forward_selection`, `highest_weights`, `lasso_path`, `aic`, `bic`, `l1_reg`. Defaults to None. alpha (Optional[float], optional): Hyperparameter for L1 regularization. Defaults to None. Raises: ValueError: Unknown method, or the requirements of a method have not been satisfied. Returns: np.ndarray: Indices of selected features. \"\"\" if self.model is None and method in ['forward_selection', 'highest_weights']: raise ValueError(f'{self.__class__.__name__} requires a `model` to use methods forward_selection and ', 'highest_weights') if method not in [None, 'forward_selection', 'highest_weights', 'lasso_path', 'aic', 'bic', 'l1_reg']: raise ValueError(f'Unknown {method=}') n_features = min(X.shape[1], n_features) # Do not perform feature selection, but return all if n_features == X.shape[1] and method not in ['aic', 'bic', 'l1_reg'] or method is None: return np.arange(X.shape[1]) # Perform feature selection if method == 'forward_selection': return self._forward_selection(X, y, weights=weights, n_features=n_features) elif method == 'highest_weights': return self._highest_weights(X, y, weights=weights, n_features=n_features) elif method == 'lasso_path': return self._lasso_path(X, y, weights=weights, n_features=n_features) elif method in ['aic', 'bic']: return self._information_criterion(X, y, criterion=method) elif method == 'l1_reg': return self._l1_reg(X, y, n_features=n_features, alpha=alpha) def select(self, *args, **kwargs): \"\"\"Alias for `FeatureSelector().__call__()`\"\"\" return self(*args, **kwargs)","title":"Module text_explainability.generation.feature_selection"},{"location":"reference/text_explainability/generation/feature_selection/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/generation/feature_selection/#featureselector","text":"1 2 3 class FeatureSelector ( model : Optional [ text_explainability . generation . surrogate . LinearSurrogate ] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 class FeatureSelector(Readable): def __init__(self, model: Optional[LinearSurrogate] = None): \"\"\"[summary] Args: model (Optional[LinearSurrogate], optional): Linear surrogate used to calculate feature importance scores. Defaults to None. \"\"\" super().__init__() self.model = model if self.model is not None: self.model.alpha_zero() self.model.fit_intercept = True def _forward_selection(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection with forward selection, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for y. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): [description]. Defaults to 10. Raises: ValueError: The local linear model used to calculate forward_selection was not defined. Returns: np.ndarray: Indices of selected features. .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" if self.model is None: raise ValueError('forward_selection requires a local linear model') n_features = min(X.shape[1], n_features) used_features = [] for _ in range(n_features): max_ = -100000000 best = 0 for feature in range(X.shape[1]): if feature in used_features: continue self.model.fit(X[:, used_features + [feature]], y, weights=weights) score = self.model.score(X[:, used_features + [feature]], y, weights=weights) if score > max_: best = feature max_ = score used_features.append(best) return np.sort(np.array(used_features)) def _highest_weights(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection according to highest feature importance, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. Raises: ValueError: The local linear model used to calculate highest_weights was not defined. Returns: np.ndarray: Indices of selected features. .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" if self.model is None: raise ValueError('highest_weights requires a local linear model') self.model.fit(X, y, weights=weights) weighted_data = self.model.feature_importances * X[0] feature_weights = sorted( zip(range(X.shape[1]), weighted_data), key=lambda x: np.abs(x[1]), reverse=True) return np.sort(np.array([x[0] for x in feature_weights[:n_features]])) def _lasso_path(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection with `LASSO`_, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. Returns: np.ndarray: Indices of selected features. .. _LASSO: https://en.wikipedia.org/wiki/Lasso_(statistics) .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" if weights is None: weights = np.ones(X.shape[0]) weighted_data = ((X - np.average(X, axis=0, weights=weights)) * np.sqrt(weights[:, np.newaxis])) weighted_labels = ((y - np.average(y, weights=weights)) * np.sqrt(weights)) nonzero = range(weighted_data.shape[1]) _, _, coefs = lars_path(weighted_data, weighted_labels, method='lasso', verbose=False) for i in range(len(coefs.T) - 1, 0, -1): nonzero = coefs.T[i].nonzero()[0] if len(nonzero) <= n_features: break used_features = nonzero return np.sort(np.array(used_features)) def _information_criterion(self, X: np.ndarray, y: np.ndarray, criterion='aic') -> np.ndarray: \"\"\"AIC/BIC for feature selection, as used by `SHAP`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. criterion (str, optional): Whether to use `Akaike Information Criterion`_ (`aic`) or `Bayesian Information Criterion`_ (`bic`). Defaults to 'aic'. Raises: ValueError: Unknown criterion. Returns: np.ndarray: Indices of selected features. .. _SHAP: https://github.com/slundberg/shap .. _Akaike Information Criterion: https://en.wikipedia.org/wiki/Akaike_information_criterion .. _Bayesian Information Criterion: https://en.wikipedia.org/wiki/Bayesian_information_criterion \"\"\" if criterion not in ['aic', 'bic']: raise ValueError(f'Unknown criterion \"{criterion}\", choose from [aic, bic]') # use n_features if y.ndim > 1: # TODO: multiclass support? y = y[:, 0] return np.sort(np.nonzero(LassoLarsIC(criterion=criterion).fit(X, y).coef_)[0]) def _l1_reg(self, X: np.ndarray, y: np.ndarray, n_features: int = 10, alpha: Optional[float] = None) -> np.ndarray: \"\"\"L1-regularization for feature selection, as used by `SHAP`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. n_features (int, optional): Number of features to select. Defaults to 10. alpha (Optional[float], optional): Hyperparameter for L1 regularization. Defaults to None. Returns: np.ndarray: Indices of selected features. .. _SHAP: https://github.com/slundberg/shap \"\"\" if alpha is not None: return np.nonzero(Lasso(alpha=alpha).fit(X, y).coef_)[0] # use n_features if y.ndim > 1: # TODO: multiclass support? y = y[:, 0] return np.sort(lars_path(X, y, max_iter=n_features)[1]) def __call__(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10, method: Optional[str] = None, alpha: Optional[float] = None) -> np.ndarray: \"\"\"Apply feature selection for dataset X and targets y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. method (str, optional): Method to apply for feature selection, choose from `None`, `forward_selection`, `highest_weights`, `lasso_path`, `aic`, `bic`, `l1_reg`. Defaults to None. alpha (Optional[float], optional): Hyperparameter for L1 regularization. Defaults to None. Raises: ValueError: Unknown method, or the requirements of a method have not been satisfied. Returns: np.ndarray: Indices of selected features. \"\"\" if self.model is None and method in ['forward_selection', 'highest_weights']: raise ValueError(f'{self.__class__.__name__} requires a `model` to use methods forward_selection and ', 'highest_weights') if method not in [None, 'forward_selection', 'highest_weights', 'lasso_path', 'aic', 'bic', 'l1_reg']: raise ValueError(f'Unknown {method=}') n_features = min(X.shape[1], n_features) # Do not perform feature selection, but return all if n_features == X.shape[1] and method not in ['aic', 'bic', 'l1_reg'] or method is None: return np.arange(X.shape[1]) # Perform feature selection if method == 'forward_selection': return self._forward_selection(X, y, weights=weights, n_features=n_features) elif method == 'highest_weights': return self._highest_weights(X, y, weights=weights, n_features=n_features) elif method == 'lasso_path': return self._lasso_path(X, y, weights=weights, n_features=n_features) elif method in ['aic', 'bic']: return self._information_criterion(X, y, criterion=method) elif method == 'l1_reg': return self._l1_reg(X, y, n_features=n_features, alpha=alpha) def select(self, *args, **kwargs): \"\"\"Alias for `FeatureSelector().__call__()`\"\"\" return self(*args, **kwargs)","title":"FeatureSelector"},{"location":"reference/text_explainability/generation/feature_selection/#ancestors-in-mro","text":"genbase.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/feature_selection/#methods","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/feature_selection/#select","text":"1 2 3 4 5 def select ( self , * args , ** kwargs ) Alias for FeatureSelector().__call__() View Source 1 2 3 4 5 def select(self, *args, **kwargs): \"\"\"Alias for `FeatureSelector().__call__()`\"\"\" return self(*args, **kwargs)","title":"select"},{"location":"reference/text_explainability/generation/return_types/","text":"Module text_explainability.generation.return_types General return types for global/local explanations. Todo: 1 2 3 * Add rule-based explanations * Add named label support * Test for bugs View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 \"\"\"General return types for global/local explanations. Todo: * Add rule-based explanations * Add named label support * Test for bugs \"\"\" import copy from typing import Dict, Optional, Sequence, Tuple, Union import numpy as np from genbase import MetaInfo from instancelib import InstanceProvider from instancelib.typehints import LT from ..ui.notebook import Render from .surrogate import RuleSurrogate, TreeSurrogate class BaseReturnType(MetaInfo): def __init__(self, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, type: Optional[str] = 'base', subtype: Optional[str] = None, callargs: Optional[dict] = None, **kwargs): \"\"\"Base return type. Args: labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. type (Optional[str]): Type description. Defaults to 'base'. subtype (Optional[str], optional): Subtype description. Defaults to None. callargs (Optional[dict], optional): Call arguments for reproducibility. Defaults to None. **kwargs: Optional meta descriptors. \"\"\" renderer = kwargs.pop('renderer', Render) super().__init__(type=type, subtype=subtype, callargs=callargs, renderer=renderer, **kwargs) self._labels = labels self._labelset = labelset @property def labels(self): \"\"\"Get labels property.\"\"\" if self._labels is None: return self._labels return list(self._labels) @property def labelset(self): \"\"\"Get label names property.\"\"\" return self._labelset def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx def __repr__(self) -> str: labels = [self.label_by_index(label) for label in self.labels] if self.labels is not None else None if hasattr(self, 'used_features'): return f'{self.__class__.__name__}(labels={labels}, used_features={self.used_features})' return f'{self.__class__.__name__}(label={labels})' class UsedFeaturesMixin: @property def used_features(self): \"\"\"Get used features property.\"\"\" return self._used_features class FeatureList(BaseReturnType, UsedFeaturesMixin): def __init__(self, used_features: Union[Sequence[str], Sequence[int]], scores: Union[Sequence[int], Sequence[float]], labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, type: Optional[str] = 'global_explanation', subtype: Optional[str] = 'feature_list', callargs: Optional[dict] = None, **kwargs): \"\"\"Save scores per feature, grouped per label. Examples of scores are feature importance scores, or counts of features in a dataset. Args: used_features (Union[Sequence[str], Sequence[int]]): Used features per label. scores (Union[Sequence[int], Sequence[float]]): Scores per label. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. type (Optional[str]): Type description. Defaults to 'explanation'. subtype (Optional[str], optional): Subtype description. Defaults to 'feature_list'. callargs (Optional[dict], optional): Call arguments for reproducibility. Defaults to None. **kwargs: Optional meta descriptors. \"\"\" super().__init__(labels=labels, labelset=labelset, type=type, subtype=subtype, callargs=callargs, **kwargs) self._used_features = copy.deepcopy(used_features) self._scores = scores def get_raw_scores(self, normalize: bool = False) -> np.ndarray: \"\"\"Get saved scores per label as `np.ndarray`. Args: normalize (bool, optional): Normalize scores (ensure they sum to one). Defaults to False. Returns: np.ndarray: Scores. \"\"\" def feature_scores(scores): if not isinstance(scores, np.ndarray): scores = np.array(scores) if normalize: return scores / scores.sum(axis=0) return scores if isinstance(self._scores, dict): return {k: feature_scores(v) for k, v in self._scores.items()} return feature_scores(self._scores) def get_scores(self, normalize: bool = False) -> Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: \"\"\"Get scores per label. Args: normalize (bool, optional): Whether to normalize the scores (sum to one). Defaults to False. Returns: Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: Scores per label, if no `labelset` is not set, defaults to 'all' \"\"\" # TODO: change to IDs all_scores = self.get_raw_scores(normalize=normalize) if self.labels is None: return {'all': [(feature, score_) for feature, score_ in zip(self.used_features, all_scores)]} if isinstance(self.used_features, dict): return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features[label], all_scores[i])] for i, label in enumerate(self.labels)} return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features, all_scores[i])] for i, label in enumerate(self.labels)} @property def scores(self): \"\"\"Saved scores (e.g. feature importance).\"\"\" return self.get_scores(normalize=False) @property def content(self): return self.scores def __repr__(self) -> str: return '\\n'.join([f'{a}: {str(b)}' for a, b in self.scores.items()]) class LocalDataExplanation: def __init__(self, provider: InstanceProvider, original_id: Optional[LT] = None, sampled: bool = False): \"\"\"Save the sampled/generated instances used to determine an explanation. Args: provider (InstanceProvider): Sampled or generated data, including original instance. original_id (Optional[LT], optional): ID of original instance; picks first if None. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. \"\"\" self._provider = provider original_id = next(iter(self._provider)) if original_id is None else original_id self._original_instance = copy.deepcopy(self._provider[original_id]) self._neighborhood_instances = copy.deepcopy(self._provider.get_children(self._original_instance)) self.sampled = sampled @property def original_instance(self): \"\"\"The instance for which the feature attribution scores were calculated.\"\"\" return self._original_instance @property def perturbed_instances(self): \"\"\"Perturbed versions of the original instance, if `sampled=False` during initialization.\"\"\" return None if self.sampled else self._neighborhood_instances @property def sampled_instances(self): \"\"\"Sampled instances, if `sampled=True` during initialization.\"\"\" return self._neighborhood_instances if self.sampled else None @property def neighborhood_instances(self): \"\"\"Instances in the neighborhood (either sampled or perturbed).\"\"\" return self._neighborhood_instances class ReadableDataMixin: @property def used_features(self): \"\"\"Names of features of the original instance.\"\"\" if hasattr(self.original_instance, 'tokenized'): if isinstance(self._used_features, dict): return {k: [self.original_instance.tokenized[i] for i in v] for k, v in self._used_features.items()} return [self.original_instance.tokenized[i] for i in self._used_features] return list(self._used_features) def __repr__(self) -> str: sampled_or_perturbed = 'sampled' if self.sampled else 'perturbed' n = sum(1 for _ in self.neighborhood_instances) labels = [self.label_by_index(label) for label in self.labels] if self.labels is not None else None return f'{self.__class__.__name__}(labels={labels}, ' + \\ f'used_features={self.used_features}, n_{sampled_or_perturbed}_instances={n})' class FeatureAttribution(ReadableDataMixin, FeatureList, LocalDataExplanation): def __init__(self, provider: InstanceProvider, scores: Sequence[float], used_features: Optional[Union[Sequence[str], Sequence[int]]] = None, scores_stddev: Sequence[float] = None, base_score: float = None, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, original_id: Optional[LT] = None, sampled: bool = False, type: Optional[str] = 'local_explanation', subtype: Optional[str] = 'feature_attribution', callargs: Optional[dict] = None, **kwargs): \"\"\"Create a `FeatureList` with additional information saved. The additional information contains the possibility to add standard deviations, base scores, and the sampled or generated instances used to calculate these scores. Args: provider (InstanceProvider): Sampled or generated data, including original instance. scores (Sequence[float]): Scores corresponding to the selected features. used_features (Optional[Union[Sequence[str], Sequence[int]]]): Selected features for the explanation label. Defaults to None. scores_stddev (Sequence[float], optional): Standard deviation of each feature attribution score. Defaults to None. base_score (float, optional): Base score, to which all scores are relative. Defaults to None. labels (Optional[Sequence[int]], optional): Labels for outputs (e.g. classes). Defaults to None. labelset (Optional[Sequence[str]], optional): Label names corresponding to labels. Defaults to None. original_id (Optional[LT], optional): ID of original instance; picks first if None. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. type (Optional[str]): Type description. Defaults to 'base'. subtype (Optional[str], optional): Subtype description. Defaults to None. callargs (Optional[dict], optional): Call arguments for reproducibility. Defaults to None. **kwargs: Optional meta descriptors. \"\"\" LocalDataExplanation.__init__(self, provider=provider, original_id=original_id, sampled=sampled) if used_features is None: used_features = list(range(len(self.original_instance.tokenized))) FeatureList.__init__(self, used_features=used_features, scores=scores, labels=labels, labelset=labelset, type=type, subtype=subtype, callargs=callargs, **kwargs) self._base_score = base_score self._scores_stddev = scores_stddev @property def scores(self): \"\"\"Saved feature attribution scores.\"\"\" return self.get_scores(normalize=False) @property def content(self): return {'features': list(self.original_instance.tokenized), 'scores': self.scores} class Rules(ReadableDataMixin, UsedFeaturesMixin, BaseReturnType, LocalDataExplanation): def __init__(self, provider: InstanceProvider, rules: Union[Sequence[str], TreeSurrogate, RuleSurrogate], used_features: Optional[Union[Sequence[str], Sequence[int]]] = None, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, original_id: Optional[LT] = None, sampled: bool = False, type: Optional[str] = 'local_explanation', subtype: Optional[str] = 'rules', callargs: Optional[dict] = None, **kwargs): \"\"\"Rule-based return type. Args: provider (InstanceProvider): Sampled or generated data, including original instance. rules (Union[Sequence[str], TreeSurrogate, RuleSurrogate]): Rules applicable. used_features (Optional[Union[Sequence[str], Sequence[int]]]): Used features per label. Defaults to None. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. original_id (Optional[LT], optional): ID of original instance; picks first if None. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. type (Optional[str]): Type description. Defaults to 'base'. subtype (Optional[str], optional): Subtype description. Defaults to None. callargs (Optional[dict], optional): Call arguments for reproducibility. Defaults to None. **kwargs: Optional meta descriptors. \"\"\" LocalDataExplanation.__init__(self, provider=provider, original_id=original_id, sampled=sampled) BaseReturnType.__init__(self, labels=labels, labelset=labelset, type=type, subtype=subtype, callargs=callargs, **kwargs) if used_features is None: used_features = list(range(len(self.original_instance.tokenized))) self._used_features = copy.deepcopy(used_features) self._rules = self._extract_rules(rules) def _extract_rules(self, rules: Union[Sequence[str], TreeSurrogate, RuleSurrogate]): if isinstance(rules, (TreeSurrogate, RuleSurrogate)): from skrules.rule import replace_feature_name from skrules.skope_rules import BASE_FEATURE_NAME feature_dict = {BASE_FEATURE_NAME + str(i): feat for i, feat in enumerate(self.used_features)} return [(replace_feature_name(rule, feature_dict), perf) for rule, perf in rules.rules] print(rules) raise NotImplementedError('TODO: Support lists of rules') @property def rules(self): return self._rules @property def content(self): return self.rules class Instances(BaseReturnType): def __init__(self, instances, type: Optional[str] = 'global_explanation', subtype: Optional[str] = 'prototypes', callargs: Optional[dict] = None, **kwargs): super().__init__(labels=None, labelset=None, type=type, subtype=subtype, callargs=callargs, **kwargs) self.instances = instances @property def content(self): return self.instances if isinstance(self.instances, dict) else {'instances': self.instances} Classes BaseReturnType 1 2 3 4 5 6 7 8 class BaseReturnType ( labels : Optional [ Sequence [ int ]] = None , labelset : Optional [ Sequence [ str ]] = None , type : Optional [ str ] = 'base' , subtype : Optional [ str ] = None , callargs : Optional [ dict ] = None , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class BaseReturnType(MetaInfo): def __init__(self, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, type: Optional[str] = 'base', subtype: Optional[str] = None, callargs: Optional[dict] = None, **kwargs): \"\"\"Base return type. Args: labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. type (Optional[str]): Type description. Defaults to 'base'. subtype (Optional[str], optional): Subtype description. Defaults to None. callargs (Optional[dict], optional): Call arguments for reproducibility. Defaults to None. **kwargs: Optional meta descriptors. \"\"\" renderer = kwargs.pop('renderer', Render) super().__init__(type=type, subtype=subtype, callargs=callargs, renderer=renderer, **kwargs) self._labels = labels self._labelset = labelset @property def labels(self): \"\"\"Get labels property.\"\"\" if self._labels is None: return self._labels return list(self._labels) @property def labelset(self): \"\"\"Get label names property.\"\"\" return self._labelset def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx def __repr__(self) -> str: labels = [self.label_by_index(label) for label in self.labels] if self.labels is not None else None if hasattr(self, 'used_features'): return f'{self.__class__.__name__}(labels={labels}, used_features={self.used_features})' return f'{self.__class__.__name__}(label={labels})' Ancestors (in MRO) genbase.MetaInfo genbase.Configurable Descendants text_explainability.generation.return_types.FeatureList text_explainability.generation.return_types.Rules text_explainability.generation.return_types.Instances Static methods from_config 1 2 3 4 def from_config ( config : dict , ** kwargs ) -> 'Configurable' View Source 1 2 3 4 5 6 7 8 9 @classmethod def from_config(cls, config: dict, **kwargs) -> 'Configurable': config = {**config, **kwargs} _ = config.pop('__class__', None) return cls(**config) from_json 1 2 3 4 def from_json ( json_or_path : str , ** read_args ) -> 'Configurable' Get config from JSON string or filepath. Parameters: Name Type Description Default json_or_path str File path or JSON string. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @classmethod def from_json(cls, json_or_path: str, **read_args) -> 'Configurable': \"\"\"Get config from JSON string or filepath. Args: json_or_path (str): File path or JSON string. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" if Path.is_file(json_or_path): return cls.read_json(json_or_path, **read_args) return cls.from_config(srsly.json_loads(json_or_path)) from_yaml 1 2 3 def from_yaml ( yaml_or_path : str ) -> 'Configurable' Get config from YAML string or filepath. Parameters: Name Type Description Default yaml_or_path str File path or YAML string. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @classmethod def from_yaml(cls, yaml_or_path: str) -> 'Configurable': \"\"\"Get config from YAML string or filepath. Args: yaml_or_path (str): File path or YAML string. \"\"\" if Path.is_file(yaml_or_path): return cls.read_yaml(yaml_or_path) return cls.from_config(srsly.yaml_loads(yaml_or_path)) read_json 1 2 3 4 def read_json ( path : str , ** read_args ) -> 'Configurable' Read config from JSON file (GZIP JSON, JSONL or JSON). Parameters: Name Type Description Default path str File path. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @classmethod def read_json(cls, path: str, **read_args) -> 'Configurable': \"\"\"Read config from JSON file (GZIP JSON, JSONL or JSON). Args: path (str): File path. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" read_fn = srsly.read_json if path.endswith('.json.gz'): read_fn = srsly.read_gzip_json elif path.endswith('.jsonl'): read_fn = srsly.read_jsonl return cls.from_config(read_fn(path, **read_args)) read_yaml 1 2 3 def read_yaml ( path : str ) -> 'Configurable' Read config from YAML file. Parameters: Name Type Description Default path str File path. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 @classmethod def read_yaml(cls, path: str) -> 'Configurable': \"\"\"Read config from YAML file. Args: path (str): File path. \"\"\" return cls.from_config(srsly.read_yaml(path)) Instance variables 1 callargs 1 labels Get labels property. 1 labelset Get label names property. 1 meta 1 renderargs 1 subtype 1 type Methods label_by_index 1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx to_config 1 2 3 def to_config ( self ) Convert class information into config (configuration dictionary). Parameters: Name Type Description Default exclude List[str] Names of variables to exclude. None Returns: Type Description dict [description] View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def to_config(self): if hasattr(self, 'content'): _content = self.content() if callable(self.content) else self.content content = dict(recursive_to_dict(_content, include_class=False)) else: content = super().to_config(exclude=['_type', '_subtype', '_dict', '_callargs']) return {'META': self.meta, 'CONTENT': content} to_json 1 2 3 4 def to_json ( self , indent : int = 2 ) -> str Convert config to JSON-formatted string. Parameters: Name Type Description Default indent int Number of spaces to indent JSON. Defaults to 2. 2 Returns: Type Description str Config formatted as JSON. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_json(self, indent: int = 2) -> str: \"\"\"Convert config to JSON-formatted string. Args: indent (int, optional): Number of spaces to indent JSON. Defaults to 2. Returns: str: Config formatted as JSON. \"\"\" return srsly.json_dumps(self.to_config(), indent=indent) to_yaml 1 2 3 4 def to_yaml ( self , ** write_args ) -> str Convert config to YAML-formatted string. Parameters: Name Type Description Default **write_args None Optional arguments passed to srsly.yaml_dumps() None Returns: Type Description str Config formatted as YAML. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_yaml(self, **write_args) -> str: \"\"\"Convert config to YAML-formatted string. Args: **write_args: Optional arguments passed to `srsly.yaml_dumps()` Returns: str: Config formatted as YAML. \"\"\" return srsly.yaml_dumps(self.to_config(), **write_args) write_json 1 2 3 4 5 def write_json ( self , path : str , indent : int = 2 ) -> None Write class config to JSON. Parameters: Name Type Description Default path str Path to save to. If ends in .json.gz saves as GZIP JSON, .jsonl as JSONL or JSON by default. None indent int Number of spaces to indent JSON. Defaults to 2. 2 View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def write_json(self, path: str, indent: int = 2) -> None: \"\"\"Write class config to JSON. Args: path (str): Path to save to. If ends in `.json.gz` saves as GZIP JSON, `.jsonl` as JSONL or JSON by default. indent (int, optional): Number of spaces to indent JSON. Defaults to 2. \"\"\" write_fn = srsly.write_json if path.endswith('.json.gz'): write_fn = srsly.write_gzip_json elif path.endswith('.jsonl'): write_fn = srsly.write_jsonl write_fn(path, self.to_config(), indent=indent) write_yaml 1 2 3 4 5 def write_yaml ( self , path : str , ** write_args ) -> None Write class config to YAML. Parameters: Name Type Description Default path str Path to save to. None **write_args None Optional arguments passed to srsly.write_yaml() None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def write_yaml(self, path: str, **write_args) -> None: \"\"\"Write class config to YAML. Args: path (str): Path to save to. **write_args: Optional arguments passed to `srsly.write_yaml()` \"\"\" srsly.write_yaml(path, self.to_config(), **write_args) FeatureAttribution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class FeatureAttribution ( provider : instancelib . instances . base . InstanceProvider , scores : Sequence [ float ], used_features : Union [ Sequence [ str ], Sequence [ int ], NoneType ] = None , scores_stddev : Sequence [ float ] = None , base_score : float = None , labels : Optional [ Sequence [ int ]] = None , labelset : Optional [ Sequence [ str ]] = None , original_id : Optional [ ~ LT ] = None , sampled : bool = False , type : Optional [ str ] = 'local_explanation' , subtype : Optional [ str ] = 'feature_attribution' , callargs : Optional [ dict ] = None , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 class FeatureAttribution(ReadableDataMixin, FeatureList, LocalDataExplanation): def __init__(self, provider: InstanceProvider, scores: Sequence[float], used_features: Optional[Union[Sequence[str], Sequence[int]]] = None, scores_stddev: Sequence[float] = None, base_score: float = None, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, original_id: Optional[LT] = None, sampled: bool = False, type: Optional[str] = 'local_explanation', subtype: Optional[str] = 'feature_attribution', callargs: Optional[dict] = None, **kwargs): \"\"\"Create a `FeatureList` with additional information saved. The additional information contains the possibility to add standard deviations, base scores, and the sampled or generated instances used to calculate these scores. Args: provider (InstanceProvider): Sampled or generated data, including original instance. scores (Sequence[float]): Scores corresponding to the selected features. used_features (Optional[Union[Sequence[str], Sequence[int]]]): Selected features for the explanation label. Defaults to None. scores_stddev (Sequence[float], optional): Standard deviation of each feature attribution score. Defaults to None. base_score (float, optional): Base score, to which all scores are relative. Defaults to None. labels (Optional[Sequence[int]], optional): Labels for outputs (e.g. classes). Defaults to None. labelset (Optional[Sequence[str]], optional): Label names corresponding to labels. Defaults to None. original_id (Optional[LT], optional): ID of original instance; picks first if None. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. type (Optional[str]): Type description. Defaults to 'base'. subtype (Optional[str], optional): Subtype description. Defaults to None. callargs (Optional[dict], optional): Call arguments for reproducibility. Defaults to None. **kwargs: Optional meta descriptors. \"\"\" LocalDataExplanation.__init__(self, provider=provider, original_id=original_id, sampled=sampled) if used_features is None: used_features = list(range(len(self.original_instance.tokenized))) FeatureList.__init__(self, used_features=used_features, scores=scores, labels=labels, labelset=labelset, type=type, subtype=subtype, callargs=callargs, **kwargs) self._base_score = base_score self._scores_stddev = scores_stddev @property def scores(self): \"\"\"Saved feature attribution scores.\"\"\" return self.get_scores(normalize=False) @property def content(self): return {'features': list(self.original_instance.tokenized), 'scores': self.scores} Ancestors (in MRO) text_explainability.generation.return_types.ReadableDataMixin text_explainability.generation.return_types.FeatureList text_explainability.generation.return_types.BaseReturnType genbase.MetaInfo genbase.Configurable text_explainability.generation.return_types.UsedFeaturesMixin text_explainability.generation.return_types.LocalDataExplanation Static methods from_config 1 2 3 4 def from_config ( config : dict , ** kwargs ) -> 'Configurable' View Source 1 2 3 4 5 6 7 8 9 @classmethod def from_config(cls, config: dict, **kwargs) -> 'Configurable': config = {**config, **kwargs} _ = config.pop('__class__', None) return cls(**config) from_json 1 2 3 4 def from_json ( json_or_path : str , ** read_args ) -> 'Configurable' Get config from JSON string or filepath. Parameters: Name Type Description Default json_or_path str File path or JSON string. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @classmethod def from_json(cls, json_or_path: str, **read_args) -> 'Configurable': \"\"\"Get config from JSON string or filepath. Args: json_or_path (str): File path or JSON string. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" if Path.is_file(json_or_path): return cls.read_json(json_or_path, **read_args) return cls.from_config(srsly.json_loads(json_or_path)) from_yaml 1 2 3 def from_yaml ( yaml_or_path : str ) -> 'Configurable' Get config from YAML string or filepath. Parameters: Name Type Description Default yaml_or_path str File path or YAML string. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @classmethod def from_yaml(cls, yaml_or_path: str) -> 'Configurable': \"\"\"Get config from YAML string or filepath. Args: yaml_or_path (str): File path or YAML string. \"\"\" if Path.is_file(yaml_or_path): return cls.read_yaml(yaml_or_path) return cls.from_config(srsly.yaml_loads(yaml_or_path)) read_json 1 2 3 4 def read_json ( path : str , ** read_args ) -> 'Configurable' Read config from JSON file (GZIP JSON, JSONL or JSON). Parameters: Name Type Description Default path str File path. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @classmethod def read_json(cls, path: str, **read_args) -> 'Configurable': \"\"\"Read config from JSON file (GZIP JSON, JSONL or JSON). Args: path (str): File path. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" read_fn = srsly.read_json if path.endswith('.json.gz'): read_fn = srsly.read_gzip_json elif path.endswith('.jsonl'): read_fn = srsly.read_jsonl return cls.from_config(read_fn(path, **read_args)) read_yaml 1 2 3 def read_yaml ( path : str ) -> 'Configurable' Read config from YAML file. Parameters: Name Type Description Default path str File path. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 @classmethod def read_yaml(cls, path: str) -> 'Configurable': \"\"\"Read config from YAML file. Args: path (str): File path. \"\"\" return cls.from_config(srsly.read_yaml(path)) Instance variables 1 callargs 1 content 1 labels Get labels property. 1 labelset Get label names property. 1 meta 1 neighborhood_instances Instances in the neighborhood (either sampled or perturbed). 1 original_instance The instance for which the feature attribution scores were calculated. 1 perturbed_instances Perturbed versions of the original instance, if sampled=False during initialization. 1 renderargs 1 sampled_instances Sampled instances, if sampled=True during initialization. 1 scores Saved feature attribution scores. 1 subtype 1 type 1 used_features Names of features of the original instance. Methods get_raw_scores 1 2 3 4 def get_raw_scores ( self , normalize : bool = False ) -> numpy . ndarray Get saved scores per label as np.ndarray . Parameters: Name Type Description Default normalize bool Normalize scores (ensure they sum to one). Defaults to False. False Returns: Type Description np.ndarray Scores. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def get_raw_scores(self, normalize: bool = False) -> np.ndarray: \"\"\"Get saved scores per label as `np.ndarray`. Args: normalize (bool, optional): Normalize scores (ensure they sum to one). Defaults to False. Returns: np.ndarray: Scores. \"\"\" def feature_scores(scores): if not isinstance(scores, np.ndarray): scores = np.array(scores) if normalize: return scores / scores.sum(axis=0) return scores if isinstance(self._scores, dict): return {k: feature_scores(v) for k, v in self._scores.items()} return feature_scores(self._scores) get_scores 1 2 3 4 def get_scores ( self , normalize : bool = False ) -> Dict [ Union [ str , int ], Tuple [ Union [ str , int ], Union [ float , int ]]] Get scores per label. Parameters: Name Type Description Default normalize bool Whether to normalize the scores (sum to one). Defaults to False. False Returns: Type Description Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]] Scores per label, if no labelset is not set, defaults to 'all' View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def get_scores(self, normalize: bool = False) -> Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: \"\"\"Get scores per label. Args: normalize (bool, optional): Whether to normalize the scores (sum to one). Defaults to False. Returns: Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: Scores per label, if no `labelset` is not set, defaults to 'all' \"\"\" # TODO: change to IDs all_scores = self.get_raw_scores(normalize=normalize) if self.labels is None: return {'all': [(feature, score_) for feature, score_ in zip(self.used_features, all_scores)]} if isinstance(self.used_features, dict): return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features[label], all_scores[i])] for i, label in enumerate(self.labels)} return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features, all_scores[i])] for i, label in enumerate(self.labels)} label_by_index 1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx to_config 1 2 3 def to_config ( self ) Convert class information into config (configuration dictionary). Parameters: Name Type Description Default exclude List[str] Names of variables to exclude. None Returns: Type Description dict [description] View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def to_config(self): if hasattr(self, 'content'): _content = self.content() if callable(self.content) else self.content content = dict(recursive_to_dict(_content, include_class=False)) else: content = super().to_config(exclude=['_type', '_subtype', '_dict', '_callargs']) return {'META': self.meta, 'CONTENT': content} to_json 1 2 3 4 def to_json ( self , indent : int = 2 ) -> str Convert config to JSON-formatted string. Parameters: Name Type Description Default indent int Number of spaces to indent JSON. Defaults to 2. 2 Returns: Type Description str Config formatted as JSON. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_json(self, indent: int = 2) -> str: \"\"\"Convert config to JSON-formatted string. Args: indent (int, optional): Number of spaces to indent JSON. Defaults to 2. Returns: str: Config formatted as JSON. \"\"\" return srsly.json_dumps(self.to_config(), indent=indent) to_yaml 1 2 3 4 def to_yaml ( self , ** write_args ) -> str Convert config to YAML-formatted string. Parameters: Name Type Description Default **write_args None Optional arguments passed to srsly.yaml_dumps() None Returns: Type Description str Config formatted as YAML. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_yaml(self, **write_args) -> str: \"\"\"Convert config to YAML-formatted string. Args: **write_args: Optional arguments passed to `srsly.yaml_dumps()` Returns: str: Config formatted as YAML. \"\"\" return srsly.yaml_dumps(self.to_config(), **write_args) write_json 1 2 3 4 5 def write_json ( self , path : str , indent : int = 2 ) -> None Write class config to JSON. Parameters: Name Type Description Default path str Path to save to. If ends in .json.gz saves as GZIP JSON, .jsonl as JSONL or JSON by default. None indent int Number of spaces to indent JSON. Defaults to 2. 2 View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def write_json(self, path: str, indent: int = 2) -> None: \"\"\"Write class config to JSON. Args: path (str): Path to save to. If ends in `.json.gz` saves as GZIP JSON, `.jsonl` as JSONL or JSON by default. indent (int, optional): Number of spaces to indent JSON. Defaults to 2. \"\"\" write_fn = srsly.write_json if path.endswith('.json.gz'): write_fn = srsly.write_gzip_json elif path.endswith('.jsonl'): write_fn = srsly.write_jsonl write_fn(path, self.to_config(), indent=indent) write_yaml 1 2 3 4 5 def write_yaml ( self , path : str , ** write_args ) -> None Write class config to YAML. Parameters: Name Type Description Default path str Path to save to. None **write_args None Optional arguments passed to srsly.write_yaml() None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def write_yaml(self, path: str, **write_args) -> None: \"\"\"Write class config to YAML. Args: path (str): Path to save to. **write_args: Optional arguments passed to `srsly.write_yaml()` \"\"\" srsly.write_yaml(path, self.to_config(), **write_args) FeatureList 1 2 3 4 5 6 7 8 9 10 class FeatureList ( used_features : Union [ Sequence [ str ], Sequence [ int ]], scores : Union [ Sequence [ int ], Sequence [ float ]], labels : Optional [ Sequence [ int ]] = None , labelset : Optional [ Sequence [ str ]] = None , type : Optional [ str ] = 'global_explanation' , subtype : Optional [ str ] = 'feature_list' , callargs : Optional [ dict ] = None , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 class FeatureList(BaseReturnType, UsedFeaturesMixin): def __init__(self, used_features: Union[Sequence[str], Sequence[int]], scores: Union[Sequence[int], Sequence[float]], labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, type: Optional[str] = 'global_explanation', subtype: Optional[str] = 'feature_list', callargs: Optional[dict] = None, **kwargs): \"\"\"Save scores per feature, grouped per label. Examples of scores are feature importance scores, or counts of features in a dataset. Args: used_features (Union[Sequence[str], Sequence[int]]): Used features per label. scores (Union[Sequence[int], Sequence[float]]): Scores per label. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. type (Optional[str]): Type description. Defaults to 'explanation'. subtype (Optional[str], optional): Subtype description. Defaults to 'feature_list'. callargs (Optional[dict], optional): Call arguments for reproducibility. Defaults to None. **kwargs: Optional meta descriptors. \"\"\" super().__init__(labels=labels, labelset=labelset, type=type, subtype=subtype, callargs=callargs, **kwargs) self._used_features = copy.deepcopy(used_features) self._scores = scores def get_raw_scores(self, normalize: bool = False) -> np.ndarray: \"\"\"Get saved scores per label as `np.ndarray`. Args: normalize (bool, optional): Normalize scores (ensure they sum to one). Defaults to False. Returns: np.ndarray: Scores. \"\"\" def feature_scores(scores): if not isinstance(scores, np.ndarray): scores = np.array(scores) if normalize: return scores / scores.sum(axis=0) return scores if isinstance(self._scores, dict): return {k: feature_scores(v) for k, v in self._scores.items()} return feature_scores(self._scores) def get_scores(self, normalize: bool = False) -> Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: \"\"\"Get scores per label. Args: normalize (bool, optional): Whether to normalize the scores (sum to one). Defaults to False. Returns: Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: Scores per label, if no `labelset` is not set, defaults to 'all' \"\"\" # TODO: change to IDs all_scores = self.get_raw_scores(normalize=normalize) if self.labels is None: return {'all': [(feature, score_) for feature, score_ in zip(self.used_features, all_scores)]} if isinstance(self.used_features, dict): return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features[label], all_scores[i])] for i, label in enumerate(self.labels)} return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features, all_scores[i])] for i, label in enumerate(self.labels)} @property def scores(self): \"\"\"Saved scores (e.g. feature importance).\"\"\" return self.get_scores(normalize=False) @property def content(self): return self.scores def __repr__(self) -> str: return '\\n'.join([f'{a}: {str(b)}' for a, b in self.scores.items()]) Ancestors (in MRO) text_explainability.generation.return_types.BaseReturnType genbase.MetaInfo genbase.Configurable text_explainability.generation.return_types.UsedFeaturesMixin Descendants text_explainability.generation.return_types.FeatureAttribution Static methods from_config 1 2 3 4 def from_config ( config : dict , ** kwargs ) -> 'Configurable' View Source 1 2 3 4 5 6 7 8 9 @classmethod def from_config(cls, config: dict, **kwargs) -> 'Configurable': config = {**config, **kwargs} _ = config.pop('__class__', None) return cls(**config) from_json 1 2 3 4 def from_json ( json_or_path : str , ** read_args ) -> 'Configurable' Get config from JSON string or filepath. Parameters: Name Type Description Default json_or_path str File path or JSON string. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @classmethod def from_json(cls, json_or_path: str, **read_args) -> 'Configurable': \"\"\"Get config from JSON string or filepath. Args: json_or_path (str): File path or JSON string. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" if Path.is_file(json_or_path): return cls.read_json(json_or_path, **read_args) return cls.from_config(srsly.json_loads(json_or_path)) from_yaml 1 2 3 def from_yaml ( yaml_or_path : str ) -> 'Configurable' Get config from YAML string or filepath. Parameters: Name Type Description Default yaml_or_path str File path or YAML string. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @classmethod def from_yaml(cls, yaml_or_path: str) -> 'Configurable': \"\"\"Get config from YAML string or filepath. Args: yaml_or_path (str): File path or YAML string. \"\"\" if Path.is_file(yaml_or_path): return cls.read_yaml(yaml_or_path) return cls.from_config(srsly.yaml_loads(yaml_or_path)) read_json 1 2 3 4 def read_json ( path : str , ** read_args ) -> 'Configurable' Read config from JSON file (GZIP JSON, JSONL or JSON). Parameters: Name Type Description Default path str File path. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @classmethod def read_json(cls, path: str, **read_args) -> 'Configurable': \"\"\"Read config from JSON file (GZIP JSON, JSONL or JSON). Args: path (str): File path. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" read_fn = srsly.read_json if path.endswith('.json.gz'): read_fn = srsly.read_gzip_json elif path.endswith('.jsonl'): read_fn = srsly.read_jsonl return cls.from_config(read_fn(path, **read_args)) read_yaml 1 2 3 def read_yaml ( path : str ) -> 'Configurable' Read config from YAML file. Parameters: Name Type Description Default path str File path. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 @classmethod def read_yaml(cls, path: str) -> 'Configurable': \"\"\"Read config from YAML file. Args: path (str): File path. \"\"\" return cls.from_config(srsly.read_yaml(path)) Instance variables 1 callargs 1 content 1 labels Get labels property. 1 labelset Get label names property. 1 meta 1 renderargs 1 scores Saved scores (e.g. feature importance). 1 subtype 1 type 1 used_features Get used features property. Methods get_raw_scores 1 2 3 4 def get_raw_scores ( self , normalize : bool = False ) -> numpy . ndarray Get saved scores per label as np.ndarray . Parameters: Name Type Description Default normalize bool Normalize scores (ensure they sum to one). Defaults to False. False Returns: Type Description np.ndarray Scores. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def get_raw_scores(self, normalize: bool = False) -> np.ndarray: \"\"\"Get saved scores per label as `np.ndarray`. Args: normalize (bool, optional): Normalize scores (ensure they sum to one). Defaults to False. Returns: np.ndarray: Scores. \"\"\" def feature_scores(scores): if not isinstance(scores, np.ndarray): scores = np.array(scores) if normalize: return scores / scores.sum(axis=0) return scores if isinstance(self._scores, dict): return {k: feature_scores(v) for k, v in self._scores.items()} return feature_scores(self._scores) get_scores 1 2 3 4 def get_scores ( self , normalize : bool = False ) -> Dict [ Union [ str , int ], Tuple [ Union [ str , int ], Union [ float , int ]]] Get scores per label. Parameters: Name Type Description Default normalize bool Whether to normalize the scores (sum to one). Defaults to False. False Returns: Type Description Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]] Scores per label, if no labelset is not set, defaults to 'all' View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def get_scores(self, normalize: bool = False) -> Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: \"\"\"Get scores per label. Args: normalize (bool, optional): Whether to normalize the scores (sum to one). Defaults to False. Returns: Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: Scores per label, if no `labelset` is not set, defaults to 'all' \"\"\" # TODO: change to IDs all_scores = self.get_raw_scores(normalize=normalize) if self.labels is None: return {'all': [(feature, score_) for feature, score_ in zip(self.used_features, all_scores)]} if isinstance(self.used_features, dict): return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features[label], all_scores[i])] for i, label in enumerate(self.labels)} return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features, all_scores[i])] for i, label in enumerate(self.labels)} label_by_index 1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx to_config 1 2 3 def to_config ( self ) Convert class information into config (configuration dictionary). Parameters: Name Type Description Default exclude List[str] Names of variables to exclude. None Returns: Type Description dict [description] View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def to_config(self): if hasattr(self, 'content'): _content = self.content() if callable(self.content) else self.content content = dict(recursive_to_dict(_content, include_class=False)) else: content = super().to_config(exclude=['_type', '_subtype', '_dict', '_callargs']) return {'META': self.meta, 'CONTENT': content} to_json 1 2 3 4 def to_json ( self , indent : int = 2 ) -> str Convert config to JSON-formatted string. Parameters: Name Type Description Default indent int Number of spaces to indent JSON. Defaults to 2. 2 Returns: Type Description str Config formatted as JSON. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_json(self, indent: int = 2) -> str: \"\"\"Convert config to JSON-formatted string. Args: indent (int, optional): Number of spaces to indent JSON. Defaults to 2. Returns: str: Config formatted as JSON. \"\"\" return srsly.json_dumps(self.to_config(), indent=indent) to_yaml 1 2 3 4 def to_yaml ( self , ** write_args ) -> str Convert config to YAML-formatted string. Parameters: Name Type Description Default **write_args None Optional arguments passed to srsly.yaml_dumps() None Returns: Type Description str Config formatted as YAML. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_yaml(self, **write_args) -> str: \"\"\"Convert config to YAML-formatted string. Args: **write_args: Optional arguments passed to `srsly.yaml_dumps()` Returns: str: Config formatted as YAML. \"\"\" return srsly.yaml_dumps(self.to_config(), **write_args) write_json 1 2 3 4 5 def write_json ( self , path : str , indent : int = 2 ) -> None Write class config to JSON. Parameters: Name Type Description Default path str Path to save to. If ends in .json.gz saves as GZIP JSON, .jsonl as JSONL or JSON by default. None indent int Number of spaces to indent JSON. Defaults to 2. 2 View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def write_json(self, path: str, indent: int = 2) -> None: \"\"\"Write class config to JSON. Args: path (str): Path to save to. If ends in `.json.gz` saves as GZIP JSON, `.jsonl` as JSONL or JSON by default. indent (int, optional): Number of spaces to indent JSON. Defaults to 2. \"\"\" write_fn = srsly.write_json if path.endswith('.json.gz'): write_fn = srsly.write_gzip_json elif path.endswith('.jsonl'): write_fn = srsly.write_jsonl write_fn(path, self.to_config(), indent=indent) write_yaml 1 2 3 4 5 def write_yaml ( self , path : str , ** write_args ) -> None Write class config to YAML. Parameters: Name Type Description Default path str Path to save to. None **write_args None Optional arguments passed to srsly.write_yaml() None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def write_yaml(self, path: str, **write_args) -> None: \"\"\"Write class config to YAML. Args: path (str): Path to save to. **write_args: Optional arguments passed to `srsly.write_yaml()` \"\"\" srsly.write_yaml(path, self.to_config(), **write_args) Instances 1 2 3 4 5 6 7 class Instances ( instances , type : Optional [ str ] = 'global_explanation' , subtype : Optional [ str ] = 'prototypes' , callargs : Optional [ dict ] = None , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class Instances(BaseReturnType): def __init__(self, instances, type: Optional[str] = 'global_explanation', subtype: Optional[str] = 'prototypes', callargs: Optional[dict] = None, **kwargs): super().__init__(labels=None, labelset=None, type=type, subtype=subtype, callargs=callargs, **kwargs) self.instances = instances @property def content(self): return self.instances if isinstance(self.instances, dict) else {'instances': self.instances} Ancestors (in MRO) text_explainability.generation.return_types.BaseReturnType genbase.MetaInfo genbase.Configurable Static methods from_config 1 2 3 4 def from_config ( config : dict , ** kwargs ) -> 'Configurable' View Source 1 2 3 4 5 6 7 8 9 @classmethod def from_config(cls, config: dict, **kwargs) -> 'Configurable': config = {**config, **kwargs} _ = config.pop('__class__', None) return cls(**config) from_json 1 2 3 4 def from_json ( json_or_path : str , ** read_args ) -> 'Configurable' Get config from JSON string or filepath. Parameters: Name Type Description Default json_or_path str File path or JSON string. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @classmethod def from_json(cls, json_or_path: str, **read_args) -> 'Configurable': \"\"\"Get config from JSON string or filepath. Args: json_or_path (str): File path or JSON string. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" if Path.is_file(json_or_path): return cls.read_json(json_or_path, **read_args) return cls.from_config(srsly.json_loads(json_or_path)) from_yaml 1 2 3 def from_yaml ( yaml_or_path : str ) -> 'Configurable' Get config from YAML string or filepath. Parameters: Name Type Description Default yaml_or_path str File path or YAML string. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @classmethod def from_yaml(cls, yaml_or_path: str) -> 'Configurable': \"\"\"Get config from YAML string or filepath. Args: yaml_or_path (str): File path or YAML string. \"\"\" if Path.is_file(yaml_or_path): return cls.read_yaml(yaml_or_path) return cls.from_config(srsly.yaml_loads(yaml_or_path)) read_json 1 2 3 4 def read_json ( path : str , ** read_args ) -> 'Configurable' Read config from JSON file (GZIP JSON, JSONL or JSON). Parameters: Name Type Description Default path str File path. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @classmethod def read_json(cls, path: str, **read_args) -> 'Configurable': \"\"\"Read config from JSON file (GZIP JSON, JSONL or JSON). Args: path (str): File path. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" read_fn = srsly.read_json if path.endswith('.json.gz'): read_fn = srsly.read_gzip_json elif path.endswith('.jsonl'): read_fn = srsly.read_jsonl return cls.from_config(read_fn(path, **read_args)) read_yaml 1 2 3 def read_yaml ( path : str ) -> 'Configurable' Read config from YAML file. Parameters: Name Type Description Default path str File path. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 @classmethod def read_yaml(cls, path: str) -> 'Configurable': \"\"\"Read config from YAML file. Args: path (str): File path. \"\"\" return cls.from_config(srsly.read_yaml(path)) Instance variables 1 callargs 1 content 1 labels Get labels property. 1 labelset Get label names property. 1 meta 1 renderargs 1 subtype 1 type Methods label_by_index 1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx to_config 1 2 3 def to_config ( self ) Convert class information into config (configuration dictionary). Parameters: Name Type Description Default exclude List[str] Names of variables to exclude. None Returns: Type Description dict [description] View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def to_config(self): if hasattr(self, 'content'): _content = self.content() if callable(self.content) else self.content content = dict(recursive_to_dict(_content, include_class=False)) else: content = super().to_config(exclude=['_type', '_subtype', '_dict', '_callargs']) return {'META': self.meta, 'CONTENT': content} to_json 1 2 3 4 def to_json ( self , indent : int = 2 ) -> str Convert config to JSON-formatted string. Parameters: Name Type Description Default indent int Number of spaces to indent JSON. Defaults to 2. 2 Returns: Type Description str Config formatted as JSON. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_json(self, indent: int = 2) -> str: \"\"\"Convert config to JSON-formatted string. Args: indent (int, optional): Number of spaces to indent JSON. Defaults to 2. Returns: str: Config formatted as JSON. \"\"\" return srsly.json_dumps(self.to_config(), indent=indent) to_yaml 1 2 3 4 def to_yaml ( self , ** write_args ) -> str Convert config to YAML-formatted string. Parameters: Name Type Description Default **write_args None Optional arguments passed to srsly.yaml_dumps() None Returns: Type Description str Config formatted as YAML. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_yaml(self, **write_args) -> str: \"\"\"Convert config to YAML-formatted string. Args: **write_args: Optional arguments passed to `srsly.yaml_dumps()` Returns: str: Config formatted as YAML. \"\"\" return srsly.yaml_dumps(self.to_config(), **write_args) write_json 1 2 3 4 5 def write_json ( self , path : str , indent : int = 2 ) -> None Write class config to JSON. Parameters: Name Type Description Default path str Path to save to. If ends in .json.gz saves as GZIP JSON, .jsonl as JSONL or JSON by default. None indent int Number of spaces to indent JSON. Defaults to 2. 2 View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def write_json(self, path: str, indent: int = 2) -> None: \"\"\"Write class config to JSON. Args: path (str): Path to save to. If ends in `.json.gz` saves as GZIP JSON, `.jsonl` as JSONL or JSON by default. indent (int, optional): Number of spaces to indent JSON. Defaults to 2. \"\"\" write_fn = srsly.write_json if path.endswith('.json.gz'): write_fn = srsly.write_gzip_json elif path.endswith('.jsonl'): write_fn = srsly.write_jsonl write_fn(path, self.to_config(), indent=indent) write_yaml 1 2 3 4 5 def write_yaml ( self , path : str , ** write_args ) -> None Write class config to YAML. Parameters: Name Type Description Default path str Path to save to. None **write_args None Optional arguments passed to srsly.write_yaml() None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def write_yaml(self, path: str, **write_args) -> None: \"\"\"Write class config to YAML. Args: path (str): Path to save to. **write_args: Optional arguments passed to `srsly.write_yaml()` \"\"\" srsly.write_yaml(path, self.to_config(), **write_args) LocalDataExplanation 1 2 3 4 5 class LocalDataExplanation ( provider : instancelib . instances . base . InstanceProvider , original_id : Optional [ ~ LT ] = None , sampled : bool = False ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class LocalDataExplanation: def __init__(self, provider: InstanceProvider, original_id: Optional[LT] = None, sampled: bool = False): \"\"\"Save the sampled/generated instances used to determine an explanation. Args: provider (InstanceProvider): Sampled or generated data, including original instance. original_id (Optional[LT], optional): ID of original instance; picks first if None. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. \"\"\" self._provider = provider original_id = next(iter(self._provider)) if original_id is None else original_id self._original_instance = copy.deepcopy(self._provider[original_id]) self._neighborhood_instances = copy.deepcopy(self._provider.get_children(self._original_instance)) self.sampled = sampled @property def original_instance(self): \"\"\"The instance for which the feature attribution scores were calculated.\"\"\" return self._original_instance @property def perturbed_instances(self): \"\"\"Perturbed versions of the original instance, if `sampled=False` during initialization.\"\"\" return None if self.sampled else self._neighborhood_instances @property def sampled_instances(self): \"\"\"Sampled instances, if `sampled=True` during initialization.\"\"\" return self._neighborhood_instances if self.sampled else None @property def neighborhood_instances(self): \"\"\"Instances in the neighborhood (either sampled or perturbed).\"\"\" return self._neighborhood_instances Descendants text_explainability.generation.return_types.FeatureAttribution text_explainability.generation.return_types.Rules Instance variables 1 neighborhood_instances Instances in the neighborhood (either sampled or perturbed). 1 original_instance The instance for which the feature attribution scores were calculated. 1 perturbed_instances Perturbed versions of the original instance, if sampled=False during initialization. 1 sampled_instances Sampled instances, if sampled=True during initialization. ReadableDataMixin 1 2 3 4 5 class ReadableDataMixin ( / , * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class ReadableDataMixin: @property def used_features(self): \"\"\"Names of features of the original instance.\"\"\" if hasattr(self.original_instance, 'tokenized'): if isinstance(self._used_features, dict): return {k: [self.original_instance.tokenized[i] for i in v] for k, v in self._used_features.items()} return [self.original_instance.tokenized[i] for i in self._used_features] return list(self._used_features) def __repr__(self) -> str: sampled_or_perturbed = 'sampled' if self.sampled else 'perturbed' n = sum(1 for _ in self.neighborhood_instances) labels = [self.label_by_index(label) for label in self.labels] if self.labels is not None else None return f'{self.__class__.__name__}(labels={labels}, ' + \\ f'used_features={self.used_features}, n_{sampled_or_perturbed}_instances={n})' Descendants text_explainability.generation.return_types.FeatureAttribution text_explainability.generation.return_types.Rules Instance variables 1 used_features Names of features of the original instance. Rules 1 2 3 4 5 6 7 8 9 10 11 12 13 class Rules ( provider : instancelib . instances . base . InstanceProvider , rules : Union [ Sequence [ str ], text_explainability . generation . surrogate . TreeSurrogate , text_explainability . generation . surrogate . RuleSurrogate ], used_features : Union [ Sequence [ str ], Sequence [ int ], NoneType ] = None , labels : Optional [ Sequence [ int ]] = None , labelset : Optional [ Sequence [ str ]] = None , original_id : Optional [ ~ LT ] = None , sampled : bool = False , type : Optional [ str ] = 'local_explanation' , subtype : Optional [ str ] = 'rules' , callargs : Optional [ dict ] = None , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class Rules(ReadableDataMixin, UsedFeaturesMixin, BaseReturnType, LocalDataExplanation): def __init__(self, provider: InstanceProvider, rules: Union[Sequence[str], TreeSurrogate, RuleSurrogate], used_features: Optional[Union[Sequence[str], Sequence[int]]] = None, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, original_id: Optional[LT] = None, sampled: bool = False, type: Optional[str] = 'local_explanation', subtype: Optional[str] = 'rules', callargs: Optional[dict] = None, **kwargs): \"\"\"Rule-based return type. Args: provider (InstanceProvider): Sampled or generated data, including original instance. rules (Union[Sequence[str], TreeSurrogate, RuleSurrogate]): Rules applicable. used_features (Optional[Union[Sequence[str], Sequence[int]]]): Used features per label. Defaults to None. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. original_id (Optional[LT], optional): ID of original instance; picks first if None. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. type (Optional[str]): Type description. Defaults to 'base'. subtype (Optional[str], optional): Subtype description. Defaults to None. callargs (Optional[dict], optional): Call arguments for reproducibility. Defaults to None. **kwargs: Optional meta descriptors. \"\"\" LocalDataExplanation.__init__(self, provider=provider, original_id=original_id, sampled=sampled) BaseReturnType.__init__(self, labels=labels, labelset=labelset, type=type, subtype=subtype, callargs=callargs, **kwargs) if used_features is None: used_features = list(range(len(self.original_instance.tokenized))) self._used_features = copy.deepcopy(used_features) self._rules = self._extract_rules(rules) def _extract_rules(self, rules: Union[Sequence[str], TreeSurrogate, RuleSurrogate]): if isinstance(rules, (TreeSurrogate, RuleSurrogate)): from skrules.rule import replace_feature_name from skrules.skope_rules import BASE_FEATURE_NAME feature_dict = {BASE_FEATURE_NAME + str(i): feat for i, feat in enumerate(self.used_features)} return [(replace_feature_name(rule, feature_dict), perf) for rule, perf in rules.rules] print(rules) raise NotImplementedError('TODO: Support lists of rules') @property def rules(self): return self._rules @property def content(self): return self.rules Ancestors (in MRO) text_explainability.generation.return_types.ReadableDataMixin text_explainability.generation.return_types.UsedFeaturesMixin text_explainability.generation.return_types.BaseReturnType genbase.MetaInfo genbase.Configurable text_explainability.generation.return_types.LocalDataExplanation Static methods from_config 1 2 3 4 def from_config ( config : dict , ** kwargs ) -> 'Configurable' View Source 1 2 3 4 5 6 7 8 9 @classmethod def from_config(cls, config: dict, **kwargs) -> 'Configurable': config = {**config, **kwargs} _ = config.pop('__class__', None) return cls(**config) from_json 1 2 3 4 def from_json ( json_or_path : str , ** read_args ) -> 'Configurable' Get config from JSON string or filepath. Parameters: Name Type Description Default json_or_path str File path or JSON string. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @classmethod def from_json(cls, json_or_path: str, **read_args) -> 'Configurable': \"\"\"Get config from JSON string or filepath. Args: json_or_path (str): File path or JSON string. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" if Path.is_file(json_or_path): return cls.read_json(json_or_path, **read_args) return cls.from_config(srsly.json_loads(json_or_path)) from_yaml 1 2 3 def from_yaml ( yaml_or_path : str ) -> 'Configurable' Get config from YAML string or filepath. Parameters: Name Type Description Default yaml_or_path str File path or YAML string. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @classmethod def from_yaml(cls, yaml_or_path: str) -> 'Configurable': \"\"\"Get config from YAML string or filepath. Args: yaml_or_path (str): File path or YAML string. \"\"\" if Path.is_file(yaml_or_path): return cls.read_yaml(yaml_or_path) return cls.from_config(srsly.yaml_loads(yaml_or_path)) read_json 1 2 3 4 def read_json ( path : str , ** read_args ) -> 'Configurable' Read config from JSON file (GZIP JSON, JSONL or JSON). Parameters: Name Type Description Default path str File path. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @classmethod def read_json(cls, path: str, **read_args) -> 'Configurable': \"\"\"Read config from JSON file (GZIP JSON, JSONL or JSON). Args: path (str): File path. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" read_fn = srsly.read_json if path.endswith('.json.gz'): read_fn = srsly.read_gzip_json elif path.endswith('.jsonl'): read_fn = srsly.read_jsonl return cls.from_config(read_fn(path, **read_args)) read_yaml 1 2 3 def read_yaml ( path : str ) -> 'Configurable' Read config from YAML file. Parameters: Name Type Description Default path str File path. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 @classmethod def read_yaml(cls, path: str) -> 'Configurable': \"\"\"Read config from YAML file. Args: path (str): File path. \"\"\" return cls.from_config(srsly.read_yaml(path)) Instance variables 1 callargs 1 content 1 labels Get labels property. 1 labelset Get label names property. 1 meta 1 neighborhood_instances Instances in the neighborhood (either sampled or perturbed). 1 original_instance The instance for which the feature attribution scores were calculated. 1 perturbed_instances Perturbed versions of the original instance, if sampled=False during initialization. 1 renderargs 1 rules 1 sampled_instances Sampled instances, if sampled=True during initialization. 1 subtype 1 type 1 used_features Names of features of the original instance. Methods label_by_index 1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx to_config 1 2 3 def to_config ( self ) Convert class information into config (configuration dictionary). Parameters: Name Type Description Default exclude List[str] Names of variables to exclude. None Returns: Type Description dict [description] View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def to_config(self): if hasattr(self, 'content'): _content = self.content() if callable(self.content) else self.content content = dict(recursive_to_dict(_content, include_class=False)) else: content = super().to_config(exclude=['_type', '_subtype', '_dict', '_callargs']) return {'META': self.meta, 'CONTENT': content} to_json 1 2 3 4 def to_json ( self , indent : int = 2 ) -> str Convert config to JSON-formatted string. Parameters: Name Type Description Default indent int Number of spaces to indent JSON. Defaults to 2. 2 Returns: Type Description str Config formatted as JSON. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_json(self, indent: int = 2) -> str: \"\"\"Convert config to JSON-formatted string. Args: indent (int, optional): Number of spaces to indent JSON. Defaults to 2. Returns: str: Config formatted as JSON. \"\"\" return srsly.json_dumps(self.to_config(), indent=indent) to_yaml 1 2 3 4 def to_yaml ( self , ** write_args ) -> str Convert config to YAML-formatted string. Parameters: Name Type Description Default **write_args None Optional arguments passed to srsly.yaml_dumps() None Returns: Type Description str Config formatted as YAML. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_yaml(self, **write_args) -> str: \"\"\"Convert config to YAML-formatted string. Args: **write_args: Optional arguments passed to `srsly.yaml_dumps()` Returns: str: Config formatted as YAML. \"\"\" return srsly.yaml_dumps(self.to_config(), **write_args) write_json 1 2 3 4 5 def write_json ( self , path : str , indent : int = 2 ) -> None Write class config to JSON. Parameters: Name Type Description Default path str Path to save to. If ends in .json.gz saves as GZIP JSON, .jsonl as JSONL or JSON by default. None indent int Number of spaces to indent JSON. Defaults to 2. 2 View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def write_json(self, path: str, indent: int = 2) -> None: \"\"\"Write class config to JSON. Args: path (str): Path to save to. If ends in `.json.gz` saves as GZIP JSON, `.jsonl` as JSONL or JSON by default. indent (int, optional): Number of spaces to indent JSON. Defaults to 2. \"\"\" write_fn = srsly.write_json if path.endswith('.json.gz'): write_fn = srsly.write_gzip_json elif path.endswith('.jsonl'): write_fn = srsly.write_jsonl write_fn(path, self.to_config(), indent=indent) write_yaml 1 2 3 4 5 def write_yaml ( self , path : str , ** write_args ) -> None Write class config to YAML. Parameters: Name Type Description Default path str Path to save to. None **write_args None Optional arguments passed to srsly.write_yaml() None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def write_yaml(self, path: str, **write_args) -> None: \"\"\"Write class config to YAML. Args: path (str): Path to save to. **write_args: Optional arguments passed to `srsly.write_yaml()` \"\"\" srsly.write_yaml(path, self.to_config(), **write_args) UsedFeaturesMixin 1 2 3 4 5 class UsedFeaturesMixin ( / , * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 class UsedFeaturesMixin: @property def used_features(self): \"\"\"Get used features property.\"\"\" return self._used_features Descendants text_explainability.generation.return_types.FeatureList text_explainability.generation.return_types.Rules Instance variables 1 used_features Get used features property.","title":"Return Types"},{"location":"reference/text_explainability/generation/return_types/#module-text_explainabilitygenerationreturn_types","text":"General return types for global/local explanations. Todo: 1 2 3 * Add rule-based explanations * Add named label support * Test for bugs View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 \"\"\"General return types for global/local explanations. Todo: * Add rule-based explanations * Add named label support * Test for bugs \"\"\" import copy from typing import Dict, Optional, Sequence, Tuple, Union import numpy as np from genbase import MetaInfo from instancelib import InstanceProvider from instancelib.typehints import LT from ..ui.notebook import Render from .surrogate import RuleSurrogate, TreeSurrogate class BaseReturnType(MetaInfo): def __init__(self, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, type: Optional[str] = 'base', subtype: Optional[str] = None, callargs: Optional[dict] = None, **kwargs): \"\"\"Base return type. Args: labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. type (Optional[str]): Type description. Defaults to 'base'. subtype (Optional[str], optional): Subtype description. Defaults to None. callargs (Optional[dict], optional): Call arguments for reproducibility. Defaults to None. **kwargs: Optional meta descriptors. \"\"\" renderer = kwargs.pop('renderer', Render) super().__init__(type=type, subtype=subtype, callargs=callargs, renderer=renderer, **kwargs) self._labels = labels self._labelset = labelset @property def labels(self): \"\"\"Get labels property.\"\"\" if self._labels is None: return self._labels return list(self._labels) @property def labelset(self): \"\"\"Get label names property.\"\"\" return self._labelset def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx def __repr__(self) -> str: labels = [self.label_by_index(label) for label in self.labels] if self.labels is not None else None if hasattr(self, 'used_features'): return f'{self.__class__.__name__}(labels={labels}, used_features={self.used_features})' return f'{self.__class__.__name__}(label={labels})' class UsedFeaturesMixin: @property def used_features(self): \"\"\"Get used features property.\"\"\" return self._used_features class FeatureList(BaseReturnType, UsedFeaturesMixin): def __init__(self, used_features: Union[Sequence[str], Sequence[int]], scores: Union[Sequence[int], Sequence[float]], labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, type: Optional[str] = 'global_explanation', subtype: Optional[str] = 'feature_list', callargs: Optional[dict] = None, **kwargs): \"\"\"Save scores per feature, grouped per label. Examples of scores are feature importance scores, or counts of features in a dataset. Args: used_features (Union[Sequence[str], Sequence[int]]): Used features per label. scores (Union[Sequence[int], Sequence[float]]): Scores per label. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. type (Optional[str]): Type description. Defaults to 'explanation'. subtype (Optional[str], optional): Subtype description. Defaults to 'feature_list'. callargs (Optional[dict], optional): Call arguments for reproducibility. Defaults to None. **kwargs: Optional meta descriptors. \"\"\" super().__init__(labels=labels, labelset=labelset, type=type, subtype=subtype, callargs=callargs, **kwargs) self._used_features = copy.deepcopy(used_features) self._scores = scores def get_raw_scores(self, normalize: bool = False) -> np.ndarray: \"\"\"Get saved scores per label as `np.ndarray`. Args: normalize (bool, optional): Normalize scores (ensure they sum to one). Defaults to False. Returns: np.ndarray: Scores. \"\"\" def feature_scores(scores): if not isinstance(scores, np.ndarray): scores = np.array(scores) if normalize: return scores / scores.sum(axis=0) return scores if isinstance(self._scores, dict): return {k: feature_scores(v) for k, v in self._scores.items()} return feature_scores(self._scores) def get_scores(self, normalize: bool = False) -> Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: \"\"\"Get scores per label. Args: normalize (bool, optional): Whether to normalize the scores (sum to one). Defaults to False. Returns: Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: Scores per label, if no `labelset` is not set, defaults to 'all' \"\"\" # TODO: change to IDs all_scores = self.get_raw_scores(normalize=normalize) if self.labels is None: return {'all': [(feature, score_) for feature, score_ in zip(self.used_features, all_scores)]} if isinstance(self.used_features, dict): return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features[label], all_scores[i])] for i, label in enumerate(self.labels)} return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features, all_scores[i])] for i, label in enumerate(self.labels)} @property def scores(self): \"\"\"Saved scores (e.g. feature importance).\"\"\" return self.get_scores(normalize=False) @property def content(self): return self.scores def __repr__(self) -> str: return '\\n'.join([f'{a}: {str(b)}' for a, b in self.scores.items()]) class LocalDataExplanation: def __init__(self, provider: InstanceProvider, original_id: Optional[LT] = None, sampled: bool = False): \"\"\"Save the sampled/generated instances used to determine an explanation. Args: provider (InstanceProvider): Sampled or generated data, including original instance. original_id (Optional[LT], optional): ID of original instance; picks first if None. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. \"\"\" self._provider = provider original_id = next(iter(self._provider)) if original_id is None else original_id self._original_instance = copy.deepcopy(self._provider[original_id]) self._neighborhood_instances = copy.deepcopy(self._provider.get_children(self._original_instance)) self.sampled = sampled @property def original_instance(self): \"\"\"The instance for which the feature attribution scores were calculated.\"\"\" return self._original_instance @property def perturbed_instances(self): \"\"\"Perturbed versions of the original instance, if `sampled=False` during initialization.\"\"\" return None if self.sampled else self._neighborhood_instances @property def sampled_instances(self): \"\"\"Sampled instances, if `sampled=True` during initialization.\"\"\" return self._neighborhood_instances if self.sampled else None @property def neighborhood_instances(self): \"\"\"Instances in the neighborhood (either sampled or perturbed).\"\"\" return self._neighborhood_instances class ReadableDataMixin: @property def used_features(self): \"\"\"Names of features of the original instance.\"\"\" if hasattr(self.original_instance, 'tokenized'): if isinstance(self._used_features, dict): return {k: [self.original_instance.tokenized[i] for i in v] for k, v in self._used_features.items()} return [self.original_instance.tokenized[i] for i in self._used_features] return list(self._used_features) def __repr__(self) -> str: sampled_or_perturbed = 'sampled' if self.sampled else 'perturbed' n = sum(1 for _ in self.neighborhood_instances) labels = [self.label_by_index(label) for label in self.labels] if self.labels is not None else None return f'{self.__class__.__name__}(labels={labels}, ' + \\ f'used_features={self.used_features}, n_{sampled_or_perturbed}_instances={n})' class FeatureAttribution(ReadableDataMixin, FeatureList, LocalDataExplanation): def __init__(self, provider: InstanceProvider, scores: Sequence[float], used_features: Optional[Union[Sequence[str], Sequence[int]]] = None, scores_stddev: Sequence[float] = None, base_score: float = None, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, original_id: Optional[LT] = None, sampled: bool = False, type: Optional[str] = 'local_explanation', subtype: Optional[str] = 'feature_attribution', callargs: Optional[dict] = None, **kwargs): \"\"\"Create a `FeatureList` with additional information saved. The additional information contains the possibility to add standard deviations, base scores, and the sampled or generated instances used to calculate these scores. Args: provider (InstanceProvider): Sampled or generated data, including original instance. scores (Sequence[float]): Scores corresponding to the selected features. used_features (Optional[Union[Sequence[str], Sequence[int]]]): Selected features for the explanation label. Defaults to None. scores_stddev (Sequence[float], optional): Standard deviation of each feature attribution score. Defaults to None. base_score (float, optional): Base score, to which all scores are relative. Defaults to None. labels (Optional[Sequence[int]], optional): Labels for outputs (e.g. classes). Defaults to None. labelset (Optional[Sequence[str]], optional): Label names corresponding to labels. Defaults to None. original_id (Optional[LT], optional): ID of original instance; picks first if None. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. type (Optional[str]): Type description. Defaults to 'base'. subtype (Optional[str], optional): Subtype description. Defaults to None. callargs (Optional[dict], optional): Call arguments for reproducibility. Defaults to None. **kwargs: Optional meta descriptors. \"\"\" LocalDataExplanation.__init__(self, provider=provider, original_id=original_id, sampled=sampled) if used_features is None: used_features = list(range(len(self.original_instance.tokenized))) FeatureList.__init__(self, used_features=used_features, scores=scores, labels=labels, labelset=labelset, type=type, subtype=subtype, callargs=callargs, **kwargs) self._base_score = base_score self._scores_stddev = scores_stddev @property def scores(self): \"\"\"Saved feature attribution scores.\"\"\" return self.get_scores(normalize=False) @property def content(self): return {'features': list(self.original_instance.tokenized), 'scores': self.scores} class Rules(ReadableDataMixin, UsedFeaturesMixin, BaseReturnType, LocalDataExplanation): def __init__(self, provider: InstanceProvider, rules: Union[Sequence[str], TreeSurrogate, RuleSurrogate], used_features: Optional[Union[Sequence[str], Sequence[int]]] = None, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, original_id: Optional[LT] = None, sampled: bool = False, type: Optional[str] = 'local_explanation', subtype: Optional[str] = 'rules', callargs: Optional[dict] = None, **kwargs): \"\"\"Rule-based return type. Args: provider (InstanceProvider): Sampled or generated data, including original instance. rules (Union[Sequence[str], TreeSurrogate, RuleSurrogate]): Rules applicable. used_features (Optional[Union[Sequence[str], Sequence[int]]]): Used features per label. Defaults to None. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. original_id (Optional[LT], optional): ID of original instance; picks first if None. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. type (Optional[str]): Type description. Defaults to 'base'. subtype (Optional[str], optional): Subtype description. Defaults to None. callargs (Optional[dict], optional): Call arguments for reproducibility. Defaults to None. **kwargs: Optional meta descriptors. \"\"\" LocalDataExplanation.__init__(self, provider=provider, original_id=original_id, sampled=sampled) BaseReturnType.__init__(self, labels=labels, labelset=labelset, type=type, subtype=subtype, callargs=callargs, **kwargs) if used_features is None: used_features = list(range(len(self.original_instance.tokenized))) self._used_features = copy.deepcopy(used_features) self._rules = self._extract_rules(rules) def _extract_rules(self, rules: Union[Sequence[str], TreeSurrogate, RuleSurrogate]): if isinstance(rules, (TreeSurrogate, RuleSurrogate)): from skrules.rule import replace_feature_name from skrules.skope_rules import BASE_FEATURE_NAME feature_dict = {BASE_FEATURE_NAME + str(i): feat for i, feat in enumerate(self.used_features)} return [(replace_feature_name(rule, feature_dict), perf) for rule, perf in rules.rules] print(rules) raise NotImplementedError('TODO: Support lists of rules') @property def rules(self): return self._rules @property def content(self): return self.rules class Instances(BaseReturnType): def __init__(self, instances, type: Optional[str] = 'global_explanation', subtype: Optional[str] = 'prototypes', callargs: Optional[dict] = None, **kwargs): super().__init__(labels=None, labelset=None, type=type, subtype=subtype, callargs=callargs, **kwargs) self.instances = instances @property def content(self): return self.instances if isinstance(self.instances, dict) else {'instances': self.instances}","title":"Module text_explainability.generation.return_types"},{"location":"reference/text_explainability/generation/return_types/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/generation/return_types/#basereturntype","text":"1 2 3 4 5 6 7 8 class BaseReturnType ( labels : Optional [ Sequence [ int ]] = None , labelset : Optional [ Sequence [ str ]] = None , type : Optional [ str ] = 'base' , subtype : Optional [ str ] = None , callargs : Optional [ dict ] = None , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class BaseReturnType(MetaInfo): def __init__(self, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, type: Optional[str] = 'base', subtype: Optional[str] = None, callargs: Optional[dict] = None, **kwargs): \"\"\"Base return type. Args: labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. type (Optional[str]): Type description. Defaults to 'base'. subtype (Optional[str], optional): Subtype description. Defaults to None. callargs (Optional[dict], optional): Call arguments for reproducibility. Defaults to None. **kwargs: Optional meta descriptors. \"\"\" renderer = kwargs.pop('renderer', Render) super().__init__(type=type, subtype=subtype, callargs=callargs, renderer=renderer, **kwargs) self._labels = labels self._labelset = labelset @property def labels(self): \"\"\"Get labels property.\"\"\" if self._labels is None: return self._labels return list(self._labels) @property def labelset(self): \"\"\"Get label names property.\"\"\" return self._labelset def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx def __repr__(self) -> str: labels = [self.label_by_index(label) for label in self.labels] if self.labels is not None else None if hasattr(self, 'used_features'): return f'{self.__class__.__name__}(labels={labels}, used_features={self.used_features})' return f'{self.__class__.__name__}(label={labels})'","title":"BaseReturnType"},{"location":"reference/text_explainability/generation/return_types/#ancestors-in-mro","text":"genbase.MetaInfo genbase.Configurable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/return_types/#descendants","text":"text_explainability.generation.return_types.FeatureList text_explainability.generation.return_types.Rules text_explainability.generation.return_types.Instances","title":"Descendants"},{"location":"reference/text_explainability/generation/return_types/#static-methods","text":"","title":"Static methods"},{"location":"reference/text_explainability/generation/return_types/#from_config","text":"1 2 3 4 def from_config ( config : dict , ** kwargs ) -> 'Configurable' View Source 1 2 3 4 5 6 7 8 9 @classmethod def from_config(cls, config: dict, **kwargs) -> 'Configurable': config = {**config, **kwargs} _ = config.pop('__class__', None) return cls(**config)","title":"from_config"},{"location":"reference/text_explainability/generation/return_types/#from_json","text":"1 2 3 4 def from_json ( json_or_path : str , ** read_args ) -> 'Configurable' Get config from JSON string or filepath. Parameters: Name Type Description Default json_or_path str File path or JSON string. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @classmethod def from_json(cls, json_or_path: str, **read_args) -> 'Configurable': \"\"\"Get config from JSON string or filepath. Args: json_or_path (str): File path or JSON string. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" if Path.is_file(json_or_path): return cls.read_json(json_or_path, **read_args) return cls.from_config(srsly.json_loads(json_or_path))","title":"from_json"},{"location":"reference/text_explainability/generation/return_types/#from_yaml","text":"1 2 3 def from_yaml ( yaml_or_path : str ) -> 'Configurable' Get config from YAML string or filepath. Parameters: Name Type Description Default yaml_or_path str File path or YAML string. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @classmethod def from_yaml(cls, yaml_or_path: str) -> 'Configurable': \"\"\"Get config from YAML string or filepath. Args: yaml_or_path (str): File path or YAML string. \"\"\" if Path.is_file(yaml_or_path): return cls.read_yaml(yaml_or_path) return cls.from_config(srsly.yaml_loads(yaml_or_path))","title":"from_yaml"},{"location":"reference/text_explainability/generation/return_types/#read_json","text":"1 2 3 4 def read_json ( path : str , ** read_args ) -> 'Configurable' Read config from JSON file (GZIP JSON, JSONL or JSON). Parameters: Name Type Description Default path str File path. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @classmethod def read_json(cls, path: str, **read_args) -> 'Configurable': \"\"\"Read config from JSON file (GZIP JSON, JSONL or JSON). Args: path (str): File path. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" read_fn = srsly.read_json if path.endswith('.json.gz'): read_fn = srsly.read_gzip_json elif path.endswith('.jsonl'): read_fn = srsly.read_jsonl return cls.from_config(read_fn(path, **read_args))","title":"read_json"},{"location":"reference/text_explainability/generation/return_types/#read_yaml","text":"1 2 3 def read_yaml ( path : str ) -> 'Configurable' Read config from YAML file. Parameters: Name Type Description Default path str File path. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 @classmethod def read_yaml(cls, path: str) -> 'Configurable': \"\"\"Read config from YAML file. Args: path (str): File path. \"\"\" return cls.from_config(srsly.read_yaml(path))","title":"read_yaml"},{"location":"reference/text_explainability/generation/return_types/#instance-variables","text":"1 callargs 1 labels Get labels property. 1 labelset Get label names property. 1 meta 1 renderargs 1 subtype 1 type","title":"Instance variables"},{"location":"reference/text_explainability/generation/return_types/#methods","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/return_types/#label_by_index","text":"1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx","title":"label_by_index"},{"location":"reference/text_explainability/generation/return_types/#to_config","text":"1 2 3 def to_config ( self ) Convert class information into config (configuration dictionary). Parameters: Name Type Description Default exclude List[str] Names of variables to exclude. None Returns: Type Description dict [description] View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def to_config(self): if hasattr(self, 'content'): _content = self.content() if callable(self.content) else self.content content = dict(recursive_to_dict(_content, include_class=False)) else: content = super().to_config(exclude=['_type', '_subtype', '_dict', '_callargs']) return {'META': self.meta, 'CONTENT': content}","title":"to_config"},{"location":"reference/text_explainability/generation/return_types/#to_json","text":"1 2 3 4 def to_json ( self , indent : int = 2 ) -> str Convert config to JSON-formatted string. Parameters: Name Type Description Default indent int Number of spaces to indent JSON. Defaults to 2. 2 Returns: Type Description str Config formatted as JSON. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_json(self, indent: int = 2) -> str: \"\"\"Convert config to JSON-formatted string. Args: indent (int, optional): Number of spaces to indent JSON. Defaults to 2. Returns: str: Config formatted as JSON. \"\"\" return srsly.json_dumps(self.to_config(), indent=indent)","title":"to_json"},{"location":"reference/text_explainability/generation/return_types/#to_yaml","text":"1 2 3 4 def to_yaml ( self , ** write_args ) -> str Convert config to YAML-formatted string. Parameters: Name Type Description Default **write_args None Optional arguments passed to srsly.yaml_dumps() None Returns: Type Description str Config formatted as YAML. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_yaml(self, **write_args) -> str: \"\"\"Convert config to YAML-formatted string. Args: **write_args: Optional arguments passed to `srsly.yaml_dumps()` Returns: str: Config formatted as YAML. \"\"\" return srsly.yaml_dumps(self.to_config(), **write_args)","title":"to_yaml"},{"location":"reference/text_explainability/generation/return_types/#write_json","text":"1 2 3 4 5 def write_json ( self , path : str , indent : int = 2 ) -> None Write class config to JSON. Parameters: Name Type Description Default path str Path to save to. If ends in .json.gz saves as GZIP JSON, .jsonl as JSONL or JSON by default. None indent int Number of spaces to indent JSON. Defaults to 2. 2 View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def write_json(self, path: str, indent: int = 2) -> None: \"\"\"Write class config to JSON. Args: path (str): Path to save to. If ends in `.json.gz` saves as GZIP JSON, `.jsonl` as JSONL or JSON by default. indent (int, optional): Number of spaces to indent JSON. Defaults to 2. \"\"\" write_fn = srsly.write_json if path.endswith('.json.gz'): write_fn = srsly.write_gzip_json elif path.endswith('.jsonl'): write_fn = srsly.write_jsonl write_fn(path, self.to_config(), indent=indent)","title":"write_json"},{"location":"reference/text_explainability/generation/return_types/#write_yaml","text":"1 2 3 4 5 def write_yaml ( self , path : str , ** write_args ) -> None Write class config to YAML. Parameters: Name Type Description Default path str Path to save to. None **write_args None Optional arguments passed to srsly.write_yaml() None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def write_yaml(self, path: str, **write_args) -> None: \"\"\"Write class config to YAML. Args: path (str): Path to save to. **write_args: Optional arguments passed to `srsly.write_yaml()` \"\"\" srsly.write_yaml(path, self.to_config(), **write_args)","title":"write_yaml"},{"location":"reference/text_explainability/generation/return_types/#featureattribution","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class FeatureAttribution ( provider : instancelib . instances . base . InstanceProvider , scores : Sequence [ float ], used_features : Union [ Sequence [ str ], Sequence [ int ], NoneType ] = None , scores_stddev : Sequence [ float ] = None , base_score : float = None , labels : Optional [ Sequence [ int ]] = None , labelset : Optional [ Sequence [ str ]] = None , original_id : Optional [ ~ LT ] = None , sampled : bool = False , type : Optional [ str ] = 'local_explanation' , subtype : Optional [ str ] = 'feature_attribution' , callargs : Optional [ dict ] = None , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 class FeatureAttribution(ReadableDataMixin, FeatureList, LocalDataExplanation): def __init__(self, provider: InstanceProvider, scores: Sequence[float], used_features: Optional[Union[Sequence[str], Sequence[int]]] = None, scores_stddev: Sequence[float] = None, base_score: float = None, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, original_id: Optional[LT] = None, sampled: bool = False, type: Optional[str] = 'local_explanation', subtype: Optional[str] = 'feature_attribution', callargs: Optional[dict] = None, **kwargs): \"\"\"Create a `FeatureList` with additional information saved. The additional information contains the possibility to add standard deviations, base scores, and the sampled or generated instances used to calculate these scores. Args: provider (InstanceProvider): Sampled or generated data, including original instance. scores (Sequence[float]): Scores corresponding to the selected features. used_features (Optional[Union[Sequence[str], Sequence[int]]]): Selected features for the explanation label. Defaults to None. scores_stddev (Sequence[float], optional): Standard deviation of each feature attribution score. Defaults to None. base_score (float, optional): Base score, to which all scores are relative. Defaults to None. labels (Optional[Sequence[int]], optional): Labels for outputs (e.g. classes). Defaults to None. labelset (Optional[Sequence[str]], optional): Label names corresponding to labels. Defaults to None. original_id (Optional[LT], optional): ID of original instance; picks first if None. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. type (Optional[str]): Type description. Defaults to 'base'. subtype (Optional[str], optional): Subtype description. Defaults to None. callargs (Optional[dict], optional): Call arguments for reproducibility. Defaults to None. **kwargs: Optional meta descriptors. \"\"\" LocalDataExplanation.__init__(self, provider=provider, original_id=original_id, sampled=sampled) if used_features is None: used_features = list(range(len(self.original_instance.tokenized))) FeatureList.__init__(self, used_features=used_features, scores=scores, labels=labels, labelset=labelset, type=type, subtype=subtype, callargs=callargs, **kwargs) self._base_score = base_score self._scores_stddev = scores_stddev @property def scores(self): \"\"\"Saved feature attribution scores.\"\"\" return self.get_scores(normalize=False) @property def content(self): return {'features': list(self.original_instance.tokenized), 'scores': self.scores}","title":"FeatureAttribution"},{"location":"reference/text_explainability/generation/return_types/#ancestors-in-mro_1","text":"text_explainability.generation.return_types.ReadableDataMixin text_explainability.generation.return_types.FeatureList text_explainability.generation.return_types.BaseReturnType genbase.MetaInfo genbase.Configurable text_explainability.generation.return_types.UsedFeaturesMixin text_explainability.generation.return_types.LocalDataExplanation","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/return_types/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/text_explainability/generation/return_types/#from_config_1","text":"1 2 3 4 def from_config ( config : dict , ** kwargs ) -> 'Configurable' View Source 1 2 3 4 5 6 7 8 9 @classmethod def from_config(cls, config: dict, **kwargs) -> 'Configurable': config = {**config, **kwargs} _ = config.pop('__class__', None) return cls(**config)","title":"from_config"},{"location":"reference/text_explainability/generation/return_types/#from_json_1","text":"1 2 3 4 def from_json ( json_or_path : str , ** read_args ) -> 'Configurable' Get config from JSON string or filepath. Parameters: Name Type Description Default json_or_path str File path or JSON string. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @classmethod def from_json(cls, json_or_path: str, **read_args) -> 'Configurable': \"\"\"Get config from JSON string or filepath. Args: json_or_path (str): File path or JSON string. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" if Path.is_file(json_or_path): return cls.read_json(json_or_path, **read_args) return cls.from_config(srsly.json_loads(json_or_path))","title":"from_json"},{"location":"reference/text_explainability/generation/return_types/#from_yaml_1","text":"1 2 3 def from_yaml ( yaml_or_path : str ) -> 'Configurable' Get config from YAML string or filepath. Parameters: Name Type Description Default yaml_or_path str File path or YAML string. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @classmethod def from_yaml(cls, yaml_or_path: str) -> 'Configurable': \"\"\"Get config from YAML string or filepath. Args: yaml_or_path (str): File path or YAML string. \"\"\" if Path.is_file(yaml_or_path): return cls.read_yaml(yaml_or_path) return cls.from_config(srsly.yaml_loads(yaml_or_path))","title":"from_yaml"},{"location":"reference/text_explainability/generation/return_types/#read_json_1","text":"1 2 3 4 def read_json ( path : str , ** read_args ) -> 'Configurable' Read config from JSON file (GZIP JSON, JSONL or JSON). Parameters: Name Type Description Default path str File path. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @classmethod def read_json(cls, path: str, **read_args) -> 'Configurable': \"\"\"Read config from JSON file (GZIP JSON, JSONL or JSON). Args: path (str): File path. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" read_fn = srsly.read_json if path.endswith('.json.gz'): read_fn = srsly.read_gzip_json elif path.endswith('.jsonl'): read_fn = srsly.read_jsonl return cls.from_config(read_fn(path, **read_args))","title":"read_json"},{"location":"reference/text_explainability/generation/return_types/#read_yaml_1","text":"1 2 3 def read_yaml ( path : str ) -> 'Configurable' Read config from YAML file. Parameters: Name Type Description Default path str File path. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 @classmethod def read_yaml(cls, path: str) -> 'Configurable': \"\"\"Read config from YAML file. Args: path (str): File path. \"\"\" return cls.from_config(srsly.read_yaml(path))","title":"read_yaml"},{"location":"reference/text_explainability/generation/return_types/#instance-variables_1","text":"1 callargs 1 content 1 labels Get labels property. 1 labelset Get label names property. 1 meta 1 neighborhood_instances Instances in the neighborhood (either sampled or perturbed). 1 original_instance The instance for which the feature attribution scores were calculated. 1 perturbed_instances Perturbed versions of the original instance, if sampled=False during initialization. 1 renderargs 1 sampled_instances Sampled instances, if sampled=True during initialization. 1 scores Saved feature attribution scores. 1 subtype 1 type 1 used_features Names of features of the original instance.","title":"Instance variables"},{"location":"reference/text_explainability/generation/return_types/#methods_1","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/return_types/#get_raw_scores","text":"1 2 3 4 def get_raw_scores ( self , normalize : bool = False ) -> numpy . ndarray Get saved scores per label as np.ndarray . Parameters: Name Type Description Default normalize bool Normalize scores (ensure they sum to one). Defaults to False. False Returns: Type Description np.ndarray Scores. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def get_raw_scores(self, normalize: bool = False) -> np.ndarray: \"\"\"Get saved scores per label as `np.ndarray`. Args: normalize (bool, optional): Normalize scores (ensure they sum to one). Defaults to False. Returns: np.ndarray: Scores. \"\"\" def feature_scores(scores): if not isinstance(scores, np.ndarray): scores = np.array(scores) if normalize: return scores / scores.sum(axis=0) return scores if isinstance(self._scores, dict): return {k: feature_scores(v) for k, v in self._scores.items()} return feature_scores(self._scores)","title":"get_raw_scores"},{"location":"reference/text_explainability/generation/return_types/#get_scores","text":"1 2 3 4 def get_scores ( self , normalize : bool = False ) -> Dict [ Union [ str , int ], Tuple [ Union [ str , int ], Union [ float , int ]]] Get scores per label. Parameters: Name Type Description Default normalize bool Whether to normalize the scores (sum to one). Defaults to False. False Returns: Type Description Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]] Scores per label, if no labelset is not set, defaults to 'all' View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def get_scores(self, normalize: bool = False) -> Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: \"\"\"Get scores per label. Args: normalize (bool, optional): Whether to normalize the scores (sum to one). Defaults to False. Returns: Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: Scores per label, if no `labelset` is not set, defaults to 'all' \"\"\" # TODO: change to IDs all_scores = self.get_raw_scores(normalize=normalize) if self.labels is None: return {'all': [(feature, score_) for feature, score_ in zip(self.used_features, all_scores)]} if isinstance(self.used_features, dict): return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features[label], all_scores[i])] for i, label in enumerate(self.labels)} return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features, all_scores[i])] for i, label in enumerate(self.labels)}","title":"get_scores"},{"location":"reference/text_explainability/generation/return_types/#label_by_index_1","text":"1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx","title":"label_by_index"},{"location":"reference/text_explainability/generation/return_types/#to_config_1","text":"1 2 3 def to_config ( self ) Convert class information into config (configuration dictionary). Parameters: Name Type Description Default exclude List[str] Names of variables to exclude. None Returns: Type Description dict [description] View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def to_config(self): if hasattr(self, 'content'): _content = self.content() if callable(self.content) else self.content content = dict(recursive_to_dict(_content, include_class=False)) else: content = super().to_config(exclude=['_type', '_subtype', '_dict', '_callargs']) return {'META': self.meta, 'CONTENT': content}","title":"to_config"},{"location":"reference/text_explainability/generation/return_types/#to_json_1","text":"1 2 3 4 def to_json ( self , indent : int = 2 ) -> str Convert config to JSON-formatted string. Parameters: Name Type Description Default indent int Number of spaces to indent JSON. Defaults to 2. 2 Returns: Type Description str Config formatted as JSON. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_json(self, indent: int = 2) -> str: \"\"\"Convert config to JSON-formatted string. Args: indent (int, optional): Number of spaces to indent JSON. Defaults to 2. Returns: str: Config formatted as JSON. \"\"\" return srsly.json_dumps(self.to_config(), indent=indent)","title":"to_json"},{"location":"reference/text_explainability/generation/return_types/#to_yaml_1","text":"1 2 3 4 def to_yaml ( self , ** write_args ) -> str Convert config to YAML-formatted string. Parameters: Name Type Description Default **write_args None Optional arguments passed to srsly.yaml_dumps() None Returns: Type Description str Config formatted as YAML. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_yaml(self, **write_args) -> str: \"\"\"Convert config to YAML-formatted string. Args: **write_args: Optional arguments passed to `srsly.yaml_dumps()` Returns: str: Config formatted as YAML. \"\"\" return srsly.yaml_dumps(self.to_config(), **write_args)","title":"to_yaml"},{"location":"reference/text_explainability/generation/return_types/#write_json_1","text":"1 2 3 4 5 def write_json ( self , path : str , indent : int = 2 ) -> None Write class config to JSON. Parameters: Name Type Description Default path str Path to save to. If ends in .json.gz saves as GZIP JSON, .jsonl as JSONL or JSON by default. None indent int Number of spaces to indent JSON. Defaults to 2. 2 View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def write_json(self, path: str, indent: int = 2) -> None: \"\"\"Write class config to JSON. Args: path (str): Path to save to. If ends in `.json.gz` saves as GZIP JSON, `.jsonl` as JSONL or JSON by default. indent (int, optional): Number of spaces to indent JSON. Defaults to 2. \"\"\" write_fn = srsly.write_json if path.endswith('.json.gz'): write_fn = srsly.write_gzip_json elif path.endswith('.jsonl'): write_fn = srsly.write_jsonl write_fn(path, self.to_config(), indent=indent)","title":"write_json"},{"location":"reference/text_explainability/generation/return_types/#write_yaml_1","text":"1 2 3 4 5 def write_yaml ( self , path : str , ** write_args ) -> None Write class config to YAML. Parameters: Name Type Description Default path str Path to save to. None **write_args None Optional arguments passed to srsly.write_yaml() None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def write_yaml(self, path: str, **write_args) -> None: \"\"\"Write class config to YAML. Args: path (str): Path to save to. **write_args: Optional arguments passed to `srsly.write_yaml()` \"\"\" srsly.write_yaml(path, self.to_config(), **write_args)","title":"write_yaml"},{"location":"reference/text_explainability/generation/return_types/#featurelist","text":"1 2 3 4 5 6 7 8 9 10 class FeatureList ( used_features : Union [ Sequence [ str ], Sequence [ int ]], scores : Union [ Sequence [ int ], Sequence [ float ]], labels : Optional [ Sequence [ int ]] = None , labelset : Optional [ Sequence [ str ]] = None , type : Optional [ str ] = 'global_explanation' , subtype : Optional [ str ] = 'feature_list' , callargs : Optional [ dict ] = None , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 class FeatureList(BaseReturnType, UsedFeaturesMixin): def __init__(self, used_features: Union[Sequence[str], Sequence[int]], scores: Union[Sequence[int], Sequence[float]], labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, type: Optional[str] = 'global_explanation', subtype: Optional[str] = 'feature_list', callargs: Optional[dict] = None, **kwargs): \"\"\"Save scores per feature, grouped per label. Examples of scores are feature importance scores, or counts of features in a dataset. Args: used_features (Union[Sequence[str], Sequence[int]]): Used features per label. scores (Union[Sequence[int], Sequence[float]]): Scores per label. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. type (Optional[str]): Type description. Defaults to 'explanation'. subtype (Optional[str], optional): Subtype description. Defaults to 'feature_list'. callargs (Optional[dict], optional): Call arguments for reproducibility. Defaults to None. **kwargs: Optional meta descriptors. \"\"\" super().__init__(labels=labels, labelset=labelset, type=type, subtype=subtype, callargs=callargs, **kwargs) self._used_features = copy.deepcopy(used_features) self._scores = scores def get_raw_scores(self, normalize: bool = False) -> np.ndarray: \"\"\"Get saved scores per label as `np.ndarray`. Args: normalize (bool, optional): Normalize scores (ensure they sum to one). Defaults to False. Returns: np.ndarray: Scores. \"\"\" def feature_scores(scores): if not isinstance(scores, np.ndarray): scores = np.array(scores) if normalize: return scores / scores.sum(axis=0) return scores if isinstance(self._scores, dict): return {k: feature_scores(v) for k, v in self._scores.items()} return feature_scores(self._scores) def get_scores(self, normalize: bool = False) -> Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: \"\"\"Get scores per label. Args: normalize (bool, optional): Whether to normalize the scores (sum to one). Defaults to False. Returns: Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: Scores per label, if no `labelset` is not set, defaults to 'all' \"\"\" # TODO: change to IDs all_scores = self.get_raw_scores(normalize=normalize) if self.labels is None: return {'all': [(feature, score_) for feature, score_ in zip(self.used_features, all_scores)]} if isinstance(self.used_features, dict): return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features[label], all_scores[i])] for i, label in enumerate(self.labels)} return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features, all_scores[i])] for i, label in enumerate(self.labels)} @property def scores(self): \"\"\"Saved scores (e.g. feature importance).\"\"\" return self.get_scores(normalize=False) @property def content(self): return self.scores def __repr__(self) -> str: return '\\n'.join([f'{a}: {str(b)}' for a, b in self.scores.items()])","title":"FeatureList"},{"location":"reference/text_explainability/generation/return_types/#ancestors-in-mro_2","text":"text_explainability.generation.return_types.BaseReturnType genbase.MetaInfo genbase.Configurable text_explainability.generation.return_types.UsedFeaturesMixin","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/return_types/#descendants_1","text":"text_explainability.generation.return_types.FeatureAttribution","title":"Descendants"},{"location":"reference/text_explainability/generation/return_types/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/text_explainability/generation/return_types/#from_config_2","text":"1 2 3 4 def from_config ( config : dict , ** kwargs ) -> 'Configurable' View Source 1 2 3 4 5 6 7 8 9 @classmethod def from_config(cls, config: dict, **kwargs) -> 'Configurable': config = {**config, **kwargs} _ = config.pop('__class__', None) return cls(**config)","title":"from_config"},{"location":"reference/text_explainability/generation/return_types/#from_json_2","text":"1 2 3 4 def from_json ( json_or_path : str , ** read_args ) -> 'Configurable' Get config from JSON string or filepath. Parameters: Name Type Description Default json_or_path str File path or JSON string. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @classmethod def from_json(cls, json_or_path: str, **read_args) -> 'Configurable': \"\"\"Get config from JSON string or filepath. Args: json_or_path (str): File path or JSON string. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" if Path.is_file(json_or_path): return cls.read_json(json_or_path, **read_args) return cls.from_config(srsly.json_loads(json_or_path))","title":"from_json"},{"location":"reference/text_explainability/generation/return_types/#from_yaml_2","text":"1 2 3 def from_yaml ( yaml_or_path : str ) -> 'Configurable' Get config from YAML string or filepath. Parameters: Name Type Description Default yaml_or_path str File path or YAML string. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @classmethod def from_yaml(cls, yaml_or_path: str) -> 'Configurable': \"\"\"Get config from YAML string or filepath. Args: yaml_or_path (str): File path or YAML string. \"\"\" if Path.is_file(yaml_or_path): return cls.read_yaml(yaml_or_path) return cls.from_config(srsly.yaml_loads(yaml_or_path))","title":"from_yaml"},{"location":"reference/text_explainability/generation/return_types/#read_json_2","text":"1 2 3 4 def read_json ( path : str , ** read_args ) -> 'Configurable' Read config from JSON file (GZIP JSON, JSONL or JSON). Parameters: Name Type Description Default path str File path. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @classmethod def read_json(cls, path: str, **read_args) -> 'Configurable': \"\"\"Read config from JSON file (GZIP JSON, JSONL or JSON). Args: path (str): File path. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" read_fn = srsly.read_json if path.endswith('.json.gz'): read_fn = srsly.read_gzip_json elif path.endswith('.jsonl'): read_fn = srsly.read_jsonl return cls.from_config(read_fn(path, **read_args))","title":"read_json"},{"location":"reference/text_explainability/generation/return_types/#read_yaml_2","text":"1 2 3 def read_yaml ( path : str ) -> 'Configurable' Read config from YAML file. Parameters: Name Type Description Default path str File path. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 @classmethod def read_yaml(cls, path: str) -> 'Configurable': \"\"\"Read config from YAML file. Args: path (str): File path. \"\"\" return cls.from_config(srsly.read_yaml(path))","title":"read_yaml"},{"location":"reference/text_explainability/generation/return_types/#instance-variables_2","text":"1 callargs 1 content 1 labels Get labels property. 1 labelset Get label names property. 1 meta 1 renderargs 1 scores Saved scores (e.g. feature importance). 1 subtype 1 type 1 used_features Get used features property.","title":"Instance variables"},{"location":"reference/text_explainability/generation/return_types/#methods_2","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/return_types/#get_raw_scores_1","text":"1 2 3 4 def get_raw_scores ( self , normalize : bool = False ) -> numpy . ndarray Get saved scores per label as np.ndarray . Parameters: Name Type Description Default normalize bool Normalize scores (ensure they sum to one). Defaults to False. False Returns: Type Description np.ndarray Scores. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def get_raw_scores(self, normalize: bool = False) -> np.ndarray: \"\"\"Get saved scores per label as `np.ndarray`. Args: normalize (bool, optional): Normalize scores (ensure they sum to one). Defaults to False. Returns: np.ndarray: Scores. \"\"\" def feature_scores(scores): if not isinstance(scores, np.ndarray): scores = np.array(scores) if normalize: return scores / scores.sum(axis=0) return scores if isinstance(self._scores, dict): return {k: feature_scores(v) for k, v in self._scores.items()} return feature_scores(self._scores)","title":"get_raw_scores"},{"location":"reference/text_explainability/generation/return_types/#get_scores_1","text":"1 2 3 4 def get_scores ( self , normalize : bool = False ) -> Dict [ Union [ str , int ], Tuple [ Union [ str , int ], Union [ float , int ]]] Get scores per label. Parameters: Name Type Description Default normalize bool Whether to normalize the scores (sum to one). Defaults to False. False Returns: Type Description Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]] Scores per label, if no labelset is not set, defaults to 'all' View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def get_scores(self, normalize: bool = False) -> Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: \"\"\"Get scores per label. Args: normalize (bool, optional): Whether to normalize the scores (sum to one). Defaults to False. Returns: Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: Scores per label, if no `labelset` is not set, defaults to 'all' \"\"\" # TODO: change to IDs all_scores = self.get_raw_scores(normalize=normalize) if self.labels is None: return {'all': [(feature, score_) for feature, score_ in zip(self.used_features, all_scores)]} if isinstance(self.used_features, dict): return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features[label], all_scores[i])] for i, label in enumerate(self.labels)} return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features, all_scores[i])] for i, label in enumerate(self.labels)}","title":"get_scores"},{"location":"reference/text_explainability/generation/return_types/#label_by_index_2","text":"1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx","title":"label_by_index"},{"location":"reference/text_explainability/generation/return_types/#to_config_2","text":"1 2 3 def to_config ( self ) Convert class information into config (configuration dictionary). Parameters: Name Type Description Default exclude List[str] Names of variables to exclude. None Returns: Type Description dict [description] View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def to_config(self): if hasattr(self, 'content'): _content = self.content() if callable(self.content) else self.content content = dict(recursive_to_dict(_content, include_class=False)) else: content = super().to_config(exclude=['_type', '_subtype', '_dict', '_callargs']) return {'META': self.meta, 'CONTENT': content}","title":"to_config"},{"location":"reference/text_explainability/generation/return_types/#to_json_2","text":"1 2 3 4 def to_json ( self , indent : int = 2 ) -> str Convert config to JSON-formatted string. Parameters: Name Type Description Default indent int Number of spaces to indent JSON. Defaults to 2. 2 Returns: Type Description str Config formatted as JSON. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_json(self, indent: int = 2) -> str: \"\"\"Convert config to JSON-formatted string. Args: indent (int, optional): Number of spaces to indent JSON. Defaults to 2. Returns: str: Config formatted as JSON. \"\"\" return srsly.json_dumps(self.to_config(), indent=indent)","title":"to_json"},{"location":"reference/text_explainability/generation/return_types/#to_yaml_2","text":"1 2 3 4 def to_yaml ( self , ** write_args ) -> str Convert config to YAML-formatted string. Parameters: Name Type Description Default **write_args None Optional arguments passed to srsly.yaml_dumps() None Returns: Type Description str Config formatted as YAML. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_yaml(self, **write_args) -> str: \"\"\"Convert config to YAML-formatted string. Args: **write_args: Optional arguments passed to `srsly.yaml_dumps()` Returns: str: Config formatted as YAML. \"\"\" return srsly.yaml_dumps(self.to_config(), **write_args)","title":"to_yaml"},{"location":"reference/text_explainability/generation/return_types/#write_json_2","text":"1 2 3 4 5 def write_json ( self , path : str , indent : int = 2 ) -> None Write class config to JSON. Parameters: Name Type Description Default path str Path to save to. If ends in .json.gz saves as GZIP JSON, .jsonl as JSONL or JSON by default. None indent int Number of spaces to indent JSON. Defaults to 2. 2 View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def write_json(self, path: str, indent: int = 2) -> None: \"\"\"Write class config to JSON. Args: path (str): Path to save to. If ends in `.json.gz` saves as GZIP JSON, `.jsonl` as JSONL or JSON by default. indent (int, optional): Number of spaces to indent JSON. Defaults to 2. \"\"\" write_fn = srsly.write_json if path.endswith('.json.gz'): write_fn = srsly.write_gzip_json elif path.endswith('.jsonl'): write_fn = srsly.write_jsonl write_fn(path, self.to_config(), indent=indent)","title":"write_json"},{"location":"reference/text_explainability/generation/return_types/#write_yaml_2","text":"1 2 3 4 5 def write_yaml ( self , path : str , ** write_args ) -> None Write class config to YAML. Parameters: Name Type Description Default path str Path to save to. None **write_args None Optional arguments passed to srsly.write_yaml() None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def write_yaml(self, path: str, **write_args) -> None: \"\"\"Write class config to YAML. Args: path (str): Path to save to. **write_args: Optional arguments passed to `srsly.write_yaml()` \"\"\" srsly.write_yaml(path, self.to_config(), **write_args)","title":"write_yaml"},{"location":"reference/text_explainability/generation/return_types/#instances","text":"1 2 3 4 5 6 7 class Instances ( instances , type : Optional [ str ] = 'global_explanation' , subtype : Optional [ str ] = 'prototypes' , callargs : Optional [ dict ] = None , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class Instances(BaseReturnType): def __init__(self, instances, type: Optional[str] = 'global_explanation', subtype: Optional[str] = 'prototypes', callargs: Optional[dict] = None, **kwargs): super().__init__(labels=None, labelset=None, type=type, subtype=subtype, callargs=callargs, **kwargs) self.instances = instances @property def content(self): return self.instances if isinstance(self.instances, dict) else {'instances': self.instances}","title":"Instances"},{"location":"reference/text_explainability/generation/return_types/#ancestors-in-mro_3","text":"text_explainability.generation.return_types.BaseReturnType genbase.MetaInfo genbase.Configurable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/return_types/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/text_explainability/generation/return_types/#from_config_3","text":"1 2 3 4 def from_config ( config : dict , ** kwargs ) -> 'Configurable' View Source 1 2 3 4 5 6 7 8 9 @classmethod def from_config(cls, config: dict, **kwargs) -> 'Configurable': config = {**config, **kwargs} _ = config.pop('__class__', None) return cls(**config)","title":"from_config"},{"location":"reference/text_explainability/generation/return_types/#from_json_3","text":"1 2 3 4 def from_json ( json_or_path : str , ** read_args ) -> 'Configurable' Get config from JSON string or filepath. Parameters: Name Type Description Default json_or_path str File path or JSON string. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @classmethod def from_json(cls, json_or_path: str, **read_args) -> 'Configurable': \"\"\"Get config from JSON string or filepath. Args: json_or_path (str): File path or JSON string. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" if Path.is_file(json_or_path): return cls.read_json(json_or_path, **read_args) return cls.from_config(srsly.json_loads(json_or_path))","title":"from_json"},{"location":"reference/text_explainability/generation/return_types/#from_yaml_3","text":"1 2 3 def from_yaml ( yaml_or_path : str ) -> 'Configurable' Get config from YAML string or filepath. Parameters: Name Type Description Default yaml_or_path str File path or YAML string. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @classmethod def from_yaml(cls, yaml_or_path: str) -> 'Configurable': \"\"\"Get config from YAML string or filepath. Args: yaml_or_path (str): File path or YAML string. \"\"\" if Path.is_file(yaml_or_path): return cls.read_yaml(yaml_or_path) return cls.from_config(srsly.yaml_loads(yaml_or_path))","title":"from_yaml"},{"location":"reference/text_explainability/generation/return_types/#read_json_3","text":"1 2 3 4 def read_json ( path : str , ** read_args ) -> 'Configurable' Read config from JSON file (GZIP JSON, JSONL or JSON). Parameters: Name Type Description Default path str File path. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @classmethod def read_json(cls, path: str, **read_args) -> 'Configurable': \"\"\"Read config from JSON file (GZIP JSON, JSONL or JSON). Args: path (str): File path. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" read_fn = srsly.read_json if path.endswith('.json.gz'): read_fn = srsly.read_gzip_json elif path.endswith('.jsonl'): read_fn = srsly.read_jsonl return cls.from_config(read_fn(path, **read_args))","title":"read_json"},{"location":"reference/text_explainability/generation/return_types/#read_yaml_3","text":"1 2 3 def read_yaml ( path : str ) -> 'Configurable' Read config from YAML file. Parameters: Name Type Description Default path str File path. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 @classmethod def read_yaml(cls, path: str) -> 'Configurable': \"\"\"Read config from YAML file. Args: path (str): File path. \"\"\" return cls.from_config(srsly.read_yaml(path))","title":"read_yaml"},{"location":"reference/text_explainability/generation/return_types/#instance-variables_3","text":"1 callargs 1 content 1 labels Get labels property. 1 labelset Get label names property. 1 meta 1 renderargs 1 subtype 1 type","title":"Instance variables"},{"location":"reference/text_explainability/generation/return_types/#methods_3","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/return_types/#label_by_index_3","text":"1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx","title":"label_by_index"},{"location":"reference/text_explainability/generation/return_types/#to_config_3","text":"1 2 3 def to_config ( self ) Convert class information into config (configuration dictionary). Parameters: Name Type Description Default exclude List[str] Names of variables to exclude. None Returns: Type Description dict [description] View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def to_config(self): if hasattr(self, 'content'): _content = self.content() if callable(self.content) else self.content content = dict(recursive_to_dict(_content, include_class=False)) else: content = super().to_config(exclude=['_type', '_subtype', '_dict', '_callargs']) return {'META': self.meta, 'CONTENT': content}","title":"to_config"},{"location":"reference/text_explainability/generation/return_types/#to_json_3","text":"1 2 3 4 def to_json ( self , indent : int = 2 ) -> str Convert config to JSON-formatted string. Parameters: Name Type Description Default indent int Number of spaces to indent JSON. Defaults to 2. 2 Returns: Type Description str Config formatted as JSON. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_json(self, indent: int = 2) -> str: \"\"\"Convert config to JSON-formatted string. Args: indent (int, optional): Number of spaces to indent JSON. Defaults to 2. Returns: str: Config formatted as JSON. \"\"\" return srsly.json_dumps(self.to_config(), indent=indent)","title":"to_json"},{"location":"reference/text_explainability/generation/return_types/#to_yaml_3","text":"1 2 3 4 def to_yaml ( self , ** write_args ) -> str Convert config to YAML-formatted string. Parameters: Name Type Description Default **write_args None Optional arguments passed to srsly.yaml_dumps() None Returns: Type Description str Config formatted as YAML. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_yaml(self, **write_args) -> str: \"\"\"Convert config to YAML-formatted string. Args: **write_args: Optional arguments passed to `srsly.yaml_dumps()` Returns: str: Config formatted as YAML. \"\"\" return srsly.yaml_dumps(self.to_config(), **write_args)","title":"to_yaml"},{"location":"reference/text_explainability/generation/return_types/#write_json_3","text":"1 2 3 4 5 def write_json ( self , path : str , indent : int = 2 ) -> None Write class config to JSON. Parameters: Name Type Description Default path str Path to save to. If ends in .json.gz saves as GZIP JSON, .jsonl as JSONL or JSON by default. None indent int Number of spaces to indent JSON. Defaults to 2. 2 View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def write_json(self, path: str, indent: int = 2) -> None: \"\"\"Write class config to JSON. Args: path (str): Path to save to. If ends in `.json.gz` saves as GZIP JSON, `.jsonl` as JSONL or JSON by default. indent (int, optional): Number of spaces to indent JSON. Defaults to 2. \"\"\" write_fn = srsly.write_json if path.endswith('.json.gz'): write_fn = srsly.write_gzip_json elif path.endswith('.jsonl'): write_fn = srsly.write_jsonl write_fn(path, self.to_config(), indent=indent)","title":"write_json"},{"location":"reference/text_explainability/generation/return_types/#write_yaml_3","text":"1 2 3 4 5 def write_yaml ( self , path : str , ** write_args ) -> None Write class config to YAML. Parameters: Name Type Description Default path str Path to save to. None **write_args None Optional arguments passed to srsly.write_yaml() None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def write_yaml(self, path: str, **write_args) -> None: \"\"\"Write class config to YAML. Args: path (str): Path to save to. **write_args: Optional arguments passed to `srsly.write_yaml()` \"\"\" srsly.write_yaml(path, self.to_config(), **write_args)","title":"write_yaml"},{"location":"reference/text_explainability/generation/return_types/#localdataexplanation","text":"1 2 3 4 5 class LocalDataExplanation ( provider : instancelib . instances . base . InstanceProvider , original_id : Optional [ ~ LT ] = None , sampled : bool = False ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class LocalDataExplanation: def __init__(self, provider: InstanceProvider, original_id: Optional[LT] = None, sampled: bool = False): \"\"\"Save the sampled/generated instances used to determine an explanation. Args: provider (InstanceProvider): Sampled or generated data, including original instance. original_id (Optional[LT], optional): ID of original instance; picks first if None. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. \"\"\" self._provider = provider original_id = next(iter(self._provider)) if original_id is None else original_id self._original_instance = copy.deepcopy(self._provider[original_id]) self._neighborhood_instances = copy.deepcopy(self._provider.get_children(self._original_instance)) self.sampled = sampled @property def original_instance(self): \"\"\"The instance for which the feature attribution scores were calculated.\"\"\" return self._original_instance @property def perturbed_instances(self): \"\"\"Perturbed versions of the original instance, if `sampled=False` during initialization.\"\"\" return None if self.sampled else self._neighborhood_instances @property def sampled_instances(self): \"\"\"Sampled instances, if `sampled=True` during initialization.\"\"\" return self._neighborhood_instances if self.sampled else None @property def neighborhood_instances(self): \"\"\"Instances in the neighborhood (either sampled or perturbed).\"\"\" return self._neighborhood_instances","title":"LocalDataExplanation"},{"location":"reference/text_explainability/generation/return_types/#descendants_2","text":"text_explainability.generation.return_types.FeatureAttribution text_explainability.generation.return_types.Rules","title":"Descendants"},{"location":"reference/text_explainability/generation/return_types/#instance-variables_4","text":"1 neighborhood_instances Instances in the neighborhood (either sampled or perturbed). 1 original_instance The instance for which the feature attribution scores were calculated. 1 perturbed_instances Perturbed versions of the original instance, if sampled=False during initialization. 1 sampled_instances Sampled instances, if sampled=True during initialization.","title":"Instance variables"},{"location":"reference/text_explainability/generation/return_types/#readabledatamixin","text":"1 2 3 4 5 class ReadableDataMixin ( / , * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class ReadableDataMixin: @property def used_features(self): \"\"\"Names of features of the original instance.\"\"\" if hasattr(self.original_instance, 'tokenized'): if isinstance(self._used_features, dict): return {k: [self.original_instance.tokenized[i] for i in v] for k, v in self._used_features.items()} return [self.original_instance.tokenized[i] for i in self._used_features] return list(self._used_features) def __repr__(self) -> str: sampled_or_perturbed = 'sampled' if self.sampled else 'perturbed' n = sum(1 for _ in self.neighborhood_instances) labels = [self.label_by_index(label) for label in self.labels] if self.labels is not None else None return f'{self.__class__.__name__}(labels={labels}, ' + \\ f'used_features={self.used_features}, n_{sampled_or_perturbed}_instances={n})'","title":"ReadableDataMixin"},{"location":"reference/text_explainability/generation/return_types/#descendants_3","text":"text_explainability.generation.return_types.FeatureAttribution text_explainability.generation.return_types.Rules","title":"Descendants"},{"location":"reference/text_explainability/generation/return_types/#instance-variables_5","text":"1 used_features Names of features of the original instance.","title":"Instance variables"},{"location":"reference/text_explainability/generation/return_types/#rules","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 class Rules ( provider : instancelib . instances . base . InstanceProvider , rules : Union [ Sequence [ str ], text_explainability . generation . surrogate . TreeSurrogate , text_explainability . generation . surrogate . RuleSurrogate ], used_features : Union [ Sequence [ str ], Sequence [ int ], NoneType ] = None , labels : Optional [ Sequence [ int ]] = None , labelset : Optional [ Sequence [ str ]] = None , original_id : Optional [ ~ LT ] = None , sampled : bool = False , type : Optional [ str ] = 'local_explanation' , subtype : Optional [ str ] = 'rules' , callargs : Optional [ dict ] = None , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class Rules(ReadableDataMixin, UsedFeaturesMixin, BaseReturnType, LocalDataExplanation): def __init__(self, provider: InstanceProvider, rules: Union[Sequence[str], TreeSurrogate, RuleSurrogate], used_features: Optional[Union[Sequence[str], Sequence[int]]] = None, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, original_id: Optional[LT] = None, sampled: bool = False, type: Optional[str] = 'local_explanation', subtype: Optional[str] = 'rules', callargs: Optional[dict] = None, **kwargs): \"\"\"Rule-based return type. Args: provider (InstanceProvider): Sampled or generated data, including original instance. rules (Union[Sequence[str], TreeSurrogate, RuleSurrogate]): Rules applicable. used_features (Optional[Union[Sequence[str], Sequence[int]]]): Used features per label. Defaults to None. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. original_id (Optional[LT], optional): ID of original instance; picks first if None. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. type (Optional[str]): Type description. Defaults to 'base'. subtype (Optional[str], optional): Subtype description. Defaults to None. callargs (Optional[dict], optional): Call arguments for reproducibility. Defaults to None. **kwargs: Optional meta descriptors. \"\"\" LocalDataExplanation.__init__(self, provider=provider, original_id=original_id, sampled=sampled) BaseReturnType.__init__(self, labels=labels, labelset=labelset, type=type, subtype=subtype, callargs=callargs, **kwargs) if used_features is None: used_features = list(range(len(self.original_instance.tokenized))) self._used_features = copy.deepcopy(used_features) self._rules = self._extract_rules(rules) def _extract_rules(self, rules: Union[Sequence[str], TreeSurrogate, RuleSurrogate]): if isinstance(rules, (TreeSurrogate, RuleSurrogate)): from skrules.rule import replace_feature_name from skrules.skope_rules import BASE_FEATURE_NAME feature_dict = {BASE_FEATURE_NAME + str(i): feat for i, feat in enumerate(self.used_features)} return [(replace_feature_name(rule, feature_dict), perf) for rule, perf in rules.rules] print(rules) raise NotImplementedError('TODO: Support lists of rules') @property def rules(self): return self._rules @property def content(self): return self.rules","title":"Rules"},{"location":"reference/text_explainability/generation/return_types/#ancestors-in-mro_4","text":"text_explainability.generation.return_types.ReadableDataMixin text_explainability.generation.return_types.UsedFeaturesMixin text_explainability.generation.return_types.BaseReturnType genbase.MetaInfo genbase.Configurable text_explainability.generation.return_types.LocalDataExplanation","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/return_types/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/text_explainability/generation/return_types/#from_config_4","text":"1 2 3 4 def from_config ( config : dict , ** kwargs ) -> 'Configurable' View Source 1 2 3 4 5 6 7 8 9 @classmethod def from_config(cls, config: dict, **kwargs) -> 'Configurable': config = {**config, **kwargs} _ = config.pop('__class__', None) return cls(**config)","title":"from_config"},{"location":"reference/text_explainability/generation/return_types/#from_json_4","text":"1 2 3 4 def from_json ( json_or_path : str , ** read_args ) -> 'Configurable' Get config from JSON string or filepath. Parameters: Name Type Description Default json_or_path str File path or JSON string. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @classmethod def from_json(cls, json_or_path: str, **read_args) -> 'Configurable': \"\"\"Get config from JSON string or filepath. Args: json_or_path (str): File path or JSON string. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" if Path.is_file(json_or_path): return cls.read_json(json_or_path, **read_args) return cls.from_config(srsly.json_loads(json_or_path))","title":"from_json"},{"location":"reference/text_explainability/generation/return_types/#from_yaml_4","text":"1 2 3 def from_yaml ( yaml_or_path : str ) -> 'Configurable' Get config from YAML string or filepath. Parameters: Name Type Description Default yaml_or_path str File path or YAML string. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @classmethod def from_yaml(cls, yaml_or_path: str) -> 'Configurable': \"\"\"Get config from YAML string or filepath. Args: yaml_or_path (str): File path or YAML string. \"\"\" if Path.is_file(yaml_or_path): return cls.read_yaml(yaml_or_path) return cls.from_config(srsly.yaml_loads(yaml_or_path))","title":"from_yaml"},{"location":"reference/text_explainability/generation/return_types/#read_json_4","text":"1 2 3 4 def read_json ( path : str , ** read_args ) -> 'Configurable' Read config from JSON file (GZIP JSON, JSONL or JSON). Parameters: Name Type Description Default path str File path. None **read_args None Optional arguments passed to srsly.read_json() / srsly.read_jsonl() / srsly.read_gzip_json . None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @classmethod def read_json(cls, path: str, **read_args) -> 'Configurable': \"\"\"Read config from JSON file (GZIP JSON, JSONL or JSON). Args: path (str): File path. **read_args: Optional arguments passed to `srsly.read_json()`/`srsly.read_jsonl()`/`srsly.read_gzip_json`. \"\"\" read_fn = srsly.read_json if path.endswith('.json.gz'): read_fn = srsly.read_gzip_json elif path.endswith('.jsonl'): read_fn = srsly.read_jsonl return cls.from_config(read_fn(path, **read_args))","title":"read_json"},{"location":"reference/text_explainability/generation/return_types/#read_yaml_4","text":"1 2 3 def read_yaml ( path : str ) -> 'Configurable' Read config from YAML file. Parameters: Name Type Description Default path str File path. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 @classmethod def read_yaml(cls, path: str) -> 'Configurable': \"\"\"Read config from YAML file. Args: path (str): File path. \"\"\" return cls.from_config(srsly.read_yaml(path))","title":"read_yaml"},{"location":"reference/text_explainability/generation/return_types/#instance-variables_6","text":"1 callargs 1 content 1 labels Get labels property. 1 labelset Get label names property. 1 meta 1 neighborhood_instances Instances in the neighborhood (either sampled or perturbed). 1 original_instance The instance for which the feature attribution scores were calculated. 1 perturbed_instances Perturbed versions of the original instance, if sampled=False during initialization. 1 renderargs 1 rules 1 sampled_instances Sampled instances, if sampled=True during initialization. 1 subtype 1 type 1 used_features Names of features of the original instance.","title":"Instance variables"},{"location":"reference/text_explainability/generation/return_types/#methods_4","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/return_types/#label_by_index_4","text":"1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx","title":"label_by_index"},{"location":"reference/text_explainability/generation/return_types/#to_config_4","text":"1 2 3 def to_config ( self ) Convert class information into config (configuration dictionary). Parameters: Name Type Description Default exclude List[str] Names of variables to exclude. None Returns: Type Description dict [description] View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def to_config(self): if hasattr(self, 'content'): _content = self.content() if callable(self.content) else self.content content = dict(recursive_to_dict(_content, include_class=False)) else: content = super().to_config(exclude=['_type', '_subtype', '_dict', '_callargs']) return {'META': self.meta, 'CONTENT': content}","title":"to_config"},{"location":"reference/text_explainability/generation/return_types/#to_json_4","text":"1 2 3 4 def to_json ( self , indent : int = 2 ) -> str Convert config to JSON-formatted string. Parameters: Name Type Description Default indent int Number of spaces to indent JSON. Defaults to 2. 2 Returns: Type Description str Config formatted as JSON. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_json(self, indent: int = 2) -> str: \"\"\"Convert config to JSON-formatted string. Args: indent (int, optional): Number of spaces to indent JSON. Defaults to 2. Returns: str: Config formatted as JSON. \"\"\" return srsly.json_dumps(self.to_config(), indent=indent)","title":"to_json"},{"location":"reference/text_explainability/generation/return_types/#to_yaml_4","text":"1 2 3 4 def to_yaml ( self , ** write_args ) -> str Convert config to YAML-formatted string. Parameters: Name Type Description Default **write_args None Optional arguments passed to srsly.yaml_dumps() None Returns: Type Description str Config formatted as YAML. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_yaml(self, **write_args) -> str: \"\"\"Convert config to YAML-formatted string. Args: **write_args: Optional arguments passed to `srsly.yaml_dumps()` Returns: str: Config formatted as YAML. \"\"\" return srsly.yaml_dumps(self.to_config(), **write_args)","title":"to_yaml"},{"location":"reference/text_explainability/generation/return_types/#write_json_4","text":"1 2 3 4 5 def write_json ( self , path : str , indent : int = 2 ) -> None Write class config to JSON. Parameters: Name Type Description Default path str Path to save to. If ends in .json.gz saves as GZIP JSON, .jsonl as JSONL or JSON by default. None indent int Number of spaces to indent JSON. Defaults to 2. 2 View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def write_json(self, path: str, indent: int = 2) -> None: \"\"\"Write class config to JSON. Args: path (str): Path to save to. If ends in `.json.gz` saves as GZIP JSON, `.jsonl` as JSONL or JSON by default. indent (int, optional): Number of spaces to indent JSON. Defaults to 2. \"\"\" write_fn = srsly.write_json if path.endswith('.json.gz'): write_fn = srsly.write_gzip_json elif path.endswith('.jsonl'): write_fn = srsly.write_jsonl write_fn(path, self.to_config(), indent=indent)","title":"write_json"},{"location":"reference/text_explainability/generation/return_types/#write_yaml_4","text":"1 2 3 4 5 def write_yaml ( self , path : str , ** write_args ) -> None Write class config to YAML. Parameters: Name Type Description Default path str Path to save to. None **write_args None Optional arguments passed to srsly.write_yaml() None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def write_yaml(self, path: str, **write_args) -> None: \"\"\"Write class config to YAML. Args: path (str): Path to save to. **write_args: Optional arguments passed to `srsly.write_yaml()` \"\"\" srsly.write_yaml(path, self.to_config(), **write_args)","title":"write_yaml"},{"location":"reference/text_explainability/generation/return_types/#usedfeaturesmixin","text":"1 2 3 4 5 class UsedFeaturesMixin ( / , * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 class UsedFeaturesMixin: @property def used_features(self): \"\"\"Get used features property.\"\"\" return self._used_features","title":"UsedFeaturesMixin"},{"location":"reference/text_explainability/generation/return_types/#descendants_4","text":"text_explainability.generation.return_types.FeatureList text_explainability.generation.return_types.Rules","title":"Descendants"},{"location":"reference/text_explainability/generation/return_types/#instance-variables_7","text":"1 used_features Get used features property.","title":"Instance variables"},{"location":"reference/text_explainability/generation/surrogate/","text":"Module text_explainability.generation.surrogate Wrappers for surrogate models, used for local/global explanations. Todo: 1 2 3 * Add documentation * Differentiate between classifiers and regressors * Extract rules from decision tree (https://mljar.com/blog/extract-rules-decision-tree/) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 \"\"\"Wrappers for surrogate models, used for local/global explanations. Todo: * Add documentation * Differentiate between classifiers and regressors * Extract rules from decision tree (https://mljar.com/blog/extract-rules-decision-tree/) \"\"\" from typing import Optional, Sequence import numpy as np from genbase import Readable from sklearn.base import clone class BaseSurrogate(Readable): def __init__(self, model): super().__init__() self._model = clone(model) def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self def predict(self, X): return self._model.predict(X) @property def feature_importances(self): raise NotImplementedError class LinearSurrogate(BaseSurrogate): def __init__(self, model): \"\"\"Wrapper around sklearn linear model for usage in local/global surrogate models.\"\"\" super().__init__(model) if hasattr(self._model, 'alpha'): self.__alpha_original = self._model.alpha @property def coef(self): return self._model.coef_ @property def feature_importances(self): return self.coef @property def intercept(self): return self._model.intercept_ def score(self, X, y, weights=None): return self._model.score(X, y, sample_weight=weights) def alpha_zero(self): if hasattr(self._model, 'alpha'): self._model.alpha = 0 def alpha_reset(self): if hasattr(self._model, 'alpha'): self._model.alpha = self.__alpha_original @property def fit_intercept(self): return self._model.fit_intercept @fit_intercept.setter def fit_intercept(self, fit_intercept): self._model.fit_intercept = fit_intercept class TreeSurrogate(BaseSurrogate): \"\"\"Wrapper around sklearn tree model for usage in local/global surrogate models.\"\"\" @property def feature_importances(self): return self._model.feature_importances_ @property def classes(self): return self._model.classes_ @property def max_rule_size(self): return self._model.max_depth @max_rule_size.setter def max_rule_size(self, size: Optional[int]): self._model.set_params(max_depth=size) @property def rules(self): if not hasattr(self, '_rules') or self._rules is None: self.to_rules() return self._rules def decision_path(self, X): if not isinstance(X, np.ndarray): X = np.array(X) if X.ndim < 2: X = X.reshape(1, -1) return self._model.decision_path(X).toarray() def features(self, tokens_to_map: Optional[Sequence[str]] = None): def map_token(token): if tokens_to_map is None: return token return tokens_to_map[token] return [None if f < 0 else map_token[f] for f in self._model.tree_.feature] def leaf_classes(self): # TODO: check if truly classification return [self._model.classes_[np.argmax(self._model.tree_.value[i])] if f < 0 else None for i, f in enumerate(self._model.tree_.feature)] def to_rules(self): from skrules.rule import Rule from skrules.skope_rules import BASE_FEATURE_NAME, SkopeRules feature_names = [BASE_FEATURE_NAME + str(i) for i in range(self._model.n_features_)] rules = SkopeRules._tree_to_rules(None, tree=self._model, feature_names=feature_names) # TODO: add performance metrics and output label self._rules = [tuple(rule) for rule in [Rule(r) for r in rules]] return self._rules class RuleSurrogate(BaseSurrogate): \"\"\"Wrapper around `SkopeRules`_ model for usage in local/global surrogate models. _SkopeRules: https://github.com/scikit-learn-contrib/skope-rules \"\"\" @property def rules(self): return self._model.rules_ @property def feature_names(self): return self._model.feature_names @feature_names.setter def feature_names(self, feature_names: Sequence[str]): self._model.feature_names = feature_names def score_top_rules(self, X): return self._model.score_top_rules(X) Classes BaseSurrogate 1 2 3 class BaseSurrogate ( model ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class BaseSurrogate(Readable): def __init__(self, model): super().__init__() self._model = clone(model) def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self def predict(self, X): return self._model.predict(X) @property def feature_importances(self): raise NotImplementedError Ancestors (in MRO) genbase.Readable Descendants text_explainability.generation.surrogate.LinearSurrogate text_explainability.generation.surrogate.TreeSurrogate text_explainability.generation.surrogate.RuleSurrogate Instance variables 1 feature_importances Methods fit 1 2 3 4 5 6 def fit ( self , X , y , weights = None ) View Source 1 2 3 4 5 def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self predict 1 2 3 4 def predict ( self , X ) View Source 1 2 3 def predict(self, X): return self._model.predict(X) LinearSurrogate 1 2 3 class LinearSurrogate ( model ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class LinearSurrogate(BaseSurrogate): def __init__(self, model): \"\"\"Wrapper around sklearn linear model for usage in local/global surrogate models.\"\"\" super().__init__(model) if hasattr(self._model, 'alpha'): self.__alpha_original = self._model.alpha @property def coef(self): return self._model.coef_ @property def feature_importances(self): return self.coef @property def intercept(self): return self._model.intercept_ def score(self, X, y, weights=None): return self._model.score(X, y, sample_weight=weights) def alpha_zero(self): if hasattr(self._model, 'alpha'): self._model.alpha = 0 def alpha_reset(self): if hasattr(self._model, 'alpha'): self._model.alpha = self.__alpha_original @property def fit_intercept(self): return self._model.fit_intercept @fit_intercept.setter def fit_intercept(self, fit_intercept): self._model.fit_intercept = fit_intercept Ancestors (in MRO) text_explainability.generation.surrogate.BaseSurrogate genbase.Readable Instance variables 1 coef 1 feature_importances 1 fit_intercept 1 intercept Methods alpha_reset 1 2 3 def alpha_reset ( self ) View Source 1 2 3 4 5 def alpha_reset(self): if hasattr(self._model, 'alpha'): self._model.alpha = self.__alpha_original alpha_zero 1 2 3 def alpha_zero ( self ) View Source 1 2 3 4 5 def alpha_zero(self): if hasattr(self._model, 'alpha'): self._model.alpha = 0 fit 1 2 3 4 5 6 def fit ( self , X , y , weights = None ) View Source 1 2 3 4 5 def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self predict 1 2 3 4 def predict ( self , X ) View Source 1 2 3 def predict(self, X): return self._model.predict(X) score 1 2 3 4 5 6 def score ( self , X , y , weights = None ) View Source 1 2 3 def score(self, X, y, weights=None): return self._model.score(X, y, sample_weight=weights) RuleSurrogate 1 2 3 class RuleSurrogate ( model ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class RuleSurrogate(BaseSurrogate): \"\"\"Wrapper around `SkopeRules`_ model for usage in local/global surrogate models. _SkopeRules: https://github.com/scikit-learn-contrib/skope-rules \"\"\" @property def rules(self): return self._model.rules_ @property def feature_names(self): return self._model.feature_names @feature_names.setter def feature_names(self, feature_names: Sequence[str]): self._model.feature_names = feature_names def score_top_rules(self, X): return self._model.score_top_rules(X) Ancestors (in MRO) text_explainability.generation.surrogate.BaseSurrogate genbase.Readable Instance variables 1 feature_importances 1 feature_names 1 rules Methods fit 1 2 3 4 5 6 def fit ( self , X , y , weights = None ) View Source 1 2 3 4 5 def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self predict 1 2 3 4 def predict ( self , X ) View Source 1 2 3 def predict(self, X): return self._model.predict(X) score_top_rules 1 2 3 4 def score_top_rules ( self , X ) View Source 1 2 3 def score_top_rules(self, X): return self._model.score_top_rules(X) TreeSurrogate 1 2 3 class TreeSurrogate ( model ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class TreeSurrogate(BaseSurrogate): \"\"\"Wrapper around sklearn tree model for usage in local/global surrogate models.\"\"\" @property def feature_importances(self): return self._model.feature_importances_ @property def classes(self): return self._model.classes_ @property def max_rule_size(self): return self._model.max_depth @max_rule_size.setter def max_rule_size(self, size: Optional[int]): self._model.set_params(max_depth=size) @property def rules(self): if not hasattr(self, '_rules') or self._rules is None: self.to_rules() return self._rules def decision_path(self, X): if not isinstance(X, np.ndarray): X = np.array(X) if X.ndim < 2: X = X.reshape(1, -1) return self._model.decision_path(X).toarray() def features(self, tokens_to_map: Optional[Sequence[str]] = None): def map_token(token): if tokens_to_map is None: return token return tokens_to_map[token] return [None if f < 0 else map_token[f] for f in self._model.tree_.feature] def leaf_classes(self): # TODO: check if truly classification return [self._model.classes_[np.argmax(self._model.tree_.value[i])] if f < 0 else None for i, f in enumerate(self._model.tree_.feature)] def to_rules(self): from skrules.rule import Rule from skrules.skope_rules import BASE_FEATURE_NAME, SkopeRules feature_names = [BASE_FEATURE_NAME + str(i) for i in range(self._model.n_features_)] rules = SkopeRules._tree_to_rules(None, tree=self._model, feature_names=feature_names) # TODO: add performance metrics and output label self._rules = [tuple(rule) for rule in [Rule(r) for r in rules]] return self._rules Ancestors (in MRO) text_explainability.generation.surrogate.BaseSurrogate genbase.Readable Instance variables 1 classes 1 feature_importances 1 max_rule_size 1 rules Methods decision_path 1 2 3 4 def decision_path ( self , X ) View Source 1 2 3 4 5 6 7 8 9 10 11 def decision_path(self, X): if not isinstance(X, np.ndarray): X = np.array(X) if X.ndim < 2: X = X.reshape(1, -1) return self._model.decision_path(X).toarray() features 1 2 3 4 def features ( self , tokens_to_map : Optional [ Sequence [ str ]] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 def features(self, tokens_to_map: Optional[Sequence[str]] = None): def map_token(token): if tokens_to_map is None: return token return tokens_to_map[token] return [None if f < 0 else map_token[f] for f in self._model.tree_.feature] fit 1 2 3 4 5 6 def fit ( self , X , y , weights = None ) View Source 1 2 3 4 5 def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self leaf_classes 1 2 3 def leaf_classes ( self ) View Source 1 2 3 4 5 6 7 def leaf_classes(self): # TODO: check if truly classification return [self._model.classes_[np.argmax(self._model.tree_.value[i])] if f < 0 else None for i, f in enumerate(self._model.tree_.feature)] predict 1 2 3 4 def predict ( self , X ) View Source 1 2 3 def predict(self, X): return self._model.predict(X) to_rules 1 2 3 def to_rules ( self ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_rules(self): from skrules.rule import Rule from skrules.skope_rules import BASE_FEATURE_NAME, SkopeRules feature_names = [BASE_FEATURE_NAME + str(i) for i in range(self._model.n_features_)] rules = SkopeRules._tree_to_rules(None, tree=self._model, feature_names=feature_names) # TODO: add performance metrics and output label self._rules = [tuple(rule) for rule in [Rule(r) for r in rules]] return self._rules","title":"Surrogate"},{"location":"reference/text_explainability/generation/surrogate/#module-text_explainabilitygenerationsurrogate","text":"Wrappers for surrogate models, used for local/global explanations. Todo: 1 2 3 * Add documentation * Differentiate between classifiers and regressors * Extract rules from decision tree (https://mljar.com/blog/extract-rules-decision-tree/) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 \"\"\"Wrappers for surrogate models, used for local/global explanations. Todo: * Add documentation * Differentiate between classifiers and regressors * Extract rules from decision tree (https://mljar.com/blog/extract-rules-decision-tree/) \"\"\" from typing import Optional, Sequence import numpy as np from genbase import Readable from sklearn.base import clone class BaseSurrogate(Readable): def __init__(self, model): super().__init__() self._model = clone(model) def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self def predict(self, X): return self._model.predict(X) @property def feature_importances(self): raise NotImplementedError class LinearSurrogate(BaseSurrogate): def __init__(self, model): \"\"\"Wrapper around sklearn linear model for usage in local/global surrogate models.\"\"\" super().__init__(model) if hasattr(self._model, 'alpha'): self.__alpha_original = self._model.alpha @property def coef(self): return self._model.coef_ @property def feature_importances(self): return self.coef @property def intercept(self): return self._model.intercept_ def score(self, X, y, weights=None): return self._model.score(X, y, sample_weight=weights) def alpha_zero(self): if hasattr(self._model, 'alpha'): self._model.alpha = 0 def alpha_reset(self): if hasattr(self._model, 'alpha'): self._model.alpha = self.__alpha_original @property def fit_intercept(self): return self._model.fit_intercept @fit_intercept.setter def fit_intercept(self, fit_intercept): self._model.fit_intercept = fit_intercept class TreeSurrogate(BaseSurrogate): \"\"\"Wrapper around sklearn tree model for usage in local/global surrogate models.\"\"\" @property def feature_importances(self): return self._model.feature_importances_ @property def classes(self): return self._model.classes_ @property def max_rule_size(self): return self._model.max_depth @max_rule_size.setter def max_rule_size(self, size: Optional[int]): self._model.set_params(max_depth=size) @property def rules(self): if not hasattr(self, '_rules') or self._rules is None: self.to_rules() return self._rules def decision_path(self, X): if not isinstance(X, np.ndarray): X = np.array(X) if X.ndim < 2: X = X.reshape(1, -1) return self._model.decision_path(X).toarray() def features(self, tokens_to_map: Optional[Sequence[str]] = None): def map_token(token): if tokens_to_map is None: return token return tokens_to_map[token] return [None if f < 0 else map_token[f] for f in self._model.tree_.feature] def leaf_classes(self): # TODO: check if truly classification return [self._model.classes_[np.argmax(self._model.tree_.value[i])] if f < 0 else None for i, f in enumerate(self._model.tree_.feature)] def to_rules(self): from skrules.rule import Rule from skrules.skope_rules import BASE_FEATURE_NAME, SkopeRules feature_names = [BASE_FEATURE_NAME + str(i) for i in range(self._model.n_features_)] rules = SkopeRules._tree_to_rules(None, tree=self._model, feature_names=feature_names) # TODO: add performance metrics and output label self._rules = [tuple(rule) for rule in [Rule(r) for r in rules]] return self._rules class RuleSurrogate(BaseSurrogate): \"\"\"Wrapper around `SkopeRules`_ model for usage in local/global surrogate models. _SkopeRules: https://github.com/scikit-learn-contrib/skope-rules \"\"\" @property def rules(self): return self._model.rules_ @property def feature_names(self): return self._model.feature_names @feature_names.setter def feature_names(self, feature_names: Sequence[str]): self._model.feature_names = feature_names def score_top_rules(self, X): return self._model.score_top_rules(X)","title":"Module text_explainability.generation.surrogate"},{"location":"reference/text_explainability/generation/surrogate/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/generation/surrogate/#basesurrogate","text":"1 2 3 class BaseSurrogate ( model ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class BaseSurrogate(Readable): def __init__(self, model): super().__init__() self._model = clone(model) def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self def predict(self, X): return self._model.predict(X) @property def feature_importances(self): raise NotImplementedError","title":"BaseSurrogate"},{"location":"reference/text_explainability/generation/surrogate/#ancestors-in-mro","text":"genbase.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/surrogate/#descendants","text":"text_explainability.generation.surrogate.LinearSurrogate text_explainability.generation.surrogate.TreeSurrogate text_explainability.generation.surrogate.RuleSurrogate","title":"Descendants"},{"location":"reference/text_explainability/generation/surrogate/#instance-variables","text":"1 feature_importances","title":"Instance variables"},{"location":"reference/text_explainability/generation/surrogate/#methods","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/surrogate/#fit","text":"1 2 3 4 5 6 def fit ( self , X , y , weights = None ) View Source 1 2 3 4 5 def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self","title":"fit"},{"location":"reference/text_explainability/generation/surrogate/#predict","text":"1 2 3 4 def predict ( self , X ) View Source 1 2 3 def predict(self, X): return self._model.predict(X)","title":"predict"},{"location":"reference/text_explainability/generation/surrogate/#linearsurrogate","text":"1 2 3 class LinearSurrogate ( model ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class LinearSurrogate(BaseSurrogate): def __init__(self, model): \"\"\"Wrapper around sklearn linear model for usage in local/global surrogate models.\"\"\" super().__init__(model) if hasattr(self._model, 'alpha'): self.__alpha_original = self._model.alpha @property def coef(self): return self._model.coef_ @property def feature_importances(self): return self.coef @property def intercept(self): return self._model.intercept_ def score(self, X, y, weights=None): return self._model.score(X, y, sample_weight=weights) def alpha_zero(self): if hasattr(self._model, 'alpha'): self._model.alpha = 0 def alpha_reset(self): if hasattr(self._model, 'alpha'): self._model.alpha = self.__alpha_original @property def fit_intercept(self): return self._model.fit_intercept @fit_intercept.setter def fit_intercept(self, fit_intercept): self._model.fit_intercept = fit_intercept","title":"LinearSurrogate"},{"location":"reference/text_explainability/generation/surrogate/#ancestors-in-mro_1","text":"text_explainability.generation.surrogate.BaseSurrogate genbase.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/surrogate/#instance-variables_1","text":"1 coef 1 feature_importances 1 fit_intercept 1 intercept","title":"Instance variables"},{"location":"reference/text_explainability/generation/surrogate/#methods_1","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/surrogate/#alpha_reset","text":"1 2 3 def alpha_reset ( self ) View Source 1 2 3 4 5 def alpha_reset(self): if hasattr(self._model, 'alpha'): self._model.alpha = self.__alpha_original","title":"alpha_reset"},{"location":"reference/text_explainability/generation/surrogate/#alpha_zero","text":"1 2 3 def alpha_zero ( self ) View Source 1 2 3 4 5 def alpha_zero(self): if hasattr(self._model, 'alpha'): self._model.alpha = 0","title":"alpha_zero"},{"location":"reference/text_explainability/generation/surrogate/#fit_1","text":"1 2 3 4 5 6 def fit ( self , X , y , weights = None ) View Source 1 2 3 4 5 def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self","title":"fit"},{"location":"reference/text_explainability/generation/surrogate/#predict_1","text":"1 2 3 4 def predict ( self , X ) View Source 1 2 3 def predict(self, X): return self._model.predict(X)","title":"predict"},{"location":"reference/text_explainability/generation/surrogate/#score","text":"1 2 3 4 5 6 def score ( self , X , y , weights = None ) View Source 1 2 3 def score(self, X, y, weights=None): return self._model.score(X, y, sample_weight=weights)","title":"score"},{"location":"reference/text_explainability/generation/surrogate/#rulesurrogate","text":"1 2 3 class RuleSurrogate ( model ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class RuleSurrogate(BaseSurrogate): \"\"\"Wrapper around `SkopeRules`_ model for usage in local/global surrogate models. _SkopeRules: https://github.com/scikit-learn-contrib/skope-rules \"\"\" @property def rules(self): return self._model.rules_ @property def feature_names(self): return self._model.feature_names @feature_names.setter def feature_names(self, feature_names: Sequence[str]): self._model.feature_names = feature_names def score_top_rules(self, X): return self._model.score_top_rules(X)","title":"RuleSurrogate"},{"location":"reference/text_explainability/generation/surrogate/#ancestors-in-mro_2","text":"text_explainability.generation.surrogate.BaseSurrogate genbase.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/surrogate/#instance-variables_2","text":"1 feature_importances 1 feature_names 1 rules","title":"Instance variables"},{"location":"reference/text_explainability/generation/surrogate/#methods_2","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/surrogate/#fit_2","text":"1 2 3 4 5 6 def fit ( self , X , y , weights = None ) View Source 1 2 3 4 5 def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self","title":"fit"},{"location":"reference/text_explainability/generation/surrogate/#predict_2","text":"1 2 3 4 def predict ( self , X ) View Source 1 2 3 def predict(self, X): return self._model.predict(X)","title":"predict"},{"location":"reference/text_explainability/generation/surrogate/#score_top_rules","text":"1 2 3 4 def score_top_rules ( self , X ) View Source 1 2 3 def score_top_rules(self, X): return self._model.score_top_rules(X)","title":"score_top_rules"},{"location":"reference/text_explainability/generation/surrogate/#treesurrogate","text":"1 2 3 class TreeSurrogate ( model ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class TreeSurrogate(BaseSurrogate): \"\"\"Wrapper around sklearn tree model for usage in local/global surrogate models.\"\"\" @property def feature_importances(self): return self._model.feature_importances_ @property def classes(self): return self._model.classes_ @property def max_rule_size(self): return self._model.max_depth @max_rule_size.setter def max_rule_size(self, size: Optional[int]): self._model.set_params(max_depth=size) @property def rules(self): if not hasattr(self, '_rules') or self._rules is None: self.to_rules() return self._rules def decision_path(self, X): if not isinstance(X, np.ndarray): X = np.array(X) if X.ndim < 2: X = X.reshape(1, -1) return self._model.decision_path(X).toarray() def features(self, tokens_to_map: Optional[Sequence[str]] = None): def map_token(token): if tokens_to_map is None: return token return tokens_to_map[token] return [None if f < 0 else map_token[f] for f in self._model.tree_.feature] def leaf_classes(self): # TODO: check if truly classification return [self._model.classes_[np.argmax(self._model.tree_.value[i])] if f < 0 else None for i, f in enumerate(self._model.tree_.feature)] def to_rules(self): from skrules.rule import Rule from skrules.skope_rules import BASE_FEATURE_NAME, SkopeRules feature_names = [BASE_FEATURE_NAME + str(i) for i in range(self._model.n_features_)] rules = SkopeRules._tree_to_rules(None, tree=self._model, feature_names=feature_names) # TODO: add performance metrics and output label self._rules = [tuple(rule) for rule in [Rule(r) for r in rules]] return self._rules","title":"TreeSurrogate"},{"location":"reference/text_explainability/generation/surrogate/#ancestors-in-mro_3","text":"text_explainability.generation.surrogate.BaseSurrogate genbase.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/surrogate/#instance-variables_3","text":"1 classes 1 feature_importances 1 max_rule_size 1 rules","title":"Instance variables"},{"location":"reference/text_explainability/generation/surrogate/#methods_3","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/surrogate/#decision_path","text":"1 2 3 4 def decision_path ( self , X ) View Source 1 2 3 4 5 6 7 8 9 10 11 def decision_path(self, X): if not isinstance(X, np.ndarray): X = np.array(X) if X.ndim < 2: X = X.reshape(1, -1) return self._model.decision_path(X).toarray()","title":"decision_path"},{"location":"reference/text_explainability/generation/surrogate/#features","text":"1 2 3 4 def features ( self , tokens_to_map : Optional [ Sequence [ str ]] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 def features(self, tokens_to_map: Optional[Sequence[str]] = None): def map_token(token): if tokens_to_map is None: return token return tokens_to_map[token] return [None if f < 0 else map_token[f] for f in self._model.tree_.feature]","title":"features"},{"location":"reference/text_explainability/generation/surrogate/#fit_3","text":"1 2 3 4 5 6 def fit ( self , X , y , weights = None ) View Source 1 2 3 4 5 def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self","title":"fit"},{"location":"reference/text_explainability/generation/surrogate/#leaf_classes","text":"1 2 3 def leaf_classes ( self ) View Source 1 2 3 4 5 6 7 def leaf_classes(self): # TODO: check if truly classification return [self._model.classes_[np.argmax(self._model.tree_.value[i])] if f < 0 else None for i, f in enumerate(self._model.tree_.feature)]","title":"leaf_classes"},{"location":"reference/text_explainability/generation/surrogate/#predict_3","text":"1 2 3 4 def predict ( self , X ) View Source 1 2 3 def predict(self, X): return self._model.predict(X)","title":"predict"},{"location":"reference/text_explainability/generation/surrogate/#to_rules","text":"1 2 3 def to_rules ( self ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_rules(self): from skrules.rule import Rule from skrules.skope_rules import BASE_FEATURE_NAME, SkopeRules feature_names = [BASE_FEATURE_NAME + str(i) for i in range(self._model.n_features_)] rules = SkopeRules._tree_to_rules(None, tree=self._model, feature_names=feature_names) # TODO: add performance metrics and output label self._rules = [tuple(rule) for rule in [Rule(r) for r in rules]] return self._rules","title":"to_rules"},{"location":"reference/text_explainability/generation/target_encoding/","text":"Module text_explainability.generation.target_encoding Encode targets into binary labels for contrastive explanation. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 \"\"\"Encode targets into binary labels for contrastive explanation.\"\"\" from typing import Generator, List, Optional, Sequence, Union import numpy as np from instancelib.machinelearning import AbstractClassifier class TargetEncoder: def __init__(self, labels: Optional[Union[Sequence[str], AbstractClassifier]] = None): \"\"\"Encode model predictions based on encoding rule. Args: labels (Optional[Union[Sequence[str], AbstractClassifier]], optional): Labelset for mapping labels onto. Defaults to None. \"\"\" self.labelset = labels @property def labelset(self): return self.__labelset @labelset.setter def labelset(self, labelset): if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) self.__labelset = labelset def get_label(self, y, proba_to_labels: bool = True, label_to_index: bool = True) -> Union[List[int], List[str]]: \"\"\"Get prediction label as probability, string or class index. Args: y: Predictions with optional indices. proba_to_labels (bool, optional): Whether to convert probability to highest scoring class. Defaults to True. label_to_index (bool, optional): Convert string to index in labelset. Defaults to True. Returns: Union[List[int], List[str]]: Label names (if label_to_index is False) or label indices (otherwise). \"\"\" if isinstance(y, Generator): y = list(y) if len(y) > 0: if isinstance(y[0], tuple): y = [y_[1] for y_ in y] if isinstance(y[0], frozenset): y = [list(y_) for y_ in y] if isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], str): y = [y_[0] for y_ in y] if proba_to_labels: # model.predict_proba if len(y) > 0 and isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], tuple): y = [sorted(y_, key=lambda x: x[1], reverse=True)[0][0] for y_ in y] # model.predict_proba_raw if len(y) > 0 and isinstance(y[0], np.ndarray): y = np.hstack([np.argmax(y_, axis=1) for y_ in y]) if label_to_index: return [self.labelset.index(y_) if isinstance(y_, str) else y_ for y_ in y] return y def encode(self, y): \"\"\"Encode a single instance.\"\"\" return y def __call__(self, y) -> List[int]: \"\"\"Encode multiple predicted labels. Args: y: Predictions with optional indices. Returns: List[int]: Encoded labels as indices. \"\"\" return self.encode(self.get_label(y, proba_to_labels=True, label_to_index=True)) class FactFoilEncoder(TargetEncoder): def __init__(self, foil: int, labelset: Optional[Sequence[str]] = None): super().__init__(labelset) self.foil = foil @classmethod def from_str(cls, label: str, labelset: Union[AbstractClassifier, Sequence[str]]): \"\"\"Instantiate FactFoilEncoder with a string as foil. Args: label (str): Foil (expected outcome) label. labelset (Union[AbstractClassifier, Sequence[str]]): Labelset containing the foil. Returns: FactFoilEncoder: Initialized FactFoilEncoder. \"\"\" if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) foil = labelset.index(label) return cls(foil, labelset) def encode(self, y): \"\"\"Encode a single instance into foil (0) or not foil (1).\"\"\" if all(isinstance(y_, str) for y_ in y): y = [self.labelset.index(y_) for y_ in y] if isinstance(self.foil, int): return [0 if y_ == self.foil else 1 for y_ in y] return [0 if y_ in self.foil else 1 for y_ in y] Classes FactFoilEncoder 1 2 3 4 class FactFoilEncoder ( foil : int , labelset : Optional [ Sequence [ str ]] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class FactFoilEncoder(TargetEncoder): def __init__(self, foil: int, labelset: Optional[Sequence[str]] = None): super().__init__(labelset) self.foil = foil @classmethod def from_str(cls, label: str, labelset: Union[AbstractClassifier, Sequence[str]]): \"\"\"Instantiate FactFoilEncoder with a string as foil. Args: label (str): Foil (expected outcome) label. labelset (Union[AbstractClassifier, Sequence[str]]): Labelset containing the foil. Returns: FactFoilEncoder: Initialized FactFoilEncoder. \"\"\" if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) foil = labelset.index(label) return cls(foil, labelset) def encode(self, y): \"\"\"Encode a single instance into foil (0) or not foil (1).\"\"\" if all(isinstance(y_, str) for y_ in y): y = [self.labelset.index(y_) for y_ in y] if isinstance(self.foil, int): return [0 if y_ == self.foil else 1 for y_ in y] return [0 if y_ in self.foil else 1 for y_ in y] Ancestors (in MRO) text_explainability.generation.target_encoding.TargetEncoder Static methods from_str 1 2 3 4 def from_str ( label : str , labelset : Union [ instancelib . machinelearning . base . AbstractClassifier , Sequence [ str ]] ) Instantiate FactFoilEncoder with a string as foil. Parameters: Name Type Description Default label str Foil (expected outcome) label. None labelset Union[AbstractClassifier, Sequence[str]] Labelset containing the foil. None Returns: Type Description FactFoilEncoder Initialized FactFoilEncoder. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @classmethod def from_str(cls, label: str, labelset: Union[AbstractClassifier, Sequence[str]]): \"\"\"Instantiate FactFoilEncoder with a string as foil. Args: label (str): Foil (expected outcome) label. labelset (Union[AbstractClassifier, Sequence[str]]): Labelset containing the foil. Returns: FactFoilEncoder: Initialized FactFoilEncoder. \"\"\" if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) foil = labelset.index(label) return cls(foil, labelset) Instance variables 1 labelset Methods encode 1 2 3 4 def encode ( self , y ) Encode a single instance into foil (0) or not foil (1). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def encode(self, y): \"\"\"Encode a single instance into foil (0) or not foil (1).\"\"\" if all(isinstance(y_, str) for y_ in y): y = [self.labelset.index(y_) for y_ in y] if isinstance(self.foil, int): return [0 if y_ == self.foil else 1 for y_ in y] return [0 if y_ in self.foil else 1 for y_ in y] get_label 1 2 3 4 5 6 def get_label ( self , y , proba_to_labels : bool = True , label_to_index : bool = True ) -> Union [ List [ int ], List [ str ]] Get prediction label as probability, string or class index. Parameters: Name Type Description Default y None Predictions with optional indices. None proba_to_labels bool Whether to convert probability to highest scoring class. Defaults to True. True label_to_index bool Convert string to index in labelset. Defaults to True. True Returns: Type Description Union[List[int], List[str]] Label names (if label_to_index is False) or label indices (otherwise). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def get_label(self, y, proba_to_labels: bool = True, label_to_index: bool = True) -> Union[List[int], List[str]]: \"\"\"Get prediction label as probability, string or class index. Args: y: Predictions with optional indices. proba_to_labels (bool, optional): Whether to convert probability to highest scoring class. Defaults to True. label_to_index (bool, optional): Convert string to index in labelset. Defaults to True. Returns: Union[List[int], List[str]]: Label names (if label_to_index is False) or label indices (otherwise). \"\"\" if isinstance(y, Generator): y = list(y) if len(y) > 0: if isinstance(y[0], tuple): y = [y_[1] for y_ in y] if isinstance(y[0], frozenset): y = [list(y_) for y_ in y] if isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], str): y = [y_[0] for y_ in y] if proba_to_labels: # model.predict_proba if len(y) > 0 and isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], tuple): y = [sorted(y_, key=lambda x: x[1], reverse=True)[0][0] for y_ in y] # model.predict_proba_raw if len(y) > 0 and isinstance(y[0], np.ndarray): y = np.hstack([np.argmax(y_, axis=1) for y_ in y]) if label_to_index: return [self.labelset.index(y_) if isinstance(y_, str) else y_ for y_ in y] return y TargetEncoder 1 2 3 class TargetEncoder ( labels : Union [ Sequence [ str ], instancelib . machinelearning . base . AbstractClassifier , NoneType ] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class TargetEncoder: def __init__(self, labels: Optional[Union[Sequence[str], AbstractClassifier]] = None): \"\"\"Encode model predictions based on encoding rule. Args: labels (Optional[Union[Sequence[str], AbstractClassifier]], optional): Labelset for mapping labels onto. Defaults to None. \"\"\" self.labelset = labels @property def labelset(self): return self.__labelset @labelset.setter def labelset(self, labelset): if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) self.__labelset = labelset def get_label(self, y, proba_to_labels: bool = True, label_to_index: bool = True) -> Union[List[int], List[str]]: \"\"\"Get prediction label as probability, string or class index. Args: y: Predictions with optional indices. proba_to_labels (bool, optional): Whether to convert probability to highest scoring class. Defaults to True. label_to_index (bool, optional): Convert string to index in labelset. Defaults to True. Returns: Union[List[int], List[str]]: Label names (if label_to_index is False) or label indices (otherwise). \"\"\" if isinstance(y, Generator): y = list(y) if len(y) > 0: if isinstance(y[0], tuple): y = [y_[1] for y_ in y] if isinstance(y[0], frozenset): y = [list(y_) for y_ in y] if isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], str): y = [y_[0] for y_ in y] if proba_to_labels: # model.predict_proba if len(y) > 0 and isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], tuple): y = [sorted(y_, key=lambda x: x[1], reverse=True)[0][0] for y_ in y] # model.predict_proba_raw if len(y) > 0 and isinstance(y[0], np.ndarray): y = np.hstack([np.argmax(y_, axis=1) for y_ in y]) if label_to_index: return [self.labelset.index(y_) if isinstance(y_, str) else y_ for y_ in y] return y def encode(self, y): \"\"\"Encode a single instance.\"\"\" return y def __call__(self, y) -> List[int]: \"\"\"Encode multiple predicted labels. Args: y: Predictions with optional indices. Returns: List[int]: Encoded labels as indices. \"\"\" return self.encode(self.get_label(y, proba_to_labels=True, label_to_index=True)) Descendants text_explainability.generation.target_encoding.FactFoilEncoder Instance variables 1 labelset Methods encode 1 2 3 4 def encode ( self , y ) Encode a single instance. View Source 1 2 3 4 5 def encode(self, y): \"\"\"Encode a single instance.\"\"\" return y get_label 1 2 3 4 5 6 def get_label ( self , y , proba_to_labels : bool = True , label_to_index : bool = True ) -> Union [ List [ int ], List [ str ]] Get prediction label as probability, string or class index. Parameters: Name Type Description Default y None Predictions with optional indices. None proba_to_labels bool Whether to convert probability to highest scoring class. Defaults to True. True label_to_index bool Convert string to index in labelset. Defaults to True. True Returns: Type Description Union[List[int], List[str]] Label names (if label_to_index is False) or label indices (otherwise). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def get_label(self, y, proba_to_labels: bool = True, label_to_index: bool = True) -> Union[List[int], List[str]]: \"\"\"Get prediction label as probability, string or class index. Args: y: Predictions with optional indices. proba_to_labels (bool, optional): Whether to convert probability to highest scoring class. Defaults to True. label_to_index (bool, optional): Convert string to index in labelset. Defaults to True. Returns: Union[List[int], List[str]]: Label names (if label_to_index is False) or label indices (otherwise). \"\"\" if isinstance(y, Generator): y = list(y) if len(y) > 0: if isinstance(y[0], tuple): y = [y_[1] for y_ in y] if isinstance(y[0], frozenset): y = [list(y_) for y_ in y] if isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], str): y = [y_[0] for y_ in y] if proba_to_labels: # model.predict_proba if len(y) > 0 and isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], tuple): y = [sorted(y_, key=lambda x: x[1], reverse=True)[0][0] for y_ in y] # model.predict_proba_raw if len(y) > 0 and isinstance(y[0], np.ndarray): y = np.hstack([np.argmax(y_, axis=1) for y_ in y]) if label_to_index: return [self.labelset.index(y_) if isinstance(y_, str) else y_ for y_ in y] return y","title":"Target Encoding"},{"location":"reference/text_explainability/generation/target_encoding/#module-text_explainabilitygenerationtarget_encoding","text":"Encode targets into binary labels for contrastive explanation. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 \"\"\"Encode targets into binary labels for contrastive explanation.\"\"\" from typing import Generator, List, Optional, Sequence, Union import numpy as np from instancelib.machinelearning import AbstractClassifier class TargetEncoder: def __init__(self, labels: Optional[Union[Sequence[str], AbstractClassifier]] = None): \"\"\"Encode model predictions based on encoding rule. Args: labels (Optional[Union[Sequence[str], AbstractClassifier]], optional): Labelset for mapping labels onto. Defaults to None. \"\"\" self.labelset = labels @property def labelset(self): return self.__labelset @labelset.setter def labelset(self, labelset): if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) self.__labelset = labelset def get_label(self, y, proba_to_labels: bool = True, label_to_index: bool = True) -> Union[List[int], List[str]]: \"\"\"Get prediction label as probability, string or class index. Args: y: Predictions with optional indices. proba_to_labels (bool, optional): Whether to convert probability to highest scoring class. Defaults to True. label_to_index (bool, optional): Convert string to index in labelset. Defaults to True. Returns: Union[List[int], List[str]]: Label names (if label_to_index is False) or label indices (otherwise). \"\"\" if isinstance(y, Generator): y = list(y) if len(y) > 0: if isinstance(y[0], tuple): y = [y_[1] for y_ in y] if isinstance(y[0], frozenset): y = [list(y_) for y_ in y] if isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], str): y = [y_[0] for y_ in y] if proba_to_labels: # model.predict_proba if len(y) > 0 and isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], tuple): y = [sorted(y_, key=lambda x: x[1], reverse=True)[0][0] for y_ in y] # model.predict_proba_raw if len(y) > 0 and isinstance(y[0], np.ndarray): y = np.hstack([np.argmax(y_, axis=1) for y_ in y]) if label_to_index: return [self.labelset.index(y_) if isinstance(y_, str) else y_ for y_ in y] return y def encode(self, y): \"\"\"Encode a single instance.\"\"\" return y def __call__(self, y) -> List[int]: \"\"\"Encode multiple predicted labels. Args: y: Predictions with optional indices. Returns: List[int]: Encoded labels as indices. \"\"\" return self.encode(self.get_label(y, proba_to_labels=True, label_to_index=True)) class FactFoilEncoder(TargetEncoder): def __init__(self, foil: int, labelset: Optional[Sequence[str]] = None): super().__init__(labelset) self.foil = foil @classmethod def from_str(cls, label: str, labelset: Union[AbstractClassifier, Sequence[str]]): \"\"\"Instantiate FactFoilEncoder with a string as foil. Args: label (str): Foil (expected outcome) label. labelset (Union[AbstractClassifier, Sequence[str]]): Labelset containing the foil. Returns: FactFoilEncoder: Initialized FactFoilEncoder. \"\"\" if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) foil = labelset.index(label) return cls(foil, labelset) def encode(self, y): \"\"\"Encode a single instance into foil (0) or not foil (1).\"\"\" if all(isinstance(y_, str) for y_ in y): y = [self.labelset.index(y_) for y_ in y] if isinstance(self.foil, int): return [0 if y_ == self.foil else 1 for y_ in y] return [0 if y_ in self.foil else 1 for y_ in y]","title":"Module text_explainability.generation.target_encoding"},{"location":"reference/text_explainability/generation/target_encoding/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/generation/target_encoding/#factfoilencoder","text":"1 2 3 4 class FactFoilEncoder ( foil : int , labelset : Optional [ Sequence [ str ]] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class FactFoilEncoder(TargetEncoder): def __init__(self, foil: int, labelset: Optional[Sequence[str]] = None): super().__init__(labelset) self.foil = foil @classmethod def from_str(cls, label: str, labelset: Union[AbstractClassifier, Sequence[str]]): \"\"\"Instantiate FactFoilEncoder with a string as foil. Args: label (str): Foil (expected outcome) label. labelset (Union[AbstractClassifier, Sequence[str]]): Labelset containing the foil. Returns: FactFoilEncoder: Initialized FactFoilEncoder. \"\"\" if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) foil = labelset.index(label) return cls(foil, labelset) def encode(self, y): \"\"\"Encode a single instance into foil (0) or not foil (1).\"\"\" if all(isinstance(y_, str) for y_ in y): y = [self.labelset.index(y_) for y_ in y] if isinstance(self.foil, int): return [0 if y_ == self.foil else 1 for y_ in y] return [0 if y_ in self.foil else 1 for y_ in y]","title":"FactFoilEncoder"},{"location":"reference/text_explainability/generation/target_encoding/#ancestors-in-mro","text":"text_explainability.generation.target_encoding.TargetEncoder","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/target_encoding/#static-methods","text":"","title":"Static methods"},{"location":"reference/text_explainability/generation/target_encoding/#from_str","text":"1 2 3 4 def from_str ( label : str , labelset : Union [ instancelib . machinelearning . base . AbstractClassifier , Sequence [ str ]] ) Instantiate FactFoilEncoder with a string as foil. Parameters: Name Type Description Default label str Foil (expected outcome) label. None labelset Union[AbstractClassifier, Sequence[str]] Labelset containing the foil. None Returns: Type Description FactFoilEncoder Initialized FactFoilEncoder. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @classmethod def from_str(cls, label: str, labelset: Union[AbstractClassifier, Sequence[str]]): \"\"\"Instantiate FactFoilEncoder with a string as foil. Args: label (str): Foil (expected outcome) label. labelset (Union[AbstractClassifier, Sequence[str]]): Labelset containing the foil. Returns: FactFoilEncoder: Initialized FactFoilEncoder. \"\"\" if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) foil = labelset.index(label) return cls(foil, labelset)","title":"from_str"},{"location":"reference/text_explainability/generation/target_encoding/#instance-variables","text":"1 labelset","title":"Instance variables"},{"location":"reference/text_explainability/generation/target_encoding/#methods","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/target_encoding/#encode","text":"1 2 3 4 def encode ( self , y ) Encode a single instance into foil (0) or not foil (1). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def encode(self, y): \"\"\"Encode a single instance into foil (0) or not foil (1).\"\"\" if all(isinstance(y_, str) for y_ in y): y = [self.labelset.index(y_) for y_ in y] if isinstance(self.foil, int): return [0 if y_ == self.foil else 1 for y_ in y] return [0 if y_ in self.foil else 1 for y_ in y]","title":"encode"},{"location":"reference/text_explainability/generation/target_encoding/#get_label","text":"1 2 3 4 5 6 def get_label ( self , y , proba_to_labels : bool = True , label_to_index : bool = True ) -> Union [ List [ int ], List [ str ]] Get prediction label as probability, string or class index. Parameters: Name Type Description Default y None Predictions with optional indices. None proba_to_labels bool Whether to convert probability to highest scoring class. Defaults to True. True label_to_index bool Convert string to index in labelset. Defaults to True. True Returns: Type Description Union[List[int], List[str]] Label names (if label_to_index is False) or label indices (otherwise). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def get_label(self, y, proba_to_labels: bool = True, label_to_index: bool = True) -> Union[List[int], List[str]]: \"\"\"Get prediction label as probability, string or class index. Args: y: Predictions with optional indices. proba_to_labels (bool, optional): Whether to convert probability to highest scoring class. Defaults to True. label_to_index (bool, optional): Convert string to index in labelset. Defaults to True. Returns: Union[List[int], List[str]]: Label names (if label_to_index is False) or label indices (otherwise). \"\"\" if isinstance(y, Generator): y = list(y) if len(y) > 0: if isinstance(y[0], tuple): y = [y_[1] for y_ in y] if isinstance(y[0], frozenset): y = [list(y_) for y_ in y] if isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], str): y = [y_[0] for y_ in y] if proba_to_labels: # model.predict_proba if len(y) > 0 and isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], tuple): y = [sorted(y_, key=lambda x: x[1], reverse=True)[0][0] for y_ in y] # model.predict_proba_raw if len(y) > 0 and isinstance(y[0], np.ndarray): y = np.hstack([np.argmax(y_, axis=1) for y_ in y]) if label_to_index: return [self.labelset.index(y_) if isinstance(y_, str) else y_ for y_ in y] return y","title":"get_label"},{"location":"reference/text_explainability/generation/target_encoding/#targetencoder","text":"1 2 3 class TargetEncoder ( labels : Union [ Sequence [ str ], instancelib . machinelearning . base . AbstractClassifier , NoneType ] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class TargetEncoder: def __init__(self, labels: Optional[Union[Sequence[str], AbstractClassifier]] = None): \"\"\"Encode model predictions based on encoding rule. Args: labels (Optional[Union[Sequence[str], AbstractClassifier]], optional): Labelset for mapping labels onto. Defaults to None. \"\"\" self.labelset = labels @property def labelset(self): return self.__labelset @labelset.setter def labelset(self, labelset): if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) self.__labelset = labelset def get_label(self, y, proba_to_labels: bool = True, label_to_index: bool = True) -> Union[List[int], List[str]]: \"\"\"Get prediction label as probability, string or class index. Args: y: Predictions with optional indices. proba_to_labels (bool, optional): Whether to convert probability to highest scoring class. Defaults to True. label_to_index (bool, optional): Convert string to index in labelset. Defaults to True. Returns: Union[List[int], List[str]]: Label names (if label_to_index is False) or label indices (otherwise). \"\"\" if isinstance(y, Generator): y = list(y) if len(y) > 0: if isinstance(y[0], tuple): y = [y_[1] for y_ in y] if isinstance(y[0], frozenset): y = [list(y_) for y_ in y] if isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], str): y = [y_[0] for y_ in y] if proba_to_labels: # model.predict_proba if len(y) > 0 and isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], tuple): y = [sorted(y_, key=lambda x: x[1], reverse=True)[0][0] for y_ in y] # model.predict_proba_raw if len(y) > 0 and isinstance(y[0], np.ndarray): y = np.hstack([np.argmax(y_, axis=1) for y_ in y]) if label_to_index: return [self.labelset.index(y_) if isinstance(y_, str) else y_ for y_ in y] return y def encode(self, y): \"\"\"Encode a single instance.\"\"\" return y def __call__(self, y) -> List[int]: \"\"\"Encode multiple predicted labels. Args: y: Predictions with optional indices. Returns: List[int]: Encoded labels as indices. \"\"\" return self.encode(self.get_label(y, proba_to_labels=True, label_to_index=True))","title":"TargetEncoder"},{"location":"reference/text_explainability/generation/target_encoding/#descendants","text":"text_explainability.generation.target_encoding.FactFoilEncoder","title":"Descendants"},{"location":"reference/text_explainability/generation/target_encoding/#instance-variables_1","text":"1 labelset","title":"Instance variables"},{"location":"reference/text_explainability/generation/target_encoding/#methods_1","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/target_encoding/#encode_1","text":"1 2 3 4 def encode ( self , y ) Encode a single instance. View Source 1 2 3 4 5 def encode(self, y): \"\"\"Encode a single instance.\"\"\" return y","title":"encode"},{"location":"reference/text_explainability/generation/target_encoding/#get_label_1","text":"1 2 3 4 5 6 def get_label ( self , y , proba_to_labels : bool = True , label_to_index : bool = True ) -> Union [ List [ int ], List [ str ]] Get prediction label as probability, string or class index. Parameters: Name Type Description Default y None Predictions with optional indices. None proba_to_labels bool Whether to convert probability to highest scoring class. Defaults to True. True label_to_index bool Convert string to index in labelset. Defaults to True. True Returns: Type Description Union[List[int], List[str]] Label names (if label_to_index is False) or label indices (otherwise). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def get_label(self, y, proba_to_labels: bool = True, label_to_index: bool = True) -> Union[List[int], List[str]]: \"\"\"Get prediction label as probability, string or class index. Args: y: Predictions with optional indices. proba_to_labels (bool, optional): Whether to convert probability to highest scoring class. Defaults to True. label_to_index (bool, optional): Convert string to index in labelset. Defaults to True. Returns: Union[List[int], List[str]]: Label names (if label_to_index is False) or label indices (otherwise). \"\"\" if isinstance(y, Generator): y = list(y) if len(y) > 0: if isinstance(y[0], tuple): y = [y_[1] for y_ in y] if isinstance(y[0], frozenset): y = [list(y_) for y_ in y] if isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], str): y = [y_[0] for y_ in y] if proba_to_labels: # model.predict_proba if len(y) > 0 and isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], tuple): y = [sorted(y_, key=lambda x: x[1], reverse=True)[0][0] for y_ in y] # model.predict_proba_raw if len(y) > 0 and isinstance(y[0], np.ndarray): y = np.hstack([np.argmax(y_, axis=1) for y_ in y]) if label_to_index: return [self.labelset.index(y_) if isinstance(y_, str) else y_ for y_ in y] return y","title":"get_label"},{"location":"reference/text_explainability/ui/","text":"Module text_explainability.ui Extensions to genbase.ui . None View Source 1 2 3 \"\"\"Extensions to `genbase.ui`.\"\"\" from text_explainability.ui.notebook import Render Sub-modules text_explainability.ui.notebook","title":"Index"},{"location":"reference/text_explainability/ui/#module-text_explainabilityui","text":"Extensions to genbase.ui . None View Source 1 2 3 \"\"\"Extensions to `genbase.ui`.\"\"\" from text_explainability.ui.notebook import Render","title":"Module text_explainability.ui"},{"location":"reference/text_explainability/ui/#sub-modules","text":"text_explainability.ui.notebook","title":"Sub-modules"},{"location":"reference/text_explainability/ui/notebook/","text":"Module text_explainability.ui.notebook Extension of genbase.ui.notebook for custom rendering of `text_explainability. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 \"\"\"Extension of `genbase.ui.notebook` for custom rendering of `text_explainability.\"\"\" from typing import Optional, Tuple import pandas as pd from genbase.ui import format_instances, get_color from genbase.ui.notebook import Render as BaseRender from genbase.ui.notebook import format_label from genbase.ui.plot import plotly_available MAIN_COLOR = '#1976D2' TRANSLATION_DICT = {'lime': ('LIME', 'https://christophm.github.io/interpretable-ml-book/lime.html'), 'shap': ('SHAP', 'https://christophm.github.io/interpretable-ml-book/shap.html'), 'kernel_shap': ('KernelSHAP', 'https://christophm.github.io/interpretable-ml-book/shap.html'), 'mutual_information': ('mutual information', 'https://en.wikipedia.org/wiki/Mutual_information'), 'kmedoids': ('KMedoids', 'https://christophm.github.io/interpretable-ml-book/proto.html'), 'mmdcritic': ('MMDCritic', 'https://christophm.github.io/interpretable-ml-book/proto.html')} def default_renderer(meta: dict, content: dict, **renderargs) -> str: return f'<p>{content}</p>' def plotly_fallback(function): def inner(*args, **kwargs): return function(*args, **kwargs) if not plotly_available() else default_renderer(*args, **kwargs) return function def get_meta_descriptors(meta: dict) -> Tuple[str]: \"\"\"Get type, subtype & method from `meta`. Args: meta (dict): [description] Returns: Tuple[str]: type, subtype, method \"\"\" def fmt(x): return str(x).strip().lower().replace(' ', '_') def get_from_meta(key: str) -> str: return fmt(meta[key]) if key in meta else '' return get_from_meta('type'), get_from_meta('subtype'), get_from_meta('method') def feature_attribution_renderer(meta: dict, content, **renderargs) -> str: min_value = renderargs.pop('min_value', -1.0) max_value = renderargs.pop('max_value', 1.0) colorscale = renderargs.pop('colorscale', [(0.0, '#e57373'), (0.5, '#eee'), (1.0, '#81c784')]) def gc(x): return get_color(x, min_value=min_value, max_value=max_value, colorscale=colorscale, format='hex') features, scores = content['features'], content['scores'] def render_one(tokens_and_scores: list): scores_dict = dict(tokens_and_scores) scores_ = [(token, scores_dict[token] if token in scores_dict else None) for token in features] return ''.join([f'<span class=\"token\" style=\"background-color: {gc(score) if score else \"inherit\"};' + (' border-bottom: 3px solid rgba(0, 0, 0, 0.3);' if score is not None else '') + '\"' + (f'title=\"{score}\"' if score is not None else '') + f'>{token}' + (f'<span class=\"attribution\">{score:.3f}</span>' if score is not None else '') + '</span>' for (token, score) in scores_]) if isinstance(scores, dict): html = '' for class_name, score in scores.items(): html += format_label(class_name, label_name='Class') html += render_one(score) return html return render_one(scores) @plotly_fallback def featurelist_renderer(meta: dict, content: dict, first_element: str = 'token', second_element: str = 'frequency', vertical: bool = False, sorted: bool = True, **renderargs) -> str: import plotly.express as px from genbase.ui.plot import ExpressPlot label_name = 'Class' if 'callargs' in meta and 'explain_model' in meta['callargs']: label_name = 'Predicted class' if meta['callargs']['explain_model'] else 'Ground-truth class' def render_one(class_name: str, tokens_and_scores: list): html = '' if class_name == 'all' else format_label(class_name, label_name=label_name) df = pd.DataFrame(tokens_and_scores, columns=[first_element, second_element]) if sorted: df = df.sort_values(by=second_element) x, y = (first_element, second_element) if vertical else (second_element, first_element) html += ExpressPlot(df, px.bar, x=x, y=y, color_discrete_sequence=[MAIN_COLOR]).interactive return html return ''.join(render_one(k, v) for k, v in content.items()) def frequency_renderer(meta: dict, content: dict, **renderargs) -> str: return featurelist_renderer(meta, content, first_element='token', second_element='frequency', vertical=False, sorted=True, **renderargs) def information_renderer(meta: dict, content: dict, **renderargs) -> str: return featurelist_renderer(meta, content, first_element='token', second_element='mutual information', vertical=False, sorted=True, **renderargs) def prototype_renderer(meta: dict, content: dict, **renderargs) -> str: def render_one(instance_type: str, instances) -> str: return f'<h4>{instance_type.title()}</h4><p>{format_instances(instances)}</p>' def render_class(class_name: Optional[str], instances: dict) -> str: html = '' if class_name is None else format_label(class_name, label_name='Class') for k, v in instances.items(): html += render_one(k, v) return html if all(k in ['instances', 'prototypes', 'criticisms'] for k in content.keys()): return render_class(None, content) return ''.join(render_class(class_name, instances) for class_name, instances in content.items()) class Render(BaseRender): def __init__(self, *configs): super().__init__(*configs) self.main_color = MAIN_COLOR self.package_link = 'https://git.io/text_explainability' self.extra_css = \"\"\" .token { display: inline-block; color: #000; padding: 0.8rem 0.7rem; margin: 0 0.2rem; } .token > .attribution { color: rgba(0, 0, 0, 0.8); vertical-align: super; font-size: smaller; } .token > .attribution::before { content: \" [\"; } .token > .attribution::after { content: \"]\"; } \"\"\" def get_renderer(self, meta: dict): type, subtype, _ = get_meta_descriptors(meta) if type == 'global_explanation': if 'frequency' in subtype.split('_'): return frequency_renderer elif 'information' in subtype.split('_'): return information_renderer elif 'prototypes' in subtype.split('_'): return prototype_renderer elif type == 'local_explanation': if subtype == 'feature_attribution': return feature_attribution_renderer return default_renderer def format_title(self, title: str, h: str = 'h1', **renderargs) -> str: return super().format_title(title, h=h, **renderargs).replace('_', ' ').title() def render_subtitle(self, meta: dict, content, **renderargs) -> str: type, subtype, _ = get_meta_descriptors(meta) labelwise = meta['labelwise'] if 'labelwise' in meta else False callargs = meta['callargs'] if 'callargs' in meta else '' def fmt_method(name: str) -> str: name, url = TRANSLATION_DICT[str.lower(name)] if str.lower(name) in TRANSLATION_DICT else (name, '') return f'<a href=\"{url}\" target=\"_blank\">{name}</a>' if url else name html = [] if 'method' in meta: html.append(f'Explanation generated with method {fmt_method(meta[\"method\"])}.') if type == 'global_explanation': if callargs: if 'explain_model' in callargs: what = 'predictions according to model' if callargs['explain_model'] \\ else 'ground-truth labels in dataset' how_many = f' (maximized to top-{callargs[\"k\"]})' if 'k' in callargs else '' html.append(f'{subtype.replace(\"_\", \" \").capitalize()} of {what}{how_many}.') if 'filter_words' in callargs: tokens = ', '.join(f'\"{t}\"' for t in callargs['filter_words']) html.append(f'Excluded tokens: {tokens if tokens else \"-\"}.') if labelwise: html.append('Grouped by label.') return self.format_subtitle('<br>'.join(html)) if html else '' Variables 1 MAIN_COLOR 1 TRANSLATION_DICT Functions default_renderer 1 2 3 4 5 def default_renderer ( meta : dict , content : dict , ** renderargs ) -> str View Source 1 2 3 def default_renderer(meta: dict, content: dict, **renderargs) -> str: return f'<p>{content}</p>' feature_attribution_renderer 1 2 3 4 5 def feature_attribution_renderer ( meta : dict , content , ** renderargs ) -> str View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def feature_attribution_renderer(meta: dict, content, **renderargs) -> str: min_value = renderargs.pop('min_value', -1.0) max_value = renderargs.pop('max_value', 1.0) colorscale = renderargs.pop('colorscale', [(0.0, '#e57373'), (0.5, '#eee'), (1.0, '#81c784')]) def gc(x): return get_color(x, min_value=min_value, max_value=max_value, colorscale=colorscale, format='hex') features, scores = content['features'], content['scores'] def render_one(tokens_and_scores: list): scores_dict = dict(tokens_and_scores) scores_ = [(token, scores_dict[token] if token in scores_dict else None) for token in features] return ''.join([f'<span class=\"token\" style=\"background-color: {gc(score) if score else \"inherit\"};' + (' border-bottom: 3px solid rgba(0, 0, 0, 0.3);' if score is not None else '') + '\"' + (f'title=\"{score}\"' if score is not None else '') + f'>{token}' + (f'<span class=\"attribution\">{score:.3f}</span>' if score is not None else '') + '</span>' for (token, score) in scores_]) if isinstance(scores, dict): html = '' for class_name, score in scores.items(): html += format_label(class_name, label_name='Class') html += render_one(score) return html return render_one(scores) featurelist_renderer 1 2 3 4 5 6 7 8 9 def featurelist_renderer ( meta : dict , content : dict , first_element : str = 'token' , second_element : str = 'frequency' , vertical : bool = False , sorted : bool = True , ** renderargs ) -> str View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 @plotly_fallback def featurelist_renderer(meta: dict, content: dict, first_element: str = 'token', second_element: str = 'frequency', vertical: bool = False, sorted: bool = True, **renderargs) -> str: import plotly.express as px from genbase.ui.plot import ExpressPlot label_name = 'Class' if 'callargs' in meta and 'explain_model' in meta['callargs']: label_name = 'Predicted class' if meta['callargs']['explain_model'] else 'Ground-truth class' def render_one(class_name: str, tokens_and_scores: list): html = '' if class_name == 'all' else format_label(class_name, label_name=label_name) df = pd.DataFrame(tokens_and_scores, columns=[first_element, second_element]) if sorted: df = df.sort_values(by=second_element) x, y = (first_element, second_element) if vertical else (second_element, first_element) html += ExpressPlot(df, px.bar, x=x, y=y, color_discrete_sequence=[MAIN_COLOR]).interactive return html return ''.join(render_one(k, v) for k, v in content.items()) frequency_renderer 1 2 3 4 5 def frequency_renderer ( meta : dict , content : dict , ** renderargs ) -> str View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def frequency_renderer(meta: dict, content: dict, **renderargs) -> str: return featurelist_renderer(meta, content, first_element='token', second_element='frequency', vertical=False, sorted=True, **renderargs) get_meta_descriptors 1 2 3 def get_meta_descriptors ( meta : dict ) -> Tuple [ str ] Get type, subtype & method from meta . Parameters: Name Type Description Default meta dict [description] None Returns: Type Description Tuple[str] type, subtype, method View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def get_meta_descriptors(meta: dict) -> Tuple[str]: \"\"\"Get type, subtype & method from `meta`. Args: meta (dict): [description] Returns: Tuple[str]: type, subtype, method \"\"\" def fmt(x): return str(x).strip().lower().replace(' ', '_') def get_from_meta(key: str) -> str: return fmt(meta[key]) if key in meta else '' return get_from_meta('type'), get_from_meta('subtype'), get_from_meta('method') information_renderer 1 2 3 4 5 def information_renderer ( meta : dict , content : dict , ** renderargs ) -> str View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def information_renderer(meta: dict, content: dict, **renderargs) -> str: return featurelist_renderer(meta, content, first_element='token', second_element='mutual information', vertical=False, sorted=True, **renderargs) plotly_fallback 1 2 3 def plotly_fallback ( function ) View Source 1 2 3 4 5 6 7 def plotly_fallback(function): def inner(*args, **kwargs): return function(*args, **kwargs) if not plotly_available() else default_renderer(*args, **kwargs) return function prototype_renderer 1 2 3 4 5 def prototype_renderer ( meta : dict , content : dict , ** renderargs ) -> str View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def prototype_renderer(meta: dict, content: dict, **renderargs) -> str: def render_one(instance_type: str, instances) -> str: return f'<h4>{instance_type.title()}</h4><p>{format_instances(instances)}</p>' def render_class(class_name: Optional[str], instances: dict) -> str: html = '' if class_name is None else format_label(class_name, label_name='Class') for k, v in instances.items(): html += render_one(k, v) return html if all(k in ['instances', 'prototypes', 'criticisms'] for k in content.keys()): return render_class(None, content) return ''.join(render_class(class_name, instances) for class_name, instances in content.items()) Classes Render 1 2 3 class Render ( * configs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 class Render(BaseRender): def __init__(self, *configs): super().__init__(*configs) self.main_color = MAIN_COLOR self.package_link = 'https://git.io/text_explainability' self.extra_css = \"\"\" .token { display: inline-block; color: #000; padding: 0.8rem 0.7rem; margin: 0 0.2rem; } .token > .attribution { color: rgba(0, 0, 0, 0.8); vertical-align: super; font-size: smaller; } .token > .attribution::before { content: \" [\"; } .token > .attribution::after { content: \"]\"; } \"\"\" def get_renderer(self, meta: dict): type, subtype, _ = get_meta_descriptors(meta) if type == 'global_explanation': if 'frequency' in subtype.split('_'): return frequency_renderer elif 'information' in subtype.split('_'): return information_renderer elif 'prototypes' in subtype.split('_'): return prototype_renderer elif type == 'local_explanation': if subtype == 'feature_attribution': return feature_attribution_renderer return default_renderer def format_title(self, title: str, h: str = 'h1', **renderargs) -> str: return super().format_title(title, h=h, **renderargs).replace('_', ' ').title() def render_subtitle(self, meta: dict, content, **renderargs) -> str: type, subtype, _ = get_meta_descriptors(meta) labelwise = meta['labelwise'] if 'labelwise' in meta else False callargs = meta['callargs'] if 'callargs' in meta else '' def fmt_method(name: str) -> str: name, url = TRANSLATION_DICT[str.lower(name)] if str.lower(name) in TRANSLATION_DICT else (name, '') return f'<a href=\"{url}\" target=\"_blank\">{name}</a>' if url else name html = [] if 'method' in meta: html.append(f'Explanation generated with method {fmt_method(meta[\"method\"])}.') if type == 'global_explanation': if callargs: if 'explain_model' in callargs: what = 'predictions according to model' if callargs['explain_model'] \\ else 'ground-truth labels in dataset' how_many = f' (maximized to top-{callargs[\"k\"]})' if 'k' in callargs else '' html.append(f'{subtype.replace(\"_\", \" \").capitalize()} of {what}{how_many}.') if 'filter_words' in callargs: tokens = ', '.join(f'\"{t}\"' for t in callargs['filter_words']) html.append(f'Excluded tokens: {tokens if tokens else \"-\"}.') if labelwise: html.append('Grouped by label.') return self.format_subtitle('<br>'.join(html)) if html else '' Ancestors (in MRO) genbase.ui.notebook.Render Class variables 1 extra_css 1 main_color 1 package_link Instance variables 1 custom_tab_title Title of custom tab. 1 package_name 1 tab_title Title of content tab. Methods as_html 1 2 3 4 def as_html ( self , ** renderargs ) -> str Get HTML element for interactive environments (e.g. Jupyter notebook). Parameters: Name Type Description Default **renderags None Optional arguments for rendering. None Returns: Type Description str HTML element. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def as_html(self, **renderargs) -> str: \"\"\"Get HTML element for interactive environments (e.g. Jupyter notebook). Args: **renderags: Optional arguments for rendering. Returns: str: HTML element. \"\"\" def fmt_exception(e: Exception, fmt_type: str = 'JSON') -> str: res = f'ERROR IN PARSING {fmt_type}\\n' res += '=' * len(res) + '\\n' return res + '\\n'.join(traceback.TracebackException.from_exception(e).format()) try: json = '\\n'.join(srsly.json_dumps(config, indent=2) for config in self.configs) except TypeError as e: json = fmt_exception(e, fmt_type='JSON') try: yaml = '\\n'.join(srsly.yaml_dumps(config) for config in self.configs) except srsly.ruamel_yaml.representer.RepresenterError as e: yaml = fmt_exception(e, fmt_type='YAML') html = ''.join(self.render_elements(config, **renderargs) for config in self.configs) id = str(uuid.uuid4()) tabs_id = f'tabs-{id}' ui_id = f'ui-{id}' CUSTOM_TAB = ''.join(self.custom_tab(config, **renderargs) for config in self.configs) if CUSTOM_TAB: CUSTOM_TAB = f\"\"\"<input type=\"radio\" name=\"{tabs_id}\" id=\"{tabs_id}-tab2\"/> <label class=\"wide\" for=\"{tabs_id}-tab2\">{self.custom_tab_title}</label> <div class=\"tab\">{CUSTOM_TAB}</div>\"\"\" HTML = f\"\"\" <div id=\"{ui_id}\"> <section class=\"ui-wrapper\"> <div class=\"ui-container\"> <div class=\"ui-block\"> <div id=\"{tabs_id}\"> <input type=\"radio\" name=\"{tabs_id}\" id=\"{tabs_id}-tab1\" checked=\"checked\" /> <label class=\"wide\" for=\"{tabs_id}-tab1\">{self.tab_title}</label> <div class=\"tab\">{html}</div> {CUSTOM_TAB} <input type=\"radio\" name=\"{tabs_id}\" id=\"{tabs_id}-tab{'3' if CUSTOM_TAB else '2'}\" /> <label for=\"{tabs_id}-tab{'3' if CUSTOM_TAB else '2'}\">{self.config_title}</label> <div class=\"tab code\"> <section> <div class=\"pre-buttons\"> <a class='copy' onclick=\"copy('json-output')\" href=\"#\" title=\"Copy JSON to clipboard\"> {CLONE_SVG} </a> </div> <h3>JSON</h3> </section> <pre id=\"json-output\">{json}</pre> <section> <div class=\"pre-buttons\"> <a class='copy' onclick=\"copy('yaml-output')\" href=\"#\" title=\"Copy YAML to clipboard\"> {CLONE_SVG} </a> </div> <h3>YAML</h3> </section> <pre id=\"yaml-output\">{yaml}</pre> </div> </div> </div> </div> </section> </div> \"\"\" JS = f'<script type=\"text/javascript\">{CUSTOM_JS}</script>' if CUSTOM_JS else '' main_color = renderargs.pop('main_color', self.main_color) package = renderargs.pop('package_link', self.package_link) package_name = self.package_name CSS = self.css(ui_color=main_color, ui_id=ui_id, tabs_id=tabs_id) FOOTER = f'<footer>Generated with <a href=\"{package}\" target=\"_blank\">{package_name}</a></footer>' return f'<style>{CSS}</style>{HTML}{FOOTER}{JS}' css 1 2 3 4 def css ( self , ** replacement_kwargs ) View Source 1 2 3 4 5 6 7 8 9 def css(self, **replacement_kwargs): css_ = CUSTOM_CSS + '\\n' + self.extra_css for k, v in replacement_kwargs.items(): css_ = css_.replace(f'--var({k})', v) return css_ custom_tab 1 2 3 4 5 def custom_tab ( self , config : dict , ** renderargs ) -> str View Source 1 2 3 def custom_tab(self, config: dict, **renderargs) -> str: return '' format_subtitle 1 2 3 4 def format_subtitle ( self , subtitle : str ) -> str Format the subtitle in HTML format. Parameters: Name Type Description Default subtitle str Subtitle contents. None Returns: Type Description str Formatted subtitle. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def format_subtitle(self, subtitle: str) -> str: \"\"\"Format the subtitle in HTML format. Args: subtitle (str): Subtitle contents. Returns: str: Formatted subtitle. \"\"\" return f'<p class=\"info\">{subtitle}</p>' format_title 1 2 3 4 5 6 def format_title ( self , title : str , h : str = 'h1' , ** renderargs ) -> str Format title in HTML format. Parameters: Name Type Description Default title str Title contents. None h str h-tag (h1, h2, ...). Defaults to 'h1'. 'h1' Returns: Type Description str Formatted title. View Source 1 2 3 def format_title(self, title: str, h: str = 'h1', **renderargs) -> str: return super().format_title(title, h=h, **renderargs).replace('_', ' ').title() get_renderer 1 2 3 4 def get_renderer ( self , meta : dict ) Get a render function (Callable taking meta , content and **renderargs and returning a str ). Parameters: Name Type Description Default meta dict Meta information to decide on appropriate renderer. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def get_renderer(self, meta: dict): type, subtype, _ = get_meta_descriptors(meta) if type == 'global_explanation': if 'frequency' in subtype.split('_'): return frequency_renderer elif 'information' in subtype.split('_'): return information_renderer elif 'prototypes' in subtype.split('_'): return prototype_renderer elif type == 'local_explanation': if subtype == 'feature_attribution': return feature_attribution_renderer return default_renderer render_content 1 2 3 4 5 6 def render_content ( self , meta : dict , content : dict , ** renderargs ) -> str Render content as HTML. Overwrite this when subclassing for your custom implementation. Parameters: Name Type Description Default meta dict Meta config. None content dict Content config. None **renderags None Optional arguments for rendering. None Returns: Type Description str Formatted content. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def render_content(self, meta: dict, content: dict, **renderargs) -> str: \"\"\"Render content as HTML. Overwrite this when subclassing for your custom implementation. Args: meta (dict): Meta config. content (dict): Content config. **renderags: Optional arguments for rendering. Returns: str: Formatted content. \"\"\" renderer = self.get_renderer(meta) return renderer(meta, content, **renderargs) render_elements 1 2 3 4 5 def render_elements ( self , config : dict , ** renderargs ) -> str Render HTML title and content. Parameters: Name Type Description Default config dict Config meta & content. None **renderags None Optional arguments for rendering. None Returns: Type Description str Formatted title and content. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def render_elements(self, config: dict, **renderargs) -> str: \"\"\"Render HTML title and content. Args: config (dict): Config meta & content. **renderags: Optional arguments for rendering. Returns: str: Formatted title and content. \"\"\" meta, content = config['META'], config['CONTENT'] return self.render_title(meta, content, **renderargs) + \\ self.render_subtitle(meta, content, **renderargs) + \\ self.render_content(meta, content, **renderargs) render_subtitle 1 2 3 4 5 6 def render_subtitle ( self , meta : dict , content , ** renderargs ) -> str View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def render_subtitle(self, meta: dict, content, **renderargs) -> str: type, subtype, _ = get_meta_descriptors(meta) labelwise = meta['labelwise'] if 'labelwise' in meta else False callargs = meta['callargs'] if 'callargs' in meta else '' def fmt_method(name: str) -> str: name, url = TRANSLATION_DICT[str.lower(name)] if str.lower(name) in TRANSLATION_DICT else (name, '') return f'<a href=\"{url}\" target=\"_blank\">{name}</a>' if url else name html = [] if 'method' in meta: html.append(f'Explanation generated with method {fmt_method(meta[\"method\"])}.') if type == 'global_explanation': if callargs: if 'explain_model' in callargs: what = 'predictions according to model' if callargs['explain_model'] \\ else 'ground-truth labels in dataset' how_many = f' (maximized to top-{callargs[\"k\"]})' if 'k' in callargs else '' html.append(f'{subtype.replace(\"_\", \" \").capitalize()} of {what}{how_many}.') if 'filter_words' in callargs: tokens = ', '.join(f'\"{t}\"' for t in callargs['filter_words']) html.append(f'Excluded tokens: {tokens if tokens else \"-\"}.') if labelwise: html.append('Grouped by label.') return self.format_subtitle('<br>'.join(html)) if html else '' render_title 1 2 3 4 5 6 def render_title ( self , meta : dict , content : dict , ** renderargs ) -> str Render the title as HTML. Overwrite this when subclassing for your custom implementation. Parameters: Name Type Description Default meta dict Meta config. None content dict Content config. None **renderags None Optional arguments for rendering. None Returns: Type Description str Formatted title. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def render_title(self, meta: dict, content: dict, **renderargs) -> str: \"\"\"Render the title as HTML. Overwrite this when subclassing for your custom implementation. Args: meta (dict): Meta config. content (dict): Content config. **renderags: Optional arguments for rendering. Returns: str: Formatted title. \"\"\" title = renderargs.pop('title', None) if title is None: if 'title' in meta: title = meta['title'] elif 'type' in meta: title = meta['type'] if 'subtype' in meta: title += f' ({meta[\"subtype\"]})' return self.format_title(title, **renderargs) if title else ''","title":"Notebook"},{"location":"reference/text_explainability/ui/notebook/#module-text_explainabilityuinotebook","text":"Extension of genbase.ui.notebook for custom rendering of `text_explainability. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 \"\"\"Extension of `genbase.ui.notebook` for custom rendering of `text_explainability.\"\"\" from typing import Optional, Tuple import pandas as pd from genbase.ui import format_instances, get_color from genbase.ui.notebook import Render as BaseRender from genbase.ui.notebook import format_label from genbase.ui.plot import plotly_available MAIN_COLOR = '#1976D2' TRANSLATION_DICT = {'lime': ('LIME', 'https://christophm.github.io/interpretable-ml-book/lime.html'), 'shap': ('SHAP', 'https://christophm.github.io/interpretable-ml-book/shap.html'), 'kernel_shap': ('KernelSHAP', 'https://christophm.github.io/interpretable-ml-book/shap.html'), 'mutual_information': ('mutual information', 'https://en.wikipedia.org/wiki/Mutual_information'), 'kmedoids': ('KMedoids', 'https://christophm.github.io/interpretable-ml-book/proto.html'), 'mmdcritic': ('MMDCritic', 'https://christophm.github.io/interpretable-ml-book/proto.html')} def default_renderer(meta: dict, content: dict, **renderargs) -> str: return f'<p>{content}</p>' def plotly_fallback(function): def inner(*args, **kwargs): return function(*args, **kwargs) if not plotly_available() else default_renderer(*args, **kwargs) return function def get_meta_descriptors(meta: dict) -> Tuple[str]: \"\"\"Get type, subtype & method from `meta`. Args: meta (dict): [description] Returns: Tuple[str]: type, subtype, method \"\"\" def fmt(x): return str(x).strip().lower().replace(' ', '_') def get_from_meta(key: str) -> str: return fmt(meta[key]) if key in meta else '' return get_from_meta('type'), get_from_meta('subtype'), get_from_meta('method') def feature_attribution_renderer(meta: dict, content, **renderargs) -> str: min_value = renderargs.pop('min_value', -1.0) max_value = renderargs.pop('max_value', 1.0) colorscale = renderargs.pop('colorscale', [(0.0, '#e57373'), (0.5, '#eee'), (1.0, '#81c784')]) def gc(x): return get_color(x, min_value=min_value, max_value=max_value, colorscale=colorscale, format='hex') features, scores = content['features'], content['scores'] def render_one(tokens_and_scores: list): scores_dict = dict(tokens_and_scores) scores_ = [(token, scores_dict[token] if token in scores_dict else None) for token in features] return ''.join([f'<span class=\"token\" style=\"background-color: {gc(score) if score else \"inherit\"};' + (' border-bottom: 3px solid rgba(0, 0, 0, 0.3);' if score is not None else '') + '\"' + (f'title=\"{score}\"' if score is not None else '') + f'>{token}' + (f'<span class=\"attribution\">{score:.3f}</span>' if score is not None else '') + '</span>' for (token, score) in scores_]) if isinstance(scores, dict): html = '' for class_name, score in scores.items(): html += format_label(class_name, label_name='Class') html += render_one(score) return html return render_one(scores) @plotly_fallback def featurelist_renderer(meta: dict, content: dict, first_element: str = 'token', second_element: str = 'frequency', vertical: bool = False, sorted: bool = True, **renderargs) -> str: import plotly.express as px from genbase.ui.plot import ExpressPlot label_name = 'Class' if 'callargs' in meta and 'explain_model' in meta['callargs']: label_name = 'Predicted class' if meta['callargs']['explain_model'] else 'Ground-truth class' def render_one(class_name: str, tokens_and_scores: list): html = '' if class_name == 'all' else format_label(class_name, label_name=label_name) df = pd.DataFrame(tokens_and_scores, columns=[first_element, second_element]) if sorted: df = df.sort_values(by=second_element) x, y = (first_element, second_element) if vertical else (second_element, first_element) html += ExpressPlot(df, px.bar, x=x, y=y, color_discrete_sequence=[MAIN_COLOR]).interactive return html return ''.join(render_one(k, v) for k, v in content.items()) def frequency_renderer(meta: dict, content: dict, **renderargs) -> str: return featurelist_renderer(meta, content, first_element='token', second_element='frequency', vertical=False, sorted=True, **renderargs) def information_renderer(meta: dict, content: dict, **renderargs) -> str: return featurelist_renderer(meta, content, first_element='token', second_element='mutual information', vertical=False, sorted=True, **renderargs) def prototype_renderer(meta: dict, content: dict, **renderargs) -> str: def render_one(instance_type: str, instances) -> str: return f'<h4>{instance_type.title()}</h4><p>{format_instances(instances)}</p>' def render_class(class_name: Optional[str], instances: dict) -> str: html = '' if class_name is None else format_label(class_name, label_name='Class') for k, v in instances.items(): html += render_one(k, v) return html if all(k in ['instances', 'prototypes', 'criticisms'] for k in content.keys()): return render_class(None, content) return ''.join(render_class(class_name, instances) for class_name, instances in content.items()) class Render(BaseRender): def __init__(self, *configs): super().__init__(*configs) self.main_color = MAIN_COLOR self.package_link = 'https://git.io/text_explainability' self.extra_css = \"\"\" .token { display: inline-block; color: #000; padding: 0.8rem 0.7rem; margin: 0 0.2rem; } .token > .attribution { color: rgba(0, 0, 0, 0.8); vertical-align: super; font-size: smaller; } .token > .attribution::before { content: \" [\"; } .token > .attribution::after { content: \"]\"; } \"\"\" def get_renderer(self, meta: dict): type, subtype, _ = get_meta_descriptors(meta) if type == 'global_explanation': if 'frequency' in subtype.split('_'): return frequency_renderer elif 'information' in subtype.split('_'): return information_renderer elif 'prototypes' in subtype.split('_'): return prototype_renderer elif type == 'local_explanation': if subtype == 'feature_attribution': return feature_attribution_renderer return default_renderer def format_title(self, title: str, h: str = 'h1', **renderargs) -> str: return super().format_title(title, h=h, **renderargs).replace('_', ' ').title() def render_subtitle(self, meta: dict, content, **renderargs) -> str: type, subtype, _ = get_meta_descriptors(meta) labelwise = meta['labelwise'] if 'labelwise' in meta else False callargs = meta['callargs'] if 'callargs' in meta else '' def fmt_method(name: str) -> str: name, url = TRANSLATION_DICT[str.lower(name)] if str.lower(name) in TRANSLATION_DICT else (name, '') return f'<a href=\"{url}\" target=\"_blank\">{name}</a>' if url else name html = [] if 'method' in meta: html.append(f'Explanation generated with method {fmt_method(meta[\"method\"])}.') if type == 'global_explanation': if callargs: if 'explain_model' in callargs: what = 'predictions according to model' if callargs['explain_model'] \\ else 'ground-truth labels in dataset' how_many = f' (maximized to top-{callargs[\"k\"]})' if 'k' in callargs else '' html.append(f'{subtype.replace(\"_\", \" \").capitalize()} of {what}{how_many}.') if 'filter_words' in callargs: tokens = ', '.join(f'\"{t}\"' for t in callargs['filter_words']) html.append(f'Excluded tokens: {tokens if tokens else \"-\"}.') if labelwise: html.append('Grouped by label.') return self.format_subtitle('<br>'.join(html)) if html else ''","title":"Module text_explainability.ui.notebook"},{"location":"reference/text_explainability/ui/notebook/#variables","text":"1 MAIN_COLOR 1 TRANSLATION_DICT","title":"Variables"},{"location":"reference/text_explainability/ui/notebook/#functions","text":"","title":"Functions"},{"location":"reference/text_explainability/ui/notebook/#default_renderer","text":"1 2 3 4 5 def default_renderer ( meta : dict , content : dict , ** renderargs ) -> str View Source 1 2 3 def default_renderer(meta: dict, content: dict, **renderargs) -> str: return f'<p>{content}</p>'","title":"default_renderer"},{"location":"reference/text_explainability/ui/notebook/#feature_attribution_renderer","text":"1 2 3 4 5 def feature_attribution_renderer ( meta : dict , content , ** renderargs ) -> str View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def feature_attribution_renderer(meta: dict, content, **renderargs) -> str: min_value = renderargs.pop('min_value', -1.0) max_value = renderargs.pop('max_value', 1.0) colorscale = renderargs.pop('colorscale', [(0.0, '#e57373'), (0.5, '#eee'), (1.0, '#81c784')]) def gc(x): return get_color(x, min_value=min_value, max_value=max_value, colorscale=colorscale, format='hex') features, scores = content['features'], content['scores'] def render_one(tokens_and_scores: list): scores_dict = dict(tokens_and_scores) scores_ = [(token, scores_dict[token] if token in scores_dict else None) for token in features] return ''.join([f'<span class=\"token\" style=\"background-color: {gc(score) if score else \"inherit\"};' + (' border-bottom: 3px solid rgba(0, 0, 0, 0.3);' if score is not None else '') + '\"' + (f'title=\"{score}\"' if score is not None else '') + f'>{token}' + (f'<span class=\"attribution\">{score:.3f}</span>' if score is not None else '') + '</span>' for (token, score) in scores_]) if isinstance(scores, dict): html = '' for class_name, score in scores.items(): html += format_label(class_name, label_name='Class') html += render_one(score) return html return render_one(scores)","title":"feature_attribution_renderer"},{"location":"reference/text_explainability/ui/notebook/#featurelist_renderer","text":"1 2 3 4 5 6 7 8 9 def featurelist_renderer ( meta : dict , content : dict , first_element : str = 'token' , second_element : str = 'frequency' , vertical : bool = False , sorted : bool = True , ** renderargs ) -> str View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 @plotly_fallback def featurelist_renderer(meta: dict, content: dict, first_element: str = 'token', second_element: str = 'frequency', vertical: bool = False, sorted: bool = True, **renderargs) -> str: import plotly.express as px from genbase.ui.plot import ExpressPlot label_name = 'Class' if 'callargs' in meta and 'explain_model' in meta['callargs']: label_name = 'Predicted class' if meta['callargs']['explain_model'] else 'Ground-truth class' def render_one(class_name: str, tokens_and_scores: list): html = '' if class_name == 'all' else format_label(class_name, label_name=label_name) df = pd.DataFrame(tokens_and_scores, columns=[first_element, second_element]) if sorted: df = df.sort_values(by=second_element) x, y = (first_element, second_element) if vertical else (second_element, first_element) html += ExpressPlot(df, px.bar, x=x, y=y, color_discrete_sequence=[MAIN_COLOR]).interactive return html return ''.join(render_one(k, v) for k, v in content.items())","title":"featurelist_renderer"},{"location":"reference/text_explainability/ui/notebook/#frequency_renderer","text":"1 2 3 4 5 def frequency_renderer ( meta : dict , content : dict , ** renderargs ) -> str View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def frequency_renderer(meta: dict, content: dict, **renderargs) -> str: return featurelist_renderer(meta, content, first_element='token', second_element='frequency', vertical=False, sorted=True, **renderargs)","title":"frequency_renderer"},{"location":"reference/text_explainability/ui/notebook/#get_meta_descriptors","text":"1 2 3 def get_meta_descriptors ( meta : dict ) -> Tuple [ str ] Get type, subtype & method from meta . Parameters: Name Type Description Default meta dict [description] None Returns: Type Description Tuple[str] type, subtype, method View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def get_meta_descriptors(meta: dict) -> Tuple[str]: \"\"\"Get type, subtype & method from `meta`. Args: meta (dict): [description] Returns: Tuple[str]: type, subtype, method \"\"\" def fmt(x): return str(x).strip().lower().replace(' ', '_') def get_from_meta(key: str) -> str: return fmt(meta[key]) if key in meta else '' return get_from_meta('type'), get_from_meta('subtype'), get_from_meta('method')","title":"get_meta_descriptors"},{"location":"reference/text_explainability/ui/notebook/#information_renderer","text":"1 2 3 4 5 def information_renderer ( meta : dict , content : dict , ** renderargs ) -> str View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def information_renderer(meta: dict, content: dict, **renderargs) -> str: return featurelist_renderer(meta, content, first_element='token', second_element='mutual information', vertical=False, sorted=True, **renderargs)","title":"information_renderer"},{"location":"reference/text_explainability/ui/notebook/#plotly_fallback","text":"1 2 3 def plotly_fallback ( function ) View Source 1 2 3 4 5 6 7 def plotly_fallback(function): def inner(*args, **kwargs): return function(*args, **kwargs) if not plotly_available() else default_renderer(*args, **kwargs) return function","title":"plotly_fallback"},{"location":"reference/text_explainability/ui/notebook/#prototype_renderer","text":"1 2 3 4 5 def prototype_renderer ( meta : dict , content : dict , ** renderargs ) -> str View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def prototype_renderer(meta: dict, content: dict, **renderargs) -> str: def render_one(instance_type: str, instances) -> str: return f'<h4>{instance_type.title()}</h4><p>{format_instances(instances)}</p>' def render_class(class_name: Optional[str], instances: dict) -> str: html = '' if class_name is None else format_label(class_name, label_name='Class') for k, v in instances.items(): html += render_one(k, v) return html if all(k in ['instances', 'prototypes', 'criticisms'] for k in content.keys()): return render_class(None, content) return ''.join(render_class(class_name, instances) for class_name, instances in content.items())","title":"prototype_renderer"},{"location":"reference/text_explainability/ui/notebook/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/ui/notebook/#render","text":"1 2 3 class Render ( * configs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 class Render(BaseRender): def __init__(self, *configs): super().__init__(*configs) self.main_color = MAIN_COLOR self.package_link = 'https://git.io/text_explainability' self.extra_css = \"\"\" .token { display: inline-block; color: #000; padding: 0.8rem 0.7rem; margin: 0 0.2rem; } .token > .attribution { color: rgba(0, 0, 0, 0.8); vertical-align: super; font-size: smaller; } .token > .attribution::before { content: \" [\"; } .token > .attribution::after { content: \"]\"; } \"\"\" def get_renderer(self, meta: dict): type, subtype, _ = get_meta_descriptors(meta) if type == 'global_explanation': if 'frequency' in subtype.split('_'): return frequency_renderer elif 'information' in subtype.split('_'): return information_renderer elif 'prototypes' in subtype.split('_'): return prototype_renderer elif type == 'local_explanation': if subtype == 'feature_attribution': return feature_attribution_renderer return default_renderer def format_title(self, title: str, h: str = 'h1', **renderargs) -> str: return super().format_title(title, h=h, **renderargs).replace('_', ' ').title() def render_subtitle(self, meta: dict, content, **renderargs) -> str: type, subtype, _ = get_meta_descriptors(meta) labelwise = meta['labelwise'] if 'labelwise' in meta else False callargs = meta['callargs'] if 'callargs' in meta else '' def fmt_method(name: str) -> str: name, url = TRANSLATION_DICT[str.lower(name)] if str.lower(name) in TRANSLATION_DICT else (name, '') return f'<a href=\"{url}\" target=\"_blank\">{name}</a>' if url else name html = [] if 'method' in meta: html.append(f'Explanation generated with method {fmt_method(meta[\"method\"])}.') if type == 'global_explanation': if callargs: if 'explain_model' in callargs: what = 'predictions according to model' if callargs['explain_model'] \\ else 'ground-truth labels in dataset' how_many = f' (maximized to top-{callargs[\"k\"]})' if 'k' in callargs else '' html.append(f'{subtype.replace(\"_\", \" \").capitalize()} of {what}{how_many}.') if 'filter_words' in callargs: tokens = ', '.join(f'\"{t}\"' for t in callargs['filter_words']) html.append(f'Excluded tokens: {tokens if tokens else \"-\"}.') if labelwise: html.append('Grouped by label.') return self.format_subtitle('<br>'.join(html)) if html else ''","title":"Render"},{"location":"reference/text_explainability/ui/notebook/#ancestors-in-mro","text":"genbase.ui.notebook.Render","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/ui/notebook/#class-variables","text":"1 extra_css 1 main_color 1 package_link","title":"Class variables"},{"location":"reference/text_explainability/ui/notebook/#instance-variables","text":"1 custom_tab_title Title of custom tab. 1 package_name 1 tab_title Title of content tab.","title":"Instance variables"},{"location":"reference/text_explainability/ui/notebook/#methods","text":"","title":"Methods"},{"location":"reference/text_explainability/ui/notebook/#as_html","text":"1 2 3 4 def as_html ( self , ** renderargs ) -> str Get HTML element for interactive environments (e.g. Jupyter notebook). Parameters: Name Type Description Default **renderags None Optional arguments for rendering. None Returns: Type Description str HTML element. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def as_html(self, **renderargs) -> str: \"\"\"Get HTML element for interactive environments (e.g. Jupyter notebook). Args: **renderags: Optional arguments for rendering. Returns: str: HTML element. \"\"\" def fmt_exception(e: Exception, fmt_type: str = 'JSON') -> str: res = f'ERROR IN PARSING {fmt_type}\\n' res += '=' * len(res) + '\\n' return res + '\\n'.join(traceback.TracebackException.from_exception(e).format()) try: json = '\\n'.join(srsly.json_dumps(config, indent=2) for config in self.configs) except TypeError as e: json = fmt_exception(e, fmt_type='JSON') try: yaml = '\\n'.join(srsly.yaml_dumps(config) for config in self.configs) except srsly.ruamel_yaml.representer.RepresenterError as e: yaml = fmt_exception(e, fmt_type='YAML') html = ''.join(self.render_elements(config, **renderargs) for config in self.configs) id = str(uuid.uuid4()) tabs_id = f'tabs-{id}' ui_id = f'ui-{id}' CUSTOM_TAB = ''.join(self.custom_tab(config, **renderargs) for config in self.configs) if CUSTOM_TAB: CUSTOM_TAB = f\"\"\"<input type=\"radio\" name=\"{tabs_id}\" id=\"{tabs_id}-tab2\"/> <label class=\"wide\" for=\"{tabs_id}-tab2\">{self.custom_tab_title}</label> <div class=\"tab\">{CUSTOM_TAB}</div>\"\"\" HTML = f\"\"\" <div id=\"{ui_id}\"> <section class=\"ui-wrapper\"> <div class=\"ui-container\"> <div class=\"ui-block\"> <div id=\"{tabs_id}\"> <input type=\"radio\" name=\"{tabs_id}\" id=\"{tabs_id}-tab1\" checked=\"checked\" /> <label class=\"wide\" for=\"{tabs_id}-tab1\">{self.tab_title}</label> <div class=\"tab\">{html}</div> {CUSTOM_TAB} <input type=\"radio\" name=\"{tabs_id}\" id=\"{tabs_id}-tab{'3' if CUSTOM_TAB else '2'}\" /> <label for=\"{tabs_id}-tab{'3' if CUSTOM_TAB else '2'}\">{self.config_title}</label> <div class=\"tab code\"> <section> <div class=\"pre-buttons\"> <a class='copy' onclick=\"copy('json-output')\" href=\"#\" title=\"Copy JSON to clipboard\"> {CLONE_SVG} </a> </div> <h3>JSON</h3> </section> <pre id=\"json-output\">{json}</pre> <section> <div class=\"pre-buttons\"> <a class='copy' onclick=\"copy('yaml-output')\" href=\"#\" title=\"Copy YAML to clipboard\"> {CLONE_SVG} </a> </div> <h3>YAML</h3> </section> <pre id=\"yaml-output\">{yaml}</pre> </div> </div> </div> </div> </section> </div> \"\"\" JS = f'<script type=\"text/javascript\">{CUSTOM_JS}</script>' if CUSTOM_JS else '' main_color = renderargs.pop('main_color', self.main_color) package = renderargs.pop('package_link', self.package_link) package_name = self.package_name CSS = self.css(ui_color=main_color, ui_id=ui_id, tabs_id=tabs_id) FOOTER = f'<footer>Generated with <a href=\"{package}\" target=\"_blank\">{package_name}</a></footer>' return f'<style>{CSS}</style>{HTML}{FOOTER}{JS}'","title":"as_html"},{"location":"reference/text_explainability/ui/notebook/#css","text":"1 2 3 4 def css ( self , ** replacement_kwargs ) View Source 1 2 3 4 5 6 7 8 9 def css(self, **replacement_kwargs): css_ = CUSTOM_CSS + '\\n' + self.extra_css for k, v in replacement_kwargs.items(): css_ = css_.replace(f'--var({k})', v) return css_","title":"css"},{"location":"reference/text_explainability/ui/notebook/#custom_tab","text":"1 2 3 4 5 def custom_tab ( self , config : dict , ** renderargs ) -> str View Source 1 2 3 def custom_tab(self, config: dict, **renderargs) -> str: return ''","title":"custom_tab"},{"location":"reference/text_explainability/ui/notebook/#format_subtitle","text":"1 2 3 4 def format_subtitle ( self , subtitle : str ) -> str Format the subtitle in HTML format. Parameters: Name Type Description Default subtitle str Subtitle contents. None Returns: Type Description str Formatted subtitle. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def format_subtitle(self, subtitle: str) -> str: \"\"\"Format the subtitle in HTML format. Args: subtitle (str): Subtitle contents. Returns: str: Formatted subtitle. \"\"\" return f'<p class=\"info\">{subtitle}</p>'","title":"format_subtitle"},{"location":"reference/text_explainability/ui/notebook/#format_title","text":"1 2 3 4 5 6 def format_title ( self , title : str , h : str = 'h1' , ** renderargs ) -> str Format title in HTML format. Parameters: Name Type Description Default title str Title contents. None h str h-tag (h1, h2, ...). Defaults to 'h1'. 'h1' Returns: Type Description str Formatted title. View Source 1 2 3 def format_title(self, title: str, h: str = 'h1', **renderargs) -> str: return super().format_title(title, h=h, **renderargs).replace('_', ' ').title()","title":"format_title"},{"location":"reference/text_explainability/ui/notebook/#get_renderer","text":"1 2 3 4 def get_renderer ( self , meta : dict ) Get a render function (Callable taking meta , content and **renderargs and returning a str ). Parameters: Name Type Description Default meta dict Meta information to decide on appropriate renderer. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def get_renderer(self, meta: dict): type, subtype, _ = get_meta_descriptors(meta) if type == 'global_explanation': if 'frequency' in subtype.split('_'): return frequency_renderer elif 'information' in subtype.split('_'): return information_renderer elif 'prototypes' in subtype.split('_'): return prototype_renderer elif type == 'local_explanation': if subtype == 'feature_attribution': return feature_attribution_renderer return default_renderer","title":"get_renderer"},{"location":"reference/text_explainability/ui/notebook/#render_content","text":"1 2 3 4 5 6 def render_content ( self , meta : dict , content : dict , ** renderargs ) -> str Render content as HTML. Overwrite this when subclassing for your custom implementation. Parameters: Name Type Description Default meta dict Meta config. None content dict Content config. None **renderags None Optional arguments for rendering. None Returns: Type Description str Formatted content. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def render_content(self, meta: dict, content: dict, **renderargs) -> str: \"\"\"Render content as HTML. Overwrite this when subclassing for your custom implementation. Args: meta (dict): Meta config. content (dict): Content config. **renderags: Optional arguments for rendering. Returns: str: Formatted content. \"\"\" renderer = self.get_renderer(meta) return renderer(meta, content, **renderargs)","title":"render_content"},{"location":"reference/text_explainability/ui/notebook/#render_elements","text":"1 2 3 4 5 def render_elements ( self , config : dict , ** renderargs ) -> str Render HTML title and content. Parameters: Name Type Description Default config dict Config meta & content. None **renderags None Optional arguments for rendering. None Returns: Type Description str Formatted title and content. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def render_elements(self, config: dict, **renderargs) -> str: \"\"\"Render HTML title and content. Args: config (dict): Config meta & content. **renderags: Optional arguments for rendering. Returns: str: Formatted title and content. \"\"\" meta, content = config['META'], config['CONTENT'] return self.render_title(meta, content, **renderargs) + \\ self.render_subtitle(meta, content, **renderargs) + \\ self.render_content(meta, content, **renderargs)","title":"render_elements"},{"location":"reference/text_explainability/ui/notebook/#render_subtitle","text":"1 2 3 4 5 6 def render_subtitle ( self , meta : dict , content , ** renderargs ) -> str View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def render_subtitle(self, meta: dict, content, **renderargs) -> str: type, subtype, _ = get_meta_descriptors(meta) labelwise = meta['labelwise'] if 'labelwise' in meta else False callargs = meta['callargs'] if 'callargs' in meta else '' def fmt_method(name: str) -> str: name, url = TRANSLATION_DICT[str.lower(name)] if str.lower(name) in TRANSLATION_DICT else (name, '') return f'<a href=\"{url}\" target=\"_blank\">{name}</a>' if url else name html = [] if 'method' in meta: html.append(f'Explanation generated with method {fmt_method(meta[\"method\"])}.') if type == 'global_explanation': if callargs: if 'explain_model' in callargs: what = 'predictions according to model' if callargs['explain_model'] \\ else 'ground-truth labels in dataset' how_many = f' (maximized to top-{callargs[\"k\"]})' if 'k' in callargs else '' html.append(f'{subtype.replace(\"_\", \" \").capitalize()} of {what}{how_many}.') if 'filter_words' in callargs: tokens = ', '.join(f'\"{t}\"' for t in callargs['filter_words']) html.append(f'Excluded tokens: {tokens if tokens else \"-\"}.') if labelwise: html.append('Grouped by label.') return self.format_subtitle('<br>'.join(html)) if html else ''","title":"render_subtitle"},{"location":"reference/text_explainability/ui/notebook/#render_title","text":"1 2 3 4 5 6 def render_title ( self , meta : dict , content : dict , ** renderargs ) -> str Render the title as HTML. Overwrite this when subclassing for your custom implementation. Parameters: Name Type Description Default meta dict Meta config. None content dict Content config. None **renderags None Optional arguments for rendering. None Returns: Type Description str Formatted title. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def render_title(self, meta: dict, content: dict, **renderargs) -> str: \"\"\"Render the title as HTML. Overwrite this when subclassing for your custom implementation. Args: meta (dict): Meta config. content (dict): Content config. **renderags: Optional arguments for rendering. Returns: str: Formatted title. \"\"\" title = renderargs.pop('title', None) if title is None: if 'title' in meta: title = meta['title'] elif 'type' in meta: title = meta['type'] if 'subtype' in meta: title += f' ({meta[\"subtype\"]})' return self.format_title(title, **renderargs) if title else ''","title":"render_title"}]}